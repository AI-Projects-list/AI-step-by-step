{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6997b11",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "## üìñ What is Linear Regression?\n",
    "\n",
    "Linear Regression models the relationship between input features (X) and output (y) as a straight line:\n",
    "\n",
    "**Formula**: `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô`\n",
    "\n",
    "Where:\n",
    "- `Œ≤‚ÇÄ` = intercept (bias)\n",
    "- `Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô` = coefficients (weights)\n",
    "- `x‚ÇÅ, x‚ÇÇ, ..., x‚Çô` = features\n",
    "\n",
    "**How it works**: Finds the best-fit line that minimizes the sum of squared errors (distance between predictions and actual values).\n",
    "\n",
    "## üéØ Why Use Linear Regression?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Simple and Fast** - Easy to implement, quick to train\n",
    "2. **Interpretable** - Clear understanding of feature impact\n",
    "3. **Low Computational Cost** - Works on large datasets\n",
    "4. **Good Baseline** - Starting point for comparison\n",
    "5. **Well-Understood** - Extensive theory and diagnostics\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Assumes Linearity** - Can't capture complex non-linear patterns\n",
    "2. **Sensitive to Outliers** - Outliers heavily influence the line\n",
    "3. **Multicollinearity Issues** - Correlated features cause problems\n",
    "4. **Overfitting Risk** - With many features\n",
    "\n",
    "## ‚è±Ô∏è When to Use Linear Regression\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Linear Relationship Exists**\n",
    "- Example: Predict house price from square footage\n",
    "- Why: Price increases linearly with size\n",
    "- Data: Square footage vs price shows straight line\n",
    "\n",
    "**2. Need Interpretability**\n",
    "- Example: Medical research - understand factor impact\n",
    "- Why: Coefficients show feature importance\n",
    "- Benefit: Can explain predictions to stakeholders\n",
    "\n",
    "**3. Small to Medium Dataset**\n",
    "- Example: 100-10,000 samples\n",
    "- Why: Efficient, no risk of overfitting\n",
    "- Alternative: For huge data, consider simpler models\n",
    "\n",
    "**4. Quick Baseline Needed**\n",
    "- Example: Start of ML project\n",
    "- Why: Fast to implement and test\n",
    "- Next step: Compare against complex models\n",
    "\n",
    "**5. Few Features**\n",
    "- Example: 1-20 features\n",
    "- Why: Avoids overfitting\n",
    "- Note: Can handle more with regularization\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Non-Linear Relationships**\n",
    "- Example: Exponential growth (population, virus spread)\n",
    "- Better: Polynomial regression, tree-based models\n",
    "- Why: Linear model won't capture curves\n",
    "\n",
    "**2. Many Outliers**\n",
    "- Example: Income data (billionaires skew results)\n",
    "- Better: Robust regression, tree models\n",
    "- Why: Outliers pull the line away from most data\n",
    "\n",
    "**3. Categorical Predictions**\n",
    "- Example: Classify spam/not spam\n",
    "- Better: Logistic regression, classifiers\n",
    "- Why: Linear regression for continuous values only\n",
    "\n",
    "**4. High Multicollinearity**\n",
    "- Example: Correlated features (height in cm and inches)\n",
    "- Better: Ridge/Lasso regression, PCA first\n",
    "- Why: Unstable coefficients\n",
    "\n",
    "## üìä Complexity\n",
    "\n",
    "- **Training Time**: O(n √ó p¬≤) where n=samples, p=features\n",
    "- **Prediction Time**: O(p)\n",
    "- **Space**: O(p)\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Real Estate** - Predict house prices from features\n",
    "2. **Finance** - Stock price forecasting, risk assessment\n",
    "3. **Marketing** - Sales prediction based on ad spend\n",
    "4. **Healthcare** - Predict patient recovery time\n",
    "5. **Economics** - GDP prediction, inflation modeling\n",
    "6. **Weather** - Temperature forecasting\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Best for linear relationships  \n",
    "‚úÖ Highly interpretable - know which features matter  \n",
    "‚úÖ Fast and simple - good starting point  \n",
    "‚úÖ Assumptions: linearity, normality, homoscedasticity  \n",
    "‚úÖ Check residual plots to validate assumptions  \n",
    "‚úÖ Use regularization (Ridge/Lasso) for many features  \n",
    "‚úÖ Scale features for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION - COMPLETE EXAMPLE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LINEAR REGRESSION - HOUSE PRICE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. CREATE SAMPLE DATA\n",
    "print(\"\\n1. CREATING SAMPLE DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Features\n",
    "square_feet = np.random.randint(800, 3500, n_samples)\n",
    "bedrooms = np.random.randint(1, 6, n_samples)\n",
    "age = np.random.randint(0, 50, n_samples)\n",
    "\n",
    "# Target: Price (with some noise)\n",
    "# Formula: Price = 100 + 150*sqft + 20000*bedrooms - 2000*age + noise\n",
    "price = (100 + \n",
    "         150 * square_feet + \n",
    "         20000 * bedrooms - \n",
    "         2000 * age + \n",
    "         np.random.normal(0, 20000, n_samples))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'SquareFeet': square_feet,\n",
    "    'Bedrooms': bedrooms,\n",
    "    'Age': age,\n",
    "    'Price': price\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nStatistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 2. PREPARE DATA\n",
    "print(\"\\n2. PREPARING DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X = df[['SquareFeet', 'Bedrooms', 'Age']]\n",
    "y = df['Price']\n",
    "\n",
    "# Split into train and test sets (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# 3. CREATE AND TRAIN MODEL\n",
    "print(\"\\n3. TRAINING LINEAR REGRESSION MODEL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Initialize model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"\\nIntercept (Œ≤‚ÇÄ): ${model.intercept_:,.2f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"  {feature}: ${coef:,.2f}\")\n",
    "\n",
    "# 4. MAKE PREDICTIONS\n",
    "print(\"\\n4. MAKING PREDICTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions (first 5 test samples):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_test.values[:5],\n",
    "    'Predicted': y_pred_test[:5],\n",
    "    'Difference': y_test.values[:5] - y_pred_test[:5]\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# 5. EVALUATE MODEL\n",
    "print(\"\\n5. MODEL EVALUATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"  R¬≤ Score: {train_r2:.4f}\")\n",
    "print(f\"  RMSE: ${train_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${train_mae:,.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"  R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"  RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${test_mae:,.2f}\")\n",
    "\n",
    "# 6. INTERPRET RESULTS\n",
    "print(\"\\n6. INTERPRETATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nModel Equation:\")\n",
    "print(f\"Price = {model.intercept_:,.2f} + \")\n",
    "print(f\"        {model.coef_[0]:,.2f} √ó SquareFeet + \")\n",
    "print(f\"        {model.coef_[1]:,.2f} √ó Bedrooms + \")\n",
    "print(f\"        {model.coef_[2]:,.2f} √ó Age\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Each additional square foot adds ${model.coef_[0]:,.2f} to price\")\n",
    "print(f\"  - Each additional bedroom adds ${model.coef_[1]:,.2f} to price\")\n",
    "print(f\"  - Each year of age changes price by ${model.coef_[2]:,.2f}\")\n",
    "print(f\"  - R¬≤ = {test_r2:.2%} of variance explained by model\")\n",
    "\n",
    "# 7. MAKE NEW PREDICTION\n",
    "print(\"\\n7. PREDICT NEW HOUSE PRICE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "new_house = pd.DataFrame({\n",
    "    'SquareFeet': [2000],\n",
    "    'Bedrooms': [3],\n",
    "    'Age': [10]\n",
    "})\n",
    "\n",
    "predicted_price = model.predict(new_house)[0]\n",
    "\n",
    "print(f\"New house features:\")\n",
    "print(f\"  Square Feet: {new_house['SquareFeet'].values[0]:,}\")\n",
    "print(f\"  Bedrooms: {new_house['Bedrooms'].values[0]}\")\n",
    "print(f\"  Age: {new_house['Age'].values[0]} years\")\n",
    "print(f\"\\nPredicted Price: ${predicted_price:,.2f}\")\n",
    "\n",
    "# 8. VISUALIZE RESULTS\n",
    "print(\"\\n8. CREATING VISUALIZATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "axes[0, 0].scatter(y_test, y_pred_test, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Price')\n",
    "axes[0, 0].set_ylabel('Predicted Price')\n",
    "axes[0, 0].set_title(f'Actual vs Predicted (R¬≤ = {test_r2:.3f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[0, 1].scatter(y_pred_test, residuals, alpha=0.5)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Price')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "axes[1, 0].barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "axes[1, 0].set_xlabel('Coefficient Value')\n",
    "axes[1, 0].set_title('Feature Importance (Coefficients)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Distribution of Errors\n",
    "axes[1, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Residual Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Residuals')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Linear Regression successfully trained\")\n",
    "print(f\"‚úì Model explains {test_r2:.1%} of price variation\")\n",
    "print(f\"‚úì Average prediction error: ${test_mae:,.2f}\")\n",
    "print(\"‚úì Best for: Linear relationships, interpretability needed\")\n",
    "print(\"‚úì Assumptions: Check residual plots for validation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ab790",
   "metadata": {},
   "source": [
    "# 2. Polynomial Regression\n",
    "\n",
    "## üìñ What is Polynomial Regression?\n",
    "\n",
    "Polynomial Regression extends linear regression by adding polynomial terms (x¬≤, x¬≥, etc.) to capture **non-linear relationships**.\n",
    "\n",
    "**Formula**: `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ... + Œ≤‚Çôx‚Åø`\n",
    "\n",
    "**How it works**: \n",
    "1. Transform features into polynomial features\n",
    "2. Apply linear regression on transformed features\n",
    "3. Result: Curved line that fits data better\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Original: x = [1, 2, 3]\n",
    "Degree 2: [1, x, x¬≤] = [1, 1, 1], [1, 2, 4], [1, 3, 9]\n",
    "Degree 3: [1, x, x¬≤, x¬≥] = [1, 1, 1, 1], [1, 2, 4, 8], [1, 3, 9, 27]\n",
    "```\n",
    "\n",
    "## üéØ Why Use Polynomial Regression?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Captures Non-Linearity** - Models curves and complex patterns\n",
    "2. **Still Interpretable** - Extension of linear regression\n",
    "3. **Flexible** - Control complexity with degree\n",
    "4. **No New Algorithm** - Uses linear regression\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Overfitting Risk** - High degrees fit noise\n",
    "2. **Extrapolation Issues** - Poor predictions outside training range\n",
    "3. **More Features** - Computational cost increases\n",
    "4. **Multicollinearity** - x and x¬≤ are correlated\n",
    "\n",
    "## ‚è±Ô∏è When to Use Polynomial Regression\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Curved Relationships**\n",
    "- Example: Temperature vs ice cream sales (peaks at optimal temp)\n",
    "- Why: Linear model misses the curve\n",
    "- Degree: Usually 2-3\n",
    "\n",
    "**2. Known Non-Linear Physics**\n",
    "- Example: Projectile motion (parabolic)\n",
    "- Why: Physics follows polynomial laws\n",
    "- Degree: Based on physics (often 2)\n",
    "\n",
    "**3. Small Dataset with Curve**\n",
    "- Example: 100 samples showing clear curve\n",
    "- Why: Tree models might overfit\n",
    "- Benefit: Smooth curve generalization\n",
    "\n",
    "**4. Need Interpretability**\n",
    "- Example: Scientific research\n",
    "- Why: Can still interpret coefficients\n",
    "- Alternative: Neural networks are black boxes\n",
    "\n",
    "**5. Interpolation Needed**\n",
    "- Example: Fill gaps in time series\n",
    "- Why: Smooth curve between points\n",
    "- Warning: Don't extrapolate far!\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Need to Extrapolate**\n",
    "- Example: Predict 10 years beyond training data\n",
    "- Better: Domain-specific models\n",
    "- Why: Polynomials diverge outside range\n",
    "\n",
    "**2. Many Features**\n",
    "- Example: 100 features ‚Üí thousands after polynomial transform\n",
    "- Better: Tree-based models, neural networks\n",
    "- Why: Curse of dimensionality\n",
    "\n",
    "**3. Very Complex Non-Linearity**\n",
    "- Example: Image recognition\n",
    "- Better: Neural networks\n",
    "- Why: Polynomial can't capture complex patterns\n",
    "\n",
    "**4. Large Dataset**\n",
    "- Example: Millions of samples\n",
    "- Better: Gradient boosting, neural networks\n",
    "- Why: Computational cost too high\n",
    "\n",
    "## üìä Complexity\n",
    "\n",
    "- **Training Time**: O(n √ó p^d) where d=degree\n",
    "- **Prediction Time**: O(p^d)\n",
    "- **Space**: O(p^d) features created\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Physics** - Trajectory prediction, force relationships\n",
    "2. **Economics** - Diminishing returns, growth curves\n",
    "3. **Biology** - Population growth, enzyme kinetics\n",
    "4. **Marketing** - Sales response curves\n",
    "5. **Engineering** - Stress-strain relationships\n",
    "6. **Climate** - Temperature patterns\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Best for curved relationships  \n",
    "‚úÖ Degree 2-3 usually sufficient  \n",
    "‚úÖ Higher degree = overfitting risk  \n",
    "‚úÖ Always use cross-validation  \n",
    "‚úÖ Don't extrapolate far from training range  \n",
    "‚úÖ Consider regularization (Ridge/Lasso)  \n",
    "‚úÖ Visualize to choose appropriate degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLYNOMIAL REGRESSION - COMPLETE EXAMPLE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POLYNOMIAL REGRESSION - TEMPERATURE vs SALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. CREATE NON-LINEAR DATA\n",
    "print(\"\\n1. CREATING NON-LINEAR DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Temperature (¬∞C)\n",
    "temperature = np.random.uniform(0, 40, n_samples)\n",
    "\n",
    "# Sales: Quadratic relationship (peaks around 25¬∞C)\n",
    "# Formula: Sales = -2*(temp-25)¬≤ + 1000 + noise\n",
    "sales = -2 * (temperature - 25)**2 + 1000 + np.random.normal(0, 50, n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Temperature': temperature,\n",
    "    'Sales': sales\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. PREPARE DATA\n",
    "print(\"\\n2. PREPARING DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X = df[['Temperature']]\n",
    "y = df['Sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# 3. TRAIN MODELS WITH DIFFERENT DEGREES\n",
    "print(\"\\n3. TRAINING MODELS (Degrees 1, 2, 3, 5)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "degrees = [1, 2, 3, 5]\n",
    "models = {}\n",
    "scores = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create pipeline: PolynomialFeatures ‚Üí LinearRegression\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    models[degree] = model\n",
    "    scores[degree] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nDegree {degree}:\")\n",
    "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.2f}\")\n",
    "    \n",
    "    # Show number of features created\n",
    "    n_features = model.named_steps['poly'].n_output_features_\n",
    "    print(f\"  Features created: {n_features}\")\n",
    "\n",
    "# 4. FIND BEST DEGREE\n",
    "print(\"\\n4. SELECTING BEST DEGREE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_degree = max(scores, key=lambda k: scores[k]['test_r2'])\n",
    "best_model = models[best_degree]\n",
    "\n",
    "print(f\"Best degree: {best_degree}\")\n",
    "print(f\"Best test R¬≤: {scores[best_degree]['test_r2']:.4f}\")\n",
    "print(f\"Best test RMSE: {scores[best_degree]['test_rmse']:.2f}\")\n",
    "\n",
    "# 5. MAKE PREDICTIONS\n",
    "print(\"\\n5. MAKING PREDICTIONS WITH BEST MODEL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Predict for new temperatures\n",
    "new_temps = pd.DataFrame({'Temperature': [15, 25, 35]})\n",
    "predictions = best_model.predict(new_temps)\n",
    "\n",
    "print(\"\\nPredictions for new temperatures:\")\n",
    "for temp, pred in zip(new_temps['Temperature'], predictions):\n",
    "    print(f\"  Temperature: {temp}¬∞C ‚Üí Predicted Sales: {pred:.2f}\")\n",
    "\n",
    "# 6. VISUALIZE RESULTS\n",
    "print(\"\\n6. CREATING VISUALIZATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Create smooth curve for plotting\n",
    "X_plot = np.linspace(0, 40, 300).reshape(-1, 1)\n",
    "\n",
    "# Plot 1-4: Different polynomial degrees\n",
    "for idx, degree in enumerate(degrees):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    # Scatter plot of actual data\n",
    "    axes[row, col].scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
    "    axes[row, col].scatter(X_test, y_test, alpha=0.5, color='red', label='Test data')\n",
    "    \n",
    "    # Plot fitted curve\n",
    "    y_plot = models[degree].predict(X_plot)\n",
    "    axes[row, col].plot(X_plot, y_plot, 'g-', linewidth=2, label=f'Degree {degree} fit')\n",
    "    \n",
    "    axes[row, col].set_xlabel('Temperature (¬∞C)')\n",
    "    axes[row, col].set_ylabel('Sales')\n",
    "    axes[row, col].set_title(\n",
    "        f'Polynomial Degree {degree}\\n'\n",
    "        f'Train R¬≤={scores[degree][\"train_r2\"]:.3f}, '\n",
    "        f'Test R¬≤={scores[degree][\"test_r2\"]:.3f}'\n",
    "    )\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "for degree, color in zip(degrees, colors):\n",
    "    y_plot = models[degree].predict(X_plot)\n",
    "    plt.plot(X_plot, y_plot, color=color, linewidth=2, \n",
    "             label=f'Degree {degree} (R¬≤={scores[degree][\"test_r2\"]:.3f})')\n",
    "\n",
    "plt.xlabel('Temperature (¬∞C)')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Comparison of Polynomial Degrees')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations created!\")\n",
    "\n",
    "# 7. DEMONSTRATE OVERFITTING\n",
    "print(\"\\n7. OVERFITTING ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nDegree vs Performance:\")\n",
    "print(f\"{'Degree':<10} {'Train R¬≤':<12} {'Test R¬≤':<12} {'Difference':<12}\")\n",
    "print(\"-\"*50)\n",
    "for degree in degrees:\n",
    "    train_r2 = scores[degree]['train_r2']\n",
    "    test_r2 = scores[degree]['test_r2']\n",
    "    diff = train_r2 - test_r2\n",
    "    print(f\"{degree:<10} {train_r2:<12.4f} {test_r2:<12.4f} {diff:<12.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Large difference = Overfitting\")\n",
    "print(\"  - Degree 1 (linear) = Underfitting\")\n",
    "print(f\"  - Degree {best_degree} = Best balance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Polynomial Regression successfully trained\")\n",
    "print(f\"‚úì Best polynomial degree: {best_degree}\")\n",
    "print(\"‚úì Captures curved relationships\")\n",
    "print(\"‚úì Use When: Non-linear but smooth relationships\")\n",
    "print(\"‚úì Warning: Don't extrapolate beyond training range!\")\n",
    "print(\"‚úì Tip: Use cross-validation to choose degree\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
