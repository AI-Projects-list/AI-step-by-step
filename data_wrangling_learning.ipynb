{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be118174",
   "metadata": {},
   "source": [
    "# Data Wrangling Learning Guide\n",
    "\n",
    "## What is Data Wrangling?\n",
    "\n",
    "**Data Wrangling** (also called Data Munging) is the process of transforming and mapping raw data from one format into another to make it more appropriate and valuable for analysis and modeling. It's the essential bridge between collecting data and analyzing it.\n",
    "\n",
    "## Why is Data Wrangling Important?\n",
    "\n",
    "1. **Clean Messy Data** - Real-world data is rarely clean and ready to use\n",
    "2. **Standardize Formats** - Convert data into consistent, usable formats\n",
    "3. **Combine Multiple Sources** - Integrate data from various sources\n",
    "4. **Prepare for Analysis** - Transform data into the right shape for modeling\n",
    "5. **Handle Missing Values** - Deal with incomplete or corrupted data\n",
    "6. **Improve Data Quality** - Detect and fix errors, duplicates, and inconsistencies\n",
    "7. **Save Time** - Automate repetitive data transformation tasks\n",
    "\n",
    "## What You'll Learn in This Notebook\n",
    "\n",
    "This comprehensive guide covers **15 essential data wrangling topics**:\n",
    "\n",
    "1. [Data Loading and Inspection](#1-data-loading-and-inspection) - Loading and initial exploration\n",
    "2. [Data Cleaning](#2-data-cleaning-handling-missing-duplicates) - Handling missing values and duplicates\n",
    "3. [Data Type Conversions](#3-data-type-conversions) - Converting between data types\n",
    "4. [String Operations](#4-string-operations) - Text cleaning and manipulation\n",
    "5. [Data Filtering and Subsetting](#5-data-filtering-and-subsetting) - Selecting specific data\n",
    "6. [Data Transformation](#6-data-transformation) - Reshaping and restructuring\n",
    "7. [Sorting and Ranking](#7-sorting-and-ranking) - Ordering and ranking data\n",
    "8. [Grouping and Aggregation](#8-grouping-and-aggregation) - Summarizing data by groups\n",
    "9. [Merging and Joining](#9-merging-and-joining) - Combining multiple datasets\n",
    "10. [Pivot Tables and Cross-Tabulation](#10-pivot-tables-and-cross-tabulation) - Creating summary tables\n",
    "11. [Handling Categorical Data](#11-handling-categorical-data) - Encoding categories\n",
    "12. [Feature Engineering](#12-feature-engineering) - Creating new features\n",
    "13. [Handling Outliers](#13-handling-outliers) - Detecting and managing outliers\n",
    "14. [Time Series Wrangling](#14-time-series-wrangling) - Working with dates and times\n",
    "15. [Advanced Wrangling Techniques](#15-advanced-wrangling-techniques) - Optimization and best practices\n",
    "\n",
    "## Tools & Libraries Used\n",
    "\n",
    "- **pandas** - Primary data manipulation library\n",
    "- **numpy** - Numerical operations and array handling\n",
    "- **matplotlib & seaborn** - Data visualization\n",
    "- **datetime** - Date and time manipulation\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run cells in order** - Each section builds on previous concepts\n",
    "2. **Practice with examples** - Try the code with sample data\n",
    "3. **Apply to your data** - Replace sample data with your own datasets\n",
    "4. **Experiment** - Modify parameters to understand behavior\n",
    "5. **Take notes** - Document patterns and techniques you find useful\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "This notebook uses multiple **synthetic datasets** to demonstrate various wrangling techniques:\n",
    "\n",
    "- **Customer Data**: Demographics and transaction information\n",
    "- **Sales Data**: Product sales with dates and amounts\n",
    "- **Time Series Data**: Temporal data for resampling and aggregation\n",
    "- **Multi-source Data**: Separate tables for merging and joining\n",
    "\n",
    "Each dataset is designed to showcase specific wrangling challenges like missing values, duplicates, inconsistent formats, and complex transformations.\n",
    "\n",
    "## The Data Wrangling Workflow\n",
    "\n",
    "```\n",
    "Raw Data â†’ Load â†’ Clean â†’ Transform â†’ Integrate â†’ Validate â†’ Ready for Analysis\n",
    "```\n",
    "\n",
    "1. **Load**: Import data from various sources\n",
    "2. **Clean**: Handle missing values, duplicates, errors\n",
    "3. **Transform**: Convert types, reshape, engineer features\n",
    "4. **Integrate**: Merge and combine datasets\n",
    "5. **Validate**: Check data quality and consistency\n",
    "6. **Export**: Save processed data for analysis\n",
    "\n",
    "---\n",
    "\n",
    "Let's master data wrangling! ðŸ”§ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fbf375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94683f10",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Inspection\n",
    "\n",
    "**What is it?**\n",
    "Loading data from various sources (CSV, Excel, databases, APIs) and performing initial exploration to understand structure, types, and quality.\n",
    "\n",
    "**Why use it?**\n",
    "- Understand data structure before processing\n",
    "- Identify data quality issues early\n",
    "- Plan appropriate wrangling strategies\n",
    "- Verify data loaded correctly\n",
    "\n",
    "**When to use it?**\n",
    "- Start of every data project\n",
    "- After receiving new data\n",
    "- Before any transformations\n",
    "- When debugging data issues\n",
    "\n",
    "**Common loading methods:**\n",
    "- CSV: `pd.read_csv()`\n",
    "- Excel: `pd.read_excel()`\n",
    "- JSON: `pd.read_json()`\n",
    "- SQL: `pd.read_sql()`\n",
    "- HTML: `pd.read_html()`\n",
    "- Parquet: `pd.read_parquet()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1795f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING AND INSPECTION\n",
      "================================================================================\n",
      "\n",
      "1. Basic Dataset Information:\n",
      "\n",
      "   Sales Data:\n",
      "   Shape: (100, 6) (rows, columns)\n",
      "   Size: 600 total elements\n",
      "   Memory usage: 13.07 KB\n",
      "\n",
      "2. First 5 rows (head):\n",
      "        date  product region  sales_amount  quantity  customer_id\n",
      "0 2024-01-01   Tablet   East        159.72        11         1032\n",
      "1 2024-01-02  Monitor  South       1309.18        17         1007\n",
      "2 2024-01-03   Laptop  South        697.28         8         1043\n",
      "3 2024-01-04   Tablet   West       1066.28         4         1043\n",
      "4 2024-01-05   Tablet  South       1824.38         6         1004\n",
      "\n",
      "   Last 5 rows (tail):\n",
      "         date  product region  sales_amount  quantity  customer_id\n",
      "95 2024-04-05    Phone  South        763.50        10         1034\n",
      "96 2024-04-06    Phone  South       1479.32         6         1032\n",
      "97 2024-04-07  Monitor   West       1804.51        15         1032\n",
      "98 2024-04-08    Phone  North       1785.46        11         1042\n",
      "99 2024-04-09   Laptop   East       1581.76         5         1036\n",
      "\n",
      "3. Data Types:\n",
      "date            datetime64[ns]\n",
      "product                 object\n",
      "region                  object\n",
      "sales_amount           float64\n",
      "quantity                 int32\n",
      "customer_id              int32\n",
      "dtype: object\n",
      "\n",
      "4. Column Names:\n",
      "   ['date', 'product', 'region', 'sales_amount', 'quantity', 'customer_id']\n",
      "\n",
      "   Index:\n",
      "   Type: <class 'pandas.core.indexes.range.RangeIndex'>\n",
      "   Range: 0 to 99\n",
      "\n",
      "5. Summary Statistics:\n",
      "                      date  sales_amount  quantity  customer_id\n",
      "count                  100        100.00    100.00       100.00\n",
      "mean   2024-02-19 12:00:00       1045.88      9.56      1024.93\n",
      "min    2024-01-01 00:00:00        113.21      1.00      1000.00\n",
      "25%    2024-01-25 18:00:00        559.81      3.75      1014.50\n",
      "50%    2024-02-19 12:00:00       1060.69      9.00      1026.50\n",
      "75%    2024-03-15 06:00:00       1555.75     15.00      1036.00\n",
      "max    2024-04-09 00:00:00       1972.74     19.00      1049.00\n",
      "std                    NaN        556.91      6.00        14.32\n",
      "\n",
      "6. Comprehensive Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          100 non-null    datetime64[ns]\n",
      " 1   product       100 non-null    object        \n",
      " 2   region        100 non-null    object        \n",
      " 3   sales_amount  100 non-null    float64       \n",
      " 4   quantity      100 non-null    int32         \n",
      " 5   customer_id   100 non-null    int32         \n",
      "dtypes: datetime64[ns](1), float64(1), int32(2), object(2)\n",
      "memory usage: 4.0+ KB\n",
      "\n",
      "7. Unique Values Count:\n",
      "   product: 4 unique values\n",
      "   Values: ['Tablet' 'Monitor' 'Laptop' 'Phone']\n",
      "   region: 4 unique values\n",
      "   Values: ['East' 'South' 'West' 'North']\n",
      "\n",
      "8. Value Counts (Product):\n",
      "product\n",
      "Monitor    30\n",
      "Phone      26\n",
      "Tablet     24\n",
      "Laptop     20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "9. Missing Values:\n",
      "date            0\n",
      "product         0\n",
      "region          0\n",
      "sales_amount    0\n",
      "quantity        0\n",
      "customer_id     0\n",
      "dtype: int64\n",
      "\n",
      "10. Random Sample (3 rows):\n",
      "         date  product region  sales_amount  quantity  customer_id\n",
      "83 2024-03-24  Monitor   East       1766.94         1         1015\n",
      "53 2024-02-23  Monitor   East       1029.96        17         1004\n",
      "70 2024-03-11    Phone   West       1387.37         8         1022\n",
      "\n",
      "   âœ“ Data loading and inspection complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. DATA LOADING AND INSPECTION\n",
    "# ============================================\n",
    "\n",
    "# Create sample datasets for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sales dataset\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "sales_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], 100),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'sales_amount': np.random.uniform(100, 2000, 100),\n",
    "    'quantity': np.random.randint(1, 20, 100),\n",
    "    'customer_id': np.random.randint(1000, 1050, 100)\n",
    "})\n",
    "\n",
    "# Customer dataset\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1000, 1050),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1000, 1050)],\n",
    "    'age': np.random.randint(20, 70, 50),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston'], 50),\n",
    "    'customer_type': np.random.choice(['Regular', 'Premium', 'VIP'], 50)\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA LOADING AND INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Basic inspection methods\n",
    "print(\"\\n1. Basic Dataset Information:\")\n",
    "print(\"\\n   Sales Data:\")\n",
    "print(f\"   Shape: {sales_data.shape} (rows, columns)\")\n",
    "print(f\"   Size: {sales_data.size} total elements\")\n",
    "print(f\"   Memory usage: {sales_data.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# 2. Display first and last rows\n",
    "print(\"\\n2. First 5 rows (head):\")\n",
    "print(sales_data.head())\n",
    "\n",
    "print(\"\\n   Last 5 rows (tail):\")\n",
    "print(sales_data.tail())\n",
    "\n",
    "# 3. Data types\n",
    "print(\"\\n3. Data Types:\")\n",
    "print(sales_data.dtypes)\n",
    "\n",
    "# 4. Column names and index\n",
    "print(\"\\n4. Column Names:\")\n",
    "print(f\"   {sales_data.columns.tolist()}\")\n",
    "\n",
    "print(\"\\n   Index:\")\n",
    "print(f\"   Type: {type(sales_data.index)}\")\n",
    "print(f\"   Range: {sales_data.index[0]} to {sales_data.index[-1]}\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(\"\\n5. Summary Statistics:\")\n",
    "print(sales_data.describe())\n",
    "\n",
    "# 6. Info method (comprehensive overview)\n",
    "print(\"\\n6. Comprehensive Info:\")\n",
    "sales_data.info()\n",
    "\n",
    "# 7. Unique values\n",
    "print(\"\\n7. Unique Values Count:\")\n",
    "for col in ['product', 'region']:\n",
    "    print(f\"   {col}: {sales_data[col].nunique()} unique values\")\n",
    "    print(f\"   Values: {sales_data[col].unique()}\")\n",
    "\n",
    "# 8. Value counts\n",
    "print(\"\\n8. Value Counts (Product):\")\n",
    "print(sales_data['product'].value_counts())\n",
    "\n",
    "# 9. Missing values check\n",
    "print(\"\\n9. Missing Values:\")\n",
    "print(sales_data.isnull().sum())\n",
    "\n",
    "# 10. Sample random rows\n",
    "print(\"\\n10. Random Sample (3 rows):\")\n",
    "print(sales_data.sample(n=3, random_state=42))\n",
    "\n",
    "print(\"\\n   âœ“ Data loading and inspection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e860e84",
   "metadata": {},
   "source": [
    "## 2. Filtering and Subsetting Data\n",
    "\n",
    "**What is it?**\n",
    "Selecting specific rows and columns from a dataset based on conditions or criteria.\n",
    "\n",
    "**Why use it?**\n",
    "- Focus on relevant data\n",
    "- Reduce memory usage\n",
    "- Improve processing speed\n",
    "- Extract specific subsets for analysis\n",
    "- Remove unwanted records\n",
    "\n",
    "**When to use it?**\n",
    "- Need specific time periods\n",
    "- Focus on certain categories\n",
    "- Extract records meeting conditions\n",
    "- Remove outliers or anomalies\n",
    "- Create training/test splits\n",
    "\n",
    "**Filtering methods:**\n",
    "1. **Boolean indexing**: `df[df['column'] > value]`\n",
    "2. **loc**: Label-based selection\n",
    "3. **iloc**: Position-based selection\n",
    "4. **query()**: SQL-like filtering\n",
    "5. **isin()**: Match against list of values\n",
    "6. **between()**: Range filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e860e84",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (148314264.py, line 30)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"   Selected columns:\")\\nprint(selected_cols.head())\\n\\n# Select rows and columns\\nsubset = sales_data.loc[0:4, ['product', 'region', 'sales_amount']]\\nprint(\\\"\\\\n   Rows 0-4, specific columns:\\\")\\nprint(subset)\\n\\n# Select with condition\\nlaptops = sales_data.loc[sales_data['product'] == 'Laptop', ['date', 'sales_amount', 'quantity']]\\nprint(f\\\"\\\\n   Laptop sales only: {len(laptops)} records\\\")\\n\\n# 3. iloc - Position-based selection\\nprint(\\\"\\\\n3. iloc (Position-based selection):\\\")\\n\\n# First 10 rows, first 3 columns\\nfirst_subset = sales_data.iloc[:10, :3]\\nprint(\\\"   First 10 rows, first 3 columns:\\\")\\nprint(first_subset)\\n\\n# Specific rows and columns\\nspecific = sales_data.iloc[[0, 5, 10], [1, 3, 4]]\\nprint(\\\"\\\\n   Specific rows [0,5,10] and columns [1,3,4]:\\\")\\nprint(specific)\\n\\n# Every 10th row\\nevery_10th = sales_data.iloc[::10, :]\\nprint(f\\\"\\\\n   Every 10th row: {len(every_10th)} records\\\")\\n\\n# 4. query() method\\nprint(\\\"\\\\n4. query() Method (SQL-like):\\\")\\n\\n# Simple query\\nquery_result = sales_data.query('sales_amount > 1500 and region == \\\"North\\\"')\\nprint(f\\\"   Sales > 1500 in North: {len(query_result)} records\\\")\\nprint(query_result.head())\\n\\n# Using variables in query\\nthreshold = 1200\\nquery_result2 = sales_data.query('sales_amount > @threshold')\\nprint(f\\\"\\\\n   Sales > {threshold}: {len(query_result2)} records\\\")\\n\\n# Complex query\\ncomplex_query = sales_data.query('product in [\\\"Laptop\\\", \\\"Tablet\\\"] and quantity >= 10')\\nprint(f\\\"\\\\n   Laptop/Tablet with quantity >= 10: {len(complex_query)} records\\\")\\n\\n# 5. isin() method\\nprint(\\\"\\\\n5. isin() Method:\\\")\\n\\n# Filter by list of values\\nproducts_of_interest = ['Laptop', 'Monitor']\\nfiltered = sales_data[sales_data['product'].isin(products_of_interest)]\\nprint(f\\\"   Products in {products_of_interest}: {len(filtered)} records\\\")\\n\\n# Inverse (NOT in list)\\nnot_in_list = sales_data[~sales_data['product'].isin(['Tablet'])]\\nprint(f\\\"   Products NOT Tablet: {len(not_in_list)} records\\\")\\n\\n# 6. between() method\\nprint(\\\"\\\\n6. between() Method:\\\")\\n\\n# Numeric range\\nmid_range = sales_data[sales_data['sales_amount'].between(500, 1000)]\\nprint(f\\\"   Sales between $500-$1000: {len(mid_range)} records\\\")\\nprint(f\\\"   Range: ${mid_range['sales_amount'].min():.2f} - ${mid_range['sales_amount'].max():.2f}\\\")\\n\\n# Date range\\ndate_range = sales_data[sales_data['date'].between('2024-02-01', '2024-02-28')]\\nprint(f\\\"\\\\n   February 2024 sales: {len(date_range)} records\\\")\\n\\n# 7. Column selection shortcuts\\nprint(\\\"\\\\n7. Column Selection Shortcuts:\\\")\\n\\n# Single column (Series)\\nproduct_series = sales_data['product']\\nprint(f\\\"   Single column type: {type(product_series)}\\\")\\n\\n# Multiple columns (DataFrame)\\nmulti_cols = sales_data[['product', 'sales_amount']]\\nprint(f\\\"   Multiple columns type: {type(multi_cols)}\\\")\\nprint(multi_cols.head(3))\\n\\n# Drop columns\\ndropped = sales_data.drop(columns=['customer_id'])\\nprint(f\\\"\\\\n   After dropping 'customer_id': {dropped.columns.tolist()}\\\")\\n\\n# Select by data type\\nnumeric_only = sales_data.select_dtypes(include=[np.number])\\nprint(f\\\"\\\\n   Numeric columns only: {numeric_only.columns.tolist()}\\\")\\n\\n# 8. Filter with string methods\\nprint(\\\"\\\\n8. String Filtering:\\\")\\n\\n# Products starting with 'L'\\nstarts_with_l = sales_data[sales_data['product'].str.startswith('L')]\\nprint(f\\\"   Products starting with 'L': {starts_with_l['product'].unique()}\\\")\\n\\n# Case-insensitive contains\\ncontains_tab = sales_data[sales_data['product'].str.contains('tab', case=False)]\\nprint(f\\\"   Products containing 'tab': {contains_tab['product'].unique()}\\\")\\n\\n# 9. Top N and Bottom N\\nprint(\\\"\\\\n9. Top N and Bottom N Records:\\\")\\n\\n# Top 5 by sales amount\\ntop_5 = sales_data.nlargest(5, 'sales_amount')\\nprint(\\\"   Top 5 sales:\\\")\\nprint(top_5[['date', 'product', 'sales_amount']])\\n\\n# Bottom 5 by sales amount\\nbottom_5 = sales_data.nsmallest(5, 'sales_amount')\\nprint(\\\"\\\\n   Bottom 5 sales:\\\")\\nprint(bottom_5[['date', 'product', 'sales_amount']])\\n\\n# 10. Combining filters\\nprint(\\\"\\\\n10. Complex Combined Filters:\\\")\\n\\n# Multiple conditions with helper variables\\nmin_amount = 800\\nmax_amount = 1500\\nregions_of_interest = ['North', 'South']\\nproduct_list = ['Laptop', 'Phone']\\n\\ncomplex_filter = sales_data[\\n    (sales_data['sales_amount'].between(min_amount, max_amount)) &\\n    (sales_data['region'].isin(regions_of_interest)) &\\n    (sales_data['product'].isin(product_list)) &\\n    (sales_data['quantity'] > 5)\\n]\\n\\nprint(f\\\"   Complex filter results: {len(complex_filter)} records\\\")\\nprint(f\\\"   Conditions: Sales ${min_amount}-${max_amount}, \\\")\\nprint(f\\\"   Regions: {regions_of_interest}, Products: {product_list}, Quantity > 5\\\")\\nprint(complex_filter.head())\\n\\n# Summary\\nprint(\\\"\\\\n11. Filtering Summary:\\\")\\nsummary = pd.DataFrame({\\n    'Method': ['Boolean Indexing', 'loc', 'iloc', 'query()', 'isin()', 'between()', 'nlargest/nsmallest'],\\n    'Use_Case': [\\n        'Simple conditions',\\n        'Label-based selection',\\n        'Position-based selection',\\n        'SQL-like filtering',\\n        'Match against list',\\n        'Range filtering',\\n        'Top/Bottom N records'\\n    ],\\n    'Example': [\\n        'df[df[\\\"col\\\"] > 100]',\\n        'df.loc[df[\\\"col\\\"] > 100, [\\\"a\\\", \\\"b\\\"]]',\\n        'df.iloc[:10, :3]',\\n        'df.query(\\\"col > 100\\\")',\\n        'df[df[\\\"col\\\"].isin([1,2,3])]',\\n        'df[df[\\\"col\\\"].between(1, 10)]',\\n        'df.nlargest(5, \\\"col\\\")'\\n    ]\\n})\\nprint(summary.to_string(index=False))\\n\\nprint(\\\"\\\\n   âœ“ Filtering and subsetting complete!\\\")\"\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. FILTERING AND SUBSETTING DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FILTERING AND SUBSETTING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Boolean indexing\n",
    "print(\"\\n1. Boolean Indexing:\")\n",
    "\n",
    "# Single condition\n",
    "high_sales = sales_data[sales_data['sales_amount'] > 1000]\n",
    "print(f\"   Records with sales > $1000: {len(high_sales)}\")\n",
    "print(high_sales.head())\n",
    "\n",
    "# Multiple conditions (AND)\n",
    "north_high_sales = sales_data[(sales_data['region'] == 'North') & (sales_data['sales_amount'] > 1000)]\n",
    "print(f\"\\n   North region with sales > $1000: {len(north_high_sales)}\")\n",
    "\n",
    "# Multiple conditions (OR)\n",
    "laptop_or_phone = sales_data[(sales_data['product'] == 'Laptop') | (sales_data['product'] == 'Phone')]\n",
    "print(f\"\\n   Laptop or Phone sales: {len(laptop_or_phone)}\")\n",
    "\n",
    "# 2. loc - Label-based selection\n",
    "print(\"\\n2. loc (Label-based selection):\")\n",
    "\n",
    "# Select specific columns\n",
    "selected_cols = sales_data.loc[:, ['date', 'product', 'sales_amount']]\n",
    "print(\"   Selected columns:\")\n",
    "print(selected_cols.head())\n",
    "\n",
    "# Select rows and columns\n",
    "subset = sales_data.loc[0:4, ['product', 'region', 'sales_amount']]\n",
    "print(\"\\n   Rows 0-4, specific columns:\")\n",
    "print(subset)\n",
    "\n",
    "# Select with condition\n",
    "laptops = sales_data.loc[sales_data['product'] == 'Laptop', ['date', 'sales_amount', 'quantity']]\n",
    "print(f\"\\n   Laptop sales only: {len(laptops)} records\")\n",
    "\n",
    "# 3. iloc - Position-based selection\n",
    "print(\"\\n3. iloc (Position-based selection):\")\n",
    "\n",
    "# First 10 rows, first 3 columns\n",
    "first_subset = sales_data.iloc[:10, :3]\n",
    "print(\"   First 10 rows, first 3 columns:\")\n",
    "print(first_subset)\n",
    "\n",
    "# Specific rows and columns\n",
    "specific = sales_data.iloc[[0, 5, 10], [1, 3, 4]]\n",
    "print(\"\\n   Specific rows [0,5,10] and columns [1,3,4]:\")\n",
    "print(specific)\n",
    "\n",
    "# Every 10th row\n",
    "every_10th = sales_data.iloc[::10, :]\n",
    "print(f\"\\n   Every 10th row: {len(every_10th)} records\")\n",
    "\n",
    "# 4. query() method\n",
    "print(\"\\n4. query() Method (SQL-like):\")\n",
    "\n",
    "# Simple query\n",
    "query_result = sales_data.query('sales_amount > 1500 and region == \"North\"')\n",
    "print(f\"   Sales > 1500 in North: {len(query_result)} records\")\n",
    "print(query_result.head())\n",
    "\n",
    "# Using variables in query\n",
    "threshold = 1200\n",
    "query_result2 = sales_data.query('sales_amount > @threshold')\n",
    "print(f\"\\n   Sales > {threshold}: {len(query_result2)} records\")\n",
    "\n",
    "# Complex query\n",
    "complex_query = sales_data.query('product in [\"Laptop\", \"Tablet\"] and quantity >= 10')\n",
    "print(f\"\\n   Laptop/Tablet with quantity >= 10: {len(complex_query)} records\")\n",
    "\n",
    "# 5. isin() method\n",
    "print(\"\\n5. isin() Method:\")\n",
    "\n",
    "# Filter by list of values\n",
    "products_of_interest = ['Laptop', 'Monitor']\n",
    "filtered = sales_data[sales_data['product'].isin(products_of_interest)]\n",
    "print(f\"   Products in {products_of_interest}: {len(filtered)} records\")\n",
    "\n",
    "# Inverse (NOT in list)\n",
    "not_in_list = sales_data[~sales_data['product'].isin(['Tablet'])]\n",
    "print(f\"   Products NOT Tablet: {len(not_in_list)} records\")\n",
    "\n",
    "# 6. between() method\n",
    "print(\"\\n6. between() Method:\")\n",
    "\n",
    "# Numeric range\n",
    "mid_range = sales_data[sales_data['sales_amount'].between(500, 1000)]\n",
    "print(f\"   Sales between $500-$1000: {len(mid_range)} records\")\n",
    "print(f\"   Range: ${mid_range['sales_amount'].min():.2f} - ${mid_range['sales_amount'].max():.2f}\")\n",
    "\n",
    "# Date range\n",
    "date_range = sales_data[sales_data['date'].between('2024-02-01', '2024-02-28')]\n",
    "print(f\"\\n   February 2024 sales: {len(date_range)} records\")\n",
    "\n",
    "# 7. Column selection shortcuts\n",
    "print(\"\\n7. Column Selection Shortcuts:\")\n",
    "\n",
    "# Single column (Series)\n",
    "product_series = sales_data['product']\n",
    "print(f\"   Single column type: {type(product_series)}\")\n",
    "\n",
    "# Multiple columns (DataFrame)\n",
    "multi_cols = sales_data[['product', 'sales_amount']]\n",
    "print(f\"   Multiple columns type: {type(multi_cols)}\")\n",
    "print(multi_cols.head(3))\n",
    "\n",
    "# Drop columns\n",
    "dropped = sales_data.drop(columns=['customer_id'])\n",
    "print(f\"\\n   After dropping 'customer_id': {dropped.columns.tolist()}\")\n",
    "\n",
    "# Select by data type\n",
    "numeric_only = sales_data.select_dtypes(include=[np.number])\n",
    "print(f\"\\n   Numeric columns only: {numeric_only.columns.tolist()}\")\n",
    "\n",
    "# 8. Filter with string methods\n",
    "print(\"\\n8. String Filtering:\")\n",
    "\n",
    "# Products starting with 'L'\n",
    "starts_with_l = sales_data[sales_data['product'].str.startswith('L')]\n",
    "print(f\"   Products starting with 'L': {starts_with_l['product'].unique()}\")\n",
    "\n",
    "# Case-insensitive contains\n",
    "contains_tab = sales_data[sales_data['product'].str.contains('tab', case=False)]\n",
    "print(f\"   Products containing 'tab': {contains_tab['product'].unique()}\")\n",
    "\n",
    "# 9. Top N and Bottom N\n",
    "print(\"\\n9. Top N and Bottom N Records:\")\n",
    "\n",
    "# Top 5 by sales amount\n",
    "top_5 = sales_data.nlargest(5, 'sales_amount')\n",
    "print(\"   Top 5 sales:\")\n",
    "print(top_5[['date', 'product', 'sales_amount']])\n",
    "\n",
    "# Bottom 5 by sales amount\n",
    "bottom_5 = sales_data.nsmallest(5, 'sales_amount')\n",
    "print(\"\\n   Bottom 5 sales:\")\n",
    "print(bottom_5[['date', 'product', 'sales_amount']])\n",
    "\n",
    "# 10. Combining filters\n",
    "print(\"\\n10. Complex Combined Filters:\")\n",
    "\n",
    "# Multiple conditions with helper variables\n",
    "min_amount = 800\n",
    "max_amount = 1500\n",
    "regions_of_interest = ['North', 'South']\n",
    "product_list = ['Laptop', 'Phone']\n",
    "\n",
    "complex_filter = sales_data[\n",
    "    (sales_data['sales_amount'].between(min_amount, max_amount)) &\n",
    "    (sales_data['region'].isin(regions_of_interest)) &\n",
    "    (sales_data['product'].isin(product_list)) &\n",
    "    (sales_data['quantity'] > 5)\n",
    "]\n",
    "\n",
    "print(f\"   Complex filter results: {len(complex_filter)} records\")\n",
    "print(f\"   Conditions: Sales ${min_amount}-${max_amount}, \")\n",
    "print(f\"   Regions: {regions_of_interest}, Products: {product_list}, Quantity > 5\")\n",
    "print(complex_filter.head())\n",
    "\n",
    "# Summary\n",
    "print(\"\\n11. Filtering Summary:\")\n",
    "summary = pd.DataFrame({\n",
    "    'Method': ['Boolean Indexing', 'loc', 'iloc', 'query()', 'isin()', 'between()', 'nlargest/nsmallest'],\n",
    "    'Use_Case': [\n",
    "        'Simple conditions',\n",
    "        'Label-based selection',\n",
    "        'Position-based selection',\n",
    "        'SQL-like filtering',\n",
    "        'Match against list',\n",
    "        'Range filtering',\n",
    "        'Top/Bottom N records'\n",
    "    ],\n",
    "    'Example': [\n",
    "        'df[df[\"col\"] > 100]',\n",
    "        'df.loc[df[\"col\"] > 100, [\"a\", \"b\"]]',\n",
    "        'df.iloc[:10, :3]',\n",
    "        'df.query(\"col > 100\")',\n",
    "        'df[df[\"col\"].isin([1,2,3])]',\n",
    "        'df[df[\"col\"].between(1, 10)]',\n",
    "        'df.nlargest(5, \"col\")'\n",
    "    ]\n",
    "})\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Filtering and subsetting complete!\")\"\n",
    "   ],\n",
    "   \"outputs\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab443a",
   "metadata": {},
   "source": [
    "## 3. Sorting and Ranking\n",
    "\n",
    "**What is it?**\n",
    "Arranging data in a specific order based on one or more columns, and assigning ranks to values.\n",
    "\n",
    "**Why use it?**\n",
    "- Identify top/bottom performers\n",
    "- Order chronological data\n",
    "- Prepare data for certain algorithms\n",
    "- Find patterns in ordered data\n",
    "- Create rankings and leaderboards\n",
    "\n",
    "**When to use it?**\n",
    "- Need to find extremes (min/max)\n",
    "- Time series analysis\n",
    "- Creating rankings\n",
    "- Before groupby operations\n",
    "- Presenting sorted reports\n",
    "\n",
    "**Key methods:**\n",
    "- `sort_values()`: Sort by column values\n",
    "- `sort_index()`: Sort by index\n",
    "- `rank()`: Assign ranks to data\n",
    "- `nlargest()` / `nsmallest()`: Get top/bottom N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ac110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. SORTING AND RANKING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SORTING AND RANKING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Sort by single column\n",
    "print(\"\\n1. Sort by Single Column:\")\n",
    "\n",
    "# Ascending order\n",
    "sorted_asc = sales_data.sort_values('sales_amount')\n",
    "print(\"   Top 5 (lowest sales):\")\n",
    "print(sorted_asc[['date', 'product', 'sales_amount']].head())\n",
    "\n",
    "# Descending order\n",
    "sorted_desc = sales_data.sort_values('sales_amount', ascending=False)\n",
    "print(\"\\n   Top 5 (highest sales):\")\n",
    "print(sorted_desc[['date', 'product', 'sales_amount']].head())\n",
    "\n",
    "# 2. Sort by multiple columns\n",
    "print(\"\\n2. Sort by Multiple Columns:\")\n",
    "\n",
    "# Sort by region, then by sales_amount\n",
    "multi_sort = sales_data.sort_values(['region', 'sales_amount'], ascending=[True, False])\n",
    "print(\"   Sorted by region (asc), then sales_amount (desc):\")\n",
    "print(multi_sort[['region', 'product', 'sales_amount']].head(10))\n",
    "\n",
    "# 3. Sort by index\n",
    "print(\"\\n3. Sort by Index:\")\n",
    "\n",
    "# First shuffle the data\n",
    "shuffled = sales_data.sample(frac=1, random_state=42)\n",
    "print(f\"   Index before sort: {shuffled.index[:5].tolist()}\")\n",
    "\n",
    "# Sort by index\n",
    "index_sorted = shuffled.sort_index()\n",
    "print(f\"   Index after sort: {index_sorted.index[:5].tolist()}\")\n",
    "\n",
    "# 4. Sort with missing values\n",
    "print(\"\\n4. Handling Missing Values in Sorting:\")\n",
    "\n",
    "# Create data with NaN\n",
    "df_with_nan = sales_data.copy()\n",
    "df_with_nan.loc[0:5, 'sales_amount'] = np.nan\n",
    "\n",
    "# NaN first\n",
    "nan_first = df_with_nan.sort_values('sales_amount', na_position='first')\n",
    "print(f\"   NaN first - First 3 sales_amounts: {nan_first['sales_amount'].head(3).tolist()}\")\n",
    "\n",
    "# NaN last (default)\n",
    "nan_last = df_with_nan.sort_values('sales_amount', na_position='last')\n",
    "print(f\"   NaN last - Last 3 sales_amounts: {nan_last['sales_amount'].tail(3).tolist()}\")\n",
    "\n",
    "# 5. Inplace sorting\n",
    "print(\"\\n5. Inplace Sorting:\")\n",
    "\n",
    "temp_df = sales_data.copy()\n",
    "print(f\"   Before: First sales_amount = ${temp_df.iloc[0]['sales_amount']:.2f}\")\n",
    "\n",
    "temp_df.sort_values('sales_amount', ascending=False, inplace=True)\n",
    "print(f\"   After inplace sort: First sales_amount = ${temp_df.iloc[0]['sales_amount']:.2f}\")\n",
    "\n",
    "# 6. Ranking methods\n",
    "print(\"\\n6. Ranking Methods:\")\n",
    "\n",
    "# Create sample for ranking\n",
    "rank_sample = sales_data[['product', 'sales_amount']].head(10).copy()\n",
    "\n",
    "# Average rank (default)\n",
    "rank_sample['rank_average'] = rank_sample['sales_amount'].rank(method='average')\n",
    "\n",
    "# Min rank (ties get minimum rank)\n",
    "rank_sample['rank_min'] = rank_sample['sales_amount'].rank(method='min')\n",
    "\n",
    "# Dense rank (no gaps in ranking)\n",
    "rank_sample['rank_dense'] = rank_sample['sales_amount'].rank(method='dense')\n",
    "\n",
    "# First rank (order in data)\n",
    "rank_sample['rank_first'] = rank_sample['sales_amount'].rank(method='first')\n",
    "\n",
    "print(\"   Different ranking methods:\")\n",
    "print(rank_sample.sort_values('sales_amount'))\n",
    "\n",
    "# 7. Ranking with ascending parameter\n",
    "print(\"\\n7. Ranking Order (Ascending vs Descending):\")\n",
    "\n",
    "rank_example = sales_data[['product', 'sales_amount']].head(8).copy()\n",
    "\n",
    "# Ascending ranks (lowest = rank 1)\n",
    "rank_example['rank_asc'] = rank_example['sales_amount'].rank(ascending=True)\n",
    "\n",
    "# Descending ranks (highest = rank 1)\n",
    "rank_example['rank_desc'] = rank_example['sales_amount'].rank(ascending=False)\n",
    "\n",
    "print(\"   Ascending vs Descending ranks:\")\n",
    "print(rank_example.sort_values('sales_amount'))\n",
    "\n",
    "# 8. Percentage ranking\n",
    "print(\"\\n8. Percentage Ranking (Percentile):\\\")\\n\\nrank_pct = sales_data[['product', 'sales_amount']].copy()\\nrank_pct['percentile'] = rank_pct['sales_amount'].rank(pct=True) * 100\\n\\nprint(\\\"   Sales percentiles:\\\")\\nprint(rank_pct.sort_values('percentile', ascending=False).head())\\nprint(f\\\"\\\\n   Top sale is in {rank_pct['percentile'].max():.1f}th percentile\\\")\\n\\n# 9. Ranking within groups\\nprint(\\\"\\\\n9. Ranking Within Groups:\\\")\\n\\n# Rank sales within each region\\ngrouped_rank = sales_data.copy()\\ngrouped_rank['rank_in_region'] = grouped_rank.groupby('region')['sales_amount'].rank(ascending=False)\\n\\nprint(\\\"   Top sales in each region:\\\")\\ntop_per_region = grouped_rank[grouped_rank['rank_in_region'] <= 3].sort_values(['region', 'rank_in_region'])\\nprint(top_per_region[['region', 'product', 'sales_amount', 'rank_in_region']].head(12))\\n\\n# 10. Custom sorting with key parameter\n",
    "print(\"\\n10. Custom Sorting (with key function):\\\")\\n\\n# Sort products by length of name\\ncustom_sort = sales_data.copy()\\ncustom_sort_result = custom_sort.sort_values('product', key=lambda x: x.str.len())\\n\\nprint(\\\"   Products sorted by name length:\\\")\\nprint(custom_sort_result[['product']].drop_duplicates())\\n\\n# 11. Stable sorting\\nprint(\\\"\\n\\n11. Stable Sorting (preserves order for equal values):\\\")\\n\\nstable_df = sales_data.copy()\\nstable_df['original_order'] = range(len(stable_df))\\n\\n# Stable sort keeps original order for ties\\nstable_sorted = stable_df.sort_values('region', kind='stable')\\n\\nprint(\\\"   Stable sort maintains order within same region:\\\")\\nprint(stable_sorted[['original_order', 'region']].head(10))\\n\\n# 12. argsort for getting sorted indices\\nprint(\\\"\\n\\n12. Getting Sorted Indices (argsort):\\\")\\n\\narr = sales_data['sales_amount'].values[:10]\\nsorted_indices = np.argsort(arr)\\n\\nprint(f\\\"   Original values: {arr[:5]}\\\")\\nprint(f\\\"   Sorted indices: {sorted_indices[:5]}\\\")\\nprint(f\\\"   Sorted values: {arr[sorted_indices][:5]}\\\")\\n\\n# 13. Summary comparison\\nprint(\\\"\\n\\n13. Sorting Methods Comparison:\\\")\\n\\nsort_summary = pd.DataFrame({\\n    'Method': [\\n        'sort_values(col)',\\n        'sort_values([col1, col2])',\\n        'sort_index()',\\n        'rank()',\\n        'rank(pct=True)',\\n        'nlargest(n, col)',\\n        'nsmallest(n, col)'\\n    ],\\n    'Purpose': [\\n        'Sort by column values',\\n        'Sort by multiple columns',\\n        'Sort by index',\\n        'Assign ranks',\\n        'Percentile ranks',\\n        'Get top N',\\n        'Get bottom N'\\n    ],\\n    'Returns': [\\n        'Sorted DataFrame',\\n        'Sorted DataFrame',\\n        'Sorted DataFrame',\\n        'Series with ranks',\\n        'Series with percentiles',\\n        'Top N rows',\\n        'Bottom N rows'\\n    ]\\n})\\n\\nprint(sort_summary.to_string(index=False))\\n\\nprint(\\\"\\n\\n   âœ“ Sorting and ranking complete!\\\")\"\n",
    "   ],\n",
    "   \"outputs\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd57f7",
   "metadata": {},
   "source": [
    "## 4. Reshaping Data (Pivot, Melt, Stack, Unstack)\n",
    "\n",
    "**What is it?**\n",
    "Transforming data structure between wide and long formats to facilitate different types of analysis.\n",
    "\n",
    "**Why use it?**\n",
    "- Convert between analysis-friendly formats\n",
    "- Prepare data for visualization\n",
    "- Match requirements of different tools\n",
    "- Create pivot tables for summarization\n",
    "- Normalize/denormalize data structures\n",
    "\n",
    "**When to use it?**\n",
    "- Need pivot tables for reporting\n",
    "- Converting wide to long format (or vice versa)\n",
    "- Preparing data for plotting\n",
    "- Creating cross-tabulations\n",
    "- Restructuring hierarchical data\n",
    "\n",
    "**Key operations:**\n",
    "- **pivot()**: Reshape based on column values\n",
    "- **pivot_table()**: Pivot with aggregation\n",
    "- **melt()**: Wide to long format\n",
    "- **stack()**: Pivot columns to rows (MultiIndex)\n",
    "- **unstack()**: Pivot rows to columns\n",
    "- **crosstab()**: Cross-tabulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd226a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. RESHAPING DATA (PIVOT, MELT, STACK, UNSTACK)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESHAPING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample data for reshaping\n",
    "reshape_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=12, freq='M'),\n",
    "    'region': ['North', 'South'] * 6,\n",
    "    'product': ['Laptop', 'Laptop', 'Phone', 'Phone'] * 3,\n",
    "    'sales': np.random.randint(1000, 5000, 12),\n",
    "    'quantity': np.random.randint(10, 50, 12)\n",
    "})\n",
    "\n",
    "print(\"\\n0. Original Data (Long Format):\")\n",
    "print(reshape_data.head(8))\n",
    "\n",
    "# 1. pivot() - Basic reshape\n",
    "print(\"\\n1. pivot() - Reshape without aggregation:\")\n",
    "\n",
    "# Create simple data for pivot (needs unique index-column combinations)\n",
    "simple_pivot_data = pd.DataFrame({\n",
    "    'date': ['2024-01', '2024-01', '2024-02', '2024-02'],\n",
    "    'region': ['North', 'South', 'North', 'South'],\n",
    "    'sales': [1000, 1500, 1200, 1600]\n",
    "})\n",
    "\n",
    "pivoted = simple_pivot_data.pivot(index='date', columns='region', values='sales')\n",
    "print(\"\\n   Pivoted (regions as columns):\")\n",
    "print(pivoted)\n",
    "\n",
    "# 2. pivot_table() - Pivot with aggregation\n",
    "print(\"\\n2. pivot_table() - Pivot with aggregation:\")\n",
    "\n",
    "# Pivot with sum aggregation\n",
    "pivot_sum = reshape_data.pivot_table(\n",
    "    index='region',\n",
    "    columns='product',\n",
    "    values='sales',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "print(\"\\n   Total sales by region and product:\")\n",
    "print(pivot_sum)\n",
    "\n",
    "# Multiple aggregations\n",
    "pivot_multi_agg = reshape_data.pivot_table(\n",
    "    index='region',\n",
    "    columns='product',\n",
    "    values='sales',\n",
    "    aggfunc=['sum', 'mean', 'count']\n",
    ")\n",
    "\n",
    "print(\"\\n   Multiple aggregations:\")\n",
    "print(pivot_multi_agg)\n",
    "\n",
    "# 3. pivot_table with margins (totals)\n",
    "print(\"\\n3. Pivot Table with Margins (Totals):\")\n",
    "\n",
    "pivot_margins = reshape_data.pivot_table(\n",
    "    index='region',\n",
    "    columns='product',\n",
    "    values='sales',\n",
    "    aggfunc='sum',\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "\n",
    "print(pivot_margins)\n",
    "\n",
    "# 4. melt() - Wide to long format\n",
    "print(\"\\n4. melt() - Wide to Long Format:\")\n",
    "\n",
    "# Create wide format data\n",
    "wide_data = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'Jan': [1000, 1500, 800],\n",
    "    'Feb': [1200, 1600, 900],\n",
    "    'Mar': [1100, 1550, 850]\n",
    "})\n",
    "\n",
    "print(\"\\n   Wide format:\")\n",
    "print(wide_data)\n",
    "\n",
    "# Melt to long format\n",
    "melted = wide_data.melt(\n",
    "    id_vars=['product'],\n",
    "    value_vars=['Jan', 'Feb', 'Mar'],\n",
    "    var_name='month',\n",
    "    value_name='sales'\n",
    ")\n",
    "\n",
    "print(\"\\n   Melted to long format:\")\n",
    "print(melted)\n",
    "\n",
    "# 5. melt with multiple id variables\n",
    "print(\"\\n5. melt() with Multiple ID Variables:\")\n",
    "\n",
    "multi_id_data = pd.DataFrame({\n",
    "    'region': ['North', 'South'],\n",
    "    'product': ['Laptop', 'Phone'],\n",
    "    'Q1': [5000, 6000],\n",
    "    'Q2': [5500, 6200],\n",
    "    'Q3': [5200, 6100],\n",
    "    'Q4': [5800, 6500]\n",
    "})\n",
    "\n",
    "multi_melted = multi_id_data.melt(\n",
    "    id_vars=['region', 'product'],\n",
    "    var_name='quarter',\n",
    "    value_name='sales'\n",
    ")\n",
    "\n",
    "print(multi_melted)\n",
    "\n",
    "# 6. stack() - Pivot columns to index\n",
    "print(\"\\n6. stack() - Pivot Columns to Index:\")\n",
    "\n",
    "# Create DataFrame with column hierarchy\n",
    "stacked_data = pd.DataFrame({\n",
    "    ('Sales', 'North'): [1000, 1100],\n",
    "    ('Sales', 'South'): [1500, 1600],\n",
    "    ('Quantity', 'North'): [10, 12],\n",
    "    ('Quantity', 'South'): [15, 18]\n",
    "}, index=['Jan', 'Feb'])\n",
    "\n",
    "print(\"\\n   Before stack:\")\n",
    "print(stacked_data)\n",
    "\n",
    "# Stack innermost level\n",
    "stacked = stacked_data.stack()\n",
    "print(\"\\n   After stack:\")\n",
    "print(stacked)\n",
    "\n",
    "# 7. unstack() - Pivot index to columns\n",
    "print(\"\\n7. unstack() - Pivot Index to Columns:\")\n",
    "\n",
    "# Create multi-index data\n",
    "multi_index_data = pd.DataFrame({\n",
    "    'sales': [1000, 1500, 1200, 1600],\n",
    "    'quantity': [10, 15, 12, 18]\n",
    "}, index=pd.MultiIndex.from_product([['North', 'South'], ['Laptop', 'Phone']], \n",
    "                                     names=['region', 'product']))\n",
    "\n",
    "print(\"\\n   Before unstack:\")\n",
    "print(multi_index_data)\n",
    "\n",
    "# Unstack product level\n",
    "unstacked = multi_index_data.unstack(level='product')\n",
    "print(\"\\n   After unstack (product):  \")\n",
    "print(unstacked)\n",
    "\n",
    "# 8. crosstab() - Cross-tabulation\n",
    "print(\"\\n8. crosstab() - Cross-tabulation:\")\n",
    "\n",
    "# Simple crosstab\n",
    "ct = pd.crosstab(\n",
    "    reshape_data['region'],\n",
    "    reshape_data['product'],\n",
    "    values=reshape_data['sales'],\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "print(\"\\n   Crosstab (region vs product):\")\n",
    "print(ct)\n",
    "\n",
    "# Crosstab with margins and normalization\n",
    "ct_normalized = pd.crosstab(\n",
    "    reshape_data['region'],\n",
    "    reshape_data['product'],\n",
    "    values=reshape_data['sales'],\n",
    "    aggfunc='sum',\n",
    "    normalize='all',  # Normalize to percentages\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(\"\\n   Normalized crosstab (percentages):\")\n",
    "print(ct_normalized)\n",
    "\n",
    "# 9. reset_index and set_index for reshaping\n",
    "print(\"\\n9. reset_index() and set_index():\")\n",
    "\n",
    "# Pivot creates index - can reset to columns\n",
    "pivot_with_index = reshape_data.pivot_table(\n",
    "    index='region',\n",
    "    columns='product',\n",
    "    values='sales',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(\"\\n   With index:\")\n",
    "print(pivot_with_index)\n",
    "\n",
    "# Reset index to make it a column\n",
    "reset = pivot_with_index.reset_index()\n",
    "print(\"\\n   After reset_index():\")\n",
    "print(reset)\n",
    "\n",
    "# Set index back\n",
    "set_back = reset.set_index('region')\n",
    "print(\"\\n   After set_index('region'):\")\n",
    "print(set_back)\n",
    "\n",
    "# 10. Wide to long example (practical)\n",
    "print(\"\\n10. Practical Example - Survey Data Reshaping:\")\n",
    "\n",
    "# Survey data in wide format\n",
    "survey_wide = pd.DataFrame({\n",
    "    'respondent_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Q1_rating': [5, 4, 3],\n",
    "    'Q2_rating': [4, 5, 4],\n",
    "    'Q3_rating': [5, 3, 5]\n",
    "})\n",
    "\n",
    "print(\"\\n   Wide format (one row per respondent):\")\n",
    "print(survey_wide)\n",
    "\n",
    "# Convert to long format\n",
    "survey_long = survey_wide.melt(\n",
    "    id_vars=['respondent_id', 'name'],\n",
    "    value_vars=['Q1_rating', 'Q2_rating', 'Q3_rating'],\n",
    "    var_name='question',\n",
    "    value_name='rating'\n",
    ")\n",
    "\n",
    "# Clean question names\n",
    "survey_long['question'] = survey_long['question'].str.replace('_rating', '')\n",
    "\n",
    "print(\"\\n   Long format (one row per response):\")\n",
    "print(survey_long)\n",
    "\n",
    "# 11. Reshaping time series data\n",
    "print(\"\\n11. Time Series Reshaping:\")\n",
    "\n",
    "# Create time series data\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=6, freq='M'),\n",
    "    'metric': ['sales', 'costs'] * 3,\n",
    "    'value': [10000, 7000, 11000, 7500, 10500, 7200]\n",
    "})\n",
    "\n",
    "print(\"\\n   Long format time series:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Pivot to wide format\n",
    "ts_wide = ts_data.pivot(index='date', columns='metric', values='value')\n",
    "\n",
    "print(\"\\n   Wide format (metrics as columns):\")\n",
    "print(ts_wide)\n",
    "\n",
    "# 12. Summary of reshaping operations\n",
    "print(\"\\n12. Reshaping Operations Summary:\")\n",
    "\n",
    "reshape_summary = pd.DataFrame({\n",
    "    'Operation': [\n",
    "        'pivot()',\n",
    "        'pivot_table()',\n",
    "        'melt()',\n",
    "        'stack()',\n",
    "        'unstack()',\n",
    "        'crosstab()'\n",
    "    ],\n",
    "    'Direction': [\n",
    "        'Long â†’ Wide',\n",
    "        'Long â†’ Wide (with agg)',\n",
    "        'Wide â†’ Long',\n",
    "        'Columns â†’ Index',\n",
    "        'Index â†’ Columns',\n",
    "        'Cross-tabulation'\n",
    "    ],\n",
    "    'Use_Case': [\n",
    "        'Unique combinations',\n",
    "        'With aggregation',\n",
    "        'Normalize data',\n",
    "        'Create MultiIndex',\n",
    "        'Flatten MultiIndex',\n",
    "        'Frequency tables'\n",
    "    ],\n",
    "    'Example': [\n",
    "        'df.pivot(index=, columns=, values=)',\n",
    "        'df.pivot_table(index=, columns=, aggfunc=)',\n",
    "        'df.melt(id_vars=, value_vars=)',\n",
    "        'df.stack()',\n",
    "        'df.unstack()',\n",
    "        'pd.crosstab(rows, cols)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(reshape_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Reshaping complete!\")\"\n",
    "   ],\n",
    "   \"outputs\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e8836",
   "metadata": {},
   "source": [
    "## 5. Merging and Joining DataFrames\n",
    "\n",
    "**What is it?**\n",
    "Combining multiple DataFrames based on common columns or indices, similar to SQL joins.\n",
    "\n",
    "**Why use it?**\n",
    "- Integrate data from multiple sources\n",
    "- Enrich datasets with additional information\n",
    "- Combine related tables\n",
    "- Create comprehensive datasets for analysis\n",
    "- Maintain relational data integrity\n",
    "\n",
    "**When to use it?**\n",
    "- Data split across multiple files/tables\n",
    "- Need to add customer/product details to transactions\n",
    "- Combining datasets from different systems\n",
    "- Creating master datasets from normalized data\n",
    "- Matching records across sources\n",
    "\n",
    "**Join types:**\n",
    "- **inner**: Only matching records (intersection)\n",
    "- **left**: All from left + matching from right\n",
    "- **right**: All from right + matching from left\n",
    "- **outer**: All records from both (union)\n",
    "- **cross**: Cartesian product (all combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. MERGING AND JOINING DATAFRAMES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MERGING AND JOINING DATAFRAMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample datasets for merging\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4, 5],\n",
    "    'customer_id': [101, 102, 103, 101, 104],\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Laptop'],\n",
    "    'amount': [1200, 800, 500, 300, 1100]\n",
    "})\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103, 105],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "})\n",
    "\n",
    "products = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor'],\n",
    "    'category': ['Computer', 'Mobile', 'Mobile', 'Computer'],\n",
    "    'price': [1200, 800, 500, 300]\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample DataFrames:\")\n",
    "print(\"\\n   Orders:\")\n",
    "print(orders)\n",
    "print(\"\\n   Customers:\")\n",
    "print(customers)\n",
    "print(\"\\n   Products:\")\n",
    "print(products)\n",
    "\n",
    "# 1. Inner Join (intersection)\n",
    "print(\"\\n1. Inner Join (default):\")\n",
    "\n",
    "inner_join = pd.merge(orders, customers, on='customer_id', how='inner')\n",
    "print(\"\\n   Orders + Customers (inner join):\")\n",
    "print(inner_join)\n",
    "print(f\"\\n   Result: {len(inner_join)} rows (only matching customer_ids)\")\n",
    "\n",
    "# 2. Left Join\n",
    "print(\"\\n2. Left Join:\")\n",
    "\n",
    "left_join = pd.merge(orders, customers, on='customer_id', how='left')\n",
    "print(\"\\n   Orders + Customers (left join):\")\n",
    "print(left_join)\n",
    "print(f\"\\n   Result: {len(left_join)} rows (all orders, customer_id 104 has NaN)\")\n",
    "\n",
    "# 3. Right Join\n",
    "print(\"\\n3. Right Join:\")\n",
    "\n",
    "right_join = pd.merge(orders, customers, on='customer_id', how='right')\n",
    "print(\"\\n   Orders + Customers (right join):\")\n",
    "print(right_join)\n",
    "print(f\"\\n   Result: {len(right_join)} rows (all customers, Diana has no orders)\")\n",
    "\n",
    "# 4. Outer Join (union)\n",
    "print(\"\\n4. Outer Join:\")\n",
    "\n",
    "outer_join = pd.merge(orders, customers, on='customer_id', how='outer')\n",
    "print(\"\\n   Orders + Customers (outer join):\")\n",
    "print(outer_join)\n",
    "print(f\"\\n   Result: {len(outer_join)} rows (all records from both)\")\n",
    "\n",
    "# 5. Merge on multiple columns\n",
    "print(\"\\n5. Merge on Multiple Columns:\")\n",
    "\n",
    "sales1 = pd.DataFrame({\n",
    "    'region': ['North', 'South', 'East'],\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'Q1_sales': [1000, 1500, 800]\n",
    "})\n",
    "\n",
    "sales2 = pd.DataFrame({\n",
    "    'region': ['North', 'South', 'West'],\n",
    "    'product': ['Laptop', 'Phone', 'Monitor'],\n",
    "    'Q2_sales': [1100, 1600, 900]\n",
    "})\n",
    "\n",
    "multi_col_merge = pd.merge(sales1, sales2, on=['region', 'product'], how='outer')\n",
    "print(multi_col_merge)\n",
    "\n",
    "# 6. Merge with different column names\n",
    "print(\"\\n6. Merge with Different Column Names:\")\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'employee_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 4],\n",
    "    'department': ['Sales', 'Engineering', 'HR']\n",
    "})\n",
    "\n",
    "diff_names_merge = pd.merge(df1, df2, left_on='employee_id', right_on='emp_id', how='left')\n",
    "print(diff_names_merge)\n",
    "\n",
    "# 7. Merge on index\n",
    "print(\"\\n7. Merge on Index:\")\n",
    "\n",
    "df_a = pd.DataFrame({\n",
    "    'value_a': [10, 20, 30]\n",
    "}, index=['A', 'B', 'C'])\n",
    "\n",
    "df_b = pd.DataFrame({\n",
    "    'value_b': [100, 200, 400]\n",
    "}, index=['A', 'B', 'D'])\n",
    "\n",
    "index_merge = pd.merge(df_a, df_b, left_index=True, right_index=True, how='outer')\n",
    "print(index_merge)\n",
    "\n",
    "# 8. Join method (alternative to merge)\n",
    "print(\"\\n8. Join Method (Index-based):\")\n",
    "\n",
    "joined = df_a.join(df_b, how='outer')\n",
    "print(\"\\n   Using .join() method:\")\n",
    "print(joined)\n",
    "\n",
    "# Join with suffix for overlapping columns\n",
    "df_c = pd.DataFrame({\n",
    "    'value': [10, 20, 30]\n",
    "}, index=['A', 'B', 'C'])\n",
    "\n",
    "df_d = pd.DataFrame({\n",
    "    'value': [100, 200, 300]\n",
    "}, index=['A', 'B', 'C'])\n",
    "\n",
    "joined_suffix = df_c.join(df_d, lsuffix='_left', rsuffix='_right')\n",
    "print(\"\\n   Join with suffixes:\")\n",
    "print(joined_suffix)\n",
    "\n",
    "# 9. Indicator to show merge source\n",
    "print(\"\\n9. Merge with Indicator:\")\n",
    "\n",
    "merge_indicator = pd.merge(orders, customers, on='customer_id', how='outer', indicator=True)\n",
    "print(merge_indicator)\n",
    "\n",
    "print(\"\\n   Merge source counts:\")\n",
    "print(merge_indicator['_merge'].value_counts())\n",
    "\n",
    "# 10. Validate merge (check for duplicates)\n",
    "print(\"\\n10. Validate Merge:\")\n",
    "\n",
    "# This will raise error if there are duplicate keys\n",
    "try:\n",
    "    validated_merge = pd.merge(orders, customers, on='customer_id', validate='one_to_one')\n",
    "except Exception as e:\n",
    "    print(f\"   Validation error (expected): {type(e).__name__}\")\n",
    "\n",
    "# Correct validation\n",
    "validated_merge = pd.merge(orders, customers, on='customer_id', validate='many_to_one')\n",
    "print(\"   âœ“ Many-to-one validation passed\")\n",
    "\n",
    "# 11. Cross join (Cartesian product)\n",
    "print(\"\\n11. Cross Join:\")\n",
    "\n",
    "colors = pd.DataFrame({'color': ['Red', 'Blue']})\n",
    "sizes = pd.DataFrame({'size': ['S', 'M', 'L']})\n",
    "\n",
    "cross_join = pd.merge(colors, sizes, how='cross')\n",
    "print(\"\\n   All combinations (cross join):\")\n",
    "print(cross_join)\n",
    "print(f\"   Result: {len(cross_join)} rows (2 Ã— 3)\")\n",
    "\n",
    "# 12. Merge with suffixes\n",
    "print(\"\\n12. Merge with Overlapping Columns:\")\n",
    "\n",
    "sales_2023 = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone'],\n",
    "    'sales': [1000, 800],\n",
    "    'quantity': [10, 15]\n",
    "})\n",
    "\n",
    "sales_2024 = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone'],\n",
    "    'sales': [1200, 900],\n",
    "    'quantity': [12, 18]\n",
    "})\n",
    "\n",
    "suffixed_merge = pd.merge(sales_2023, sales_2024, on='product', suffixes=('_2023', '_2024'))\n",
    "print(suffixed_merge)\n",
    "\n",
    "# 13. Multiple merges (chaining)\n",
    "print(\"\\n13. Chaining Multiple Merges:\")\n",
    "\n",
    "result = (orders\n",
    "          .merge(customers, on='customer_id', how='left')\n",
    "          .merge(products, on='product', how='left'))\n",
    "\n",
    "print(\"\\n   Orders + Customers + Products:\")\n",
    "print(result[['order_id', 'name', 'product', 'category', 'amount', 'city']])\n",
    "\n",
    "# 14. Merge performance comparison\n",
    "print(\"\\n14. Merge Types Comparison:\")\n",
    "\n",
    "merge_comparison = pd.DataFrame({\n",
    "    'Join_Type': ['inner', 'left', 'right', 'outer', 'cross'],\n",
    "    'Returns': [\n",
    "        'Only matching records',\n",
    "        'All left + matching right',\n",
    "        'All right + matching left',\n",
    "        'All records from both',\n",
    "        'All combinations (Cartesian)'\n",
    "    ],\n",
    "    'Use_Case': [\n",
    "        'Need complete data only',\n",
    "        'Keep all main records',\n",
    "        'Keep all lookup records',\n",
    "        'Keep everything',\n",
    "        'Generate combinations'\n",
    "    ],\n",
    "    'Example': [\n",
    "        'Transaction + Valid customers',\n",
    "        'Customers + Optional orders',\n",
    "        'Products + Optional sales',\n",
    "        'Consolidate all data',\n",
    "        'Size Ã— Color combinations'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(merge_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Merging and joining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05f65d",
   "metadata": {},
   "source": [
    "## 6. Concatenating Data\n",
    "\n",
    "**What**: Stacking or combining multiple DataFrames along rows or columns.\n",
    "\n",
    "**Why**: \n",
    "- Combine data from multiple sources or time periods\n",
    "- Stack datasets with the same structure\n",
    "- Add new rows or columns to existing data\n",
    "\n",
    "**When to Use**:\n",
    "- Combining monthly/yearly reports\n",
    "- Merging data splits (train/test)\n",
    "- Appending new records\n",
    "- Adding features from different sources\n",
    "\n",
    "**Key Operations**:\n",
    "- **pd.concat()**: Stack along axis (rows/columns)\n",
    "- **append()**: Add rows (deprecated, use concat)\n",
    "- **ignore_index**: Reset index after concatenation\n",
    "- **keys**: Add hierarchical index to identify source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e812ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. CONCATENATING DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONCATENATING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample datasets\n",
    "q1_sales = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'sales': [1000, 800, 500],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1']\n",
    "})\n",
    "\n",
    "q2_sales = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'sales': [1200, 900, 550],\n",
    "    'quarter': ['Q2', 'Q2', 'Q2']\n",
    "})\n",
    "\n",
    "q3_sales = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'sales': [1100, 850, 520],\n",
    "    'quarter': ['Q3', 'Q3', 'Q3']\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample DataFrames:\")\n",
    "print(\"\\n   Q1 Sales:\")\n",
    "print(q1_sales)\n",
    "print(\"\\n   Q2 Sales:\")\n",
    "print(q2_sales)\n",
    "print(\"\\n   Q3 Sales:\")\n",
    "print(q3_sales)\n",
    "\n",
    "# 1. Concatenate rows (vertically)\n",
    "print(\"\\n1. Concatenate Rows (axis=0, default):\")\n",
    "\n",
    "yearly_sales = pd.concat([q1_sales, q2_sales, q3_sales])\n",
    "print(yearly_sales)\n",
    "print(f\"\\n   Result: {len(yearly_sales)} rows (3 + 3 + 3)\")\n",
    "\n",
    "# 2. Concatenate with ignore_index\n",
    "print(\"\\n2. Concatenate with Reset Index:\")\n",
    "\n",
    "yearly_sales_reset = pd.concat([q1_sales, q2_sales, q3_sales], ignore_index=True)\n",
    "print(yearly_sales_reset)\n",
    "print(\"\\n   Index reset: 0 to 8 instead of 0,1,2,0,1,2,0,1,2\")\n",
    "\n",
    "# 3. Concatenate columns (horizontally)\n",
    "print(\"\\n3. Concatenate Columns (axis=1):\")\n",
    "\n",
    "df_a = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "df_b = pd.DataFrame({\n",
    "    'C': [7, 8, 9],\n",
    "    'D': [10, 11, 12]\n",
    "})\n",
    "\n",
    "horizontal_concat = pd.concat([df_a, df_b], axis=1)\n",
    "print(horizontal_concat)\n",
    "\n",
    "# 4. Concatenate with keys (hierarchical index)\n",
    "print(\"\\n4. Concatenate with Keys (Hierarchical Index):\")\n",
    "\n",
    "keyed_concat = pd.concat([q1_sales, q2_sales, q3_sales], keys=['Q1', 'Q2', 'Q3'])\n",
    "print(keyed_concat)\n",
    "\n",
    "print(\"\\n   Access Q2 data:\")\n",
    "print(keyed_concat.loc['Q2'])\n",
    "\n",
    "# 5. Concatenate with mismatched columns\n",
    "print(\"\\n5. Concatenate with Mismatched Columns:\")\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2],\n",
    "    'B': [3, 4]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'B': [5, 6],\n",
    "    'C': [7, 8]\n",
    "})\n",
    "\n",
    "mismatched = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"\\n   Missing columns filled with NaN:\")\n",
    "print(mismatched)\n",
    "\n",
    "# 6. Concatenate with inner join (intersection)\n",
    "print(\"\\n6. Concatenate with Inner Join:\")\n",
    "\n",
    "inner_concat = pd.concat([df1, df2], ignore_index=True, join='inner')\n",
    "print(\"\\n   Only common columns (B):\")\n",
    "print(inner_concat)\n",
    "\n",
    "# 7. Concatenate series\n",
    "print(\"\\n7. Concatenate Series:\")\n",
    "\n",
    "s1 = pd.Series([1, 2, 3], name='A')\n",
    "s2 = pd.Series([4, 5, 6], name='B')\n",
    "s3 = pd.Series([7, 8, 9], name='C')\n",
    "\n",
    "series_concat = pd.concat([s1, s2, s3], axis=1)\n",
    "print(\"\\n   Series to DataFrame:\")\n",
    "print(series_concat)\n",
    "\n",
    "# 8. Concatenate with verify_integrity\n",
    "print(\"\\n8. Concatenate with Duplicate Check:\")\n",
    "\n",
    "df_dup1 = pd.DataFrame({'A': [1, 2]}, index=[0, 1])\n",
    "df_dup2 = pd.DataFrame({'A': [3, 4]}, index=[1, 2])\n",
    "\n",
    "try:\n",
    "    # This will raise error due to duplicate index\n",
    "    pd.concat([df_dup1, df_dup2], verify_integrity=True)\n",
    "except ValueError as e:\n",
    "    print(f\"   Error (expected): {e}\")\n",
    "\n",
    "print(\"\\n   âœ“ Use ignore_index=True or verify_integrity=False\")\n",
    "\n",
    "# 9. Concatenate multiple DataFrames at once\n",
    "print(\"\\n9. Concatenate Multiple DataFrames:\")\n",
    "\n",
    "jan = pd.DataFrame({'sales': [100, 200]}, index=['A', 'B'])\n",
    "feb = pd.DataFrame({'sales': [150, 250]}, index=['A', 'B'])\n",
    "mar = pd.DataFrame({'sales': [120, 220]}, index=['A', 'B'])\n",
    "\n",
    "# List of DataFrames\n",
    "monthly_data = [jan, feb, mar]\n",
    "all_months = pd.concat(monthly_data, keys=['Jan', 'Feb', 'Mar'])\n",
    "print(all_months)\n",
    "\n",
    "# 10. Concatenate with sort\n",
    "print(\"\\n10. Concatenate with Column Sorting:\")\n",
    "\n",
    "df_x = pd.DataFrame({\n",
    "    'Z': [1, 2],\n",
    "    'A': [3, 4]\n",
    "})\n",
    "\n",
    "df_y = pd.DataFrame({\n",
    "    'B': [5, 6],\n",
    "    'A': [7, 8]\n",
    "})\n",
    "\n",
    "sorted_concat = pd.concat([df_x, df_y], ignore_index=True, sort=True)\n",
    "print(\"\\n   Columns sorted alphabetically:\")\n",
    "print(sorted_concat)\n",
    "\n",
    "# 11. Practical example: Combining train/test/validation sets\n",
    "print(\"\\n11. Practical: Combine Data Splits:\")\n",
    "\n",
    "train = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3],\n",
    "    'feature2': [4, 5, 6],\n",
    "    'label': [0, 1, 0]\n",
    "})\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'feature1': [7, 8],\n",
    "    'feature2': [9, 10],\n",
    "    'label': [1, 0]\n",
    "})\n",
    "\n",
    "validation = pd.DataFrame({\n",
    "    'feature1': [11, 12],\n",
    "    'feature2': [13, 14],\n",
    "    'label': [0, 1]\n",
    "})\n",
    "\n",
    "# Combine all with dataset identifier\n",
    "all_data = pd.concat(\n",
    "    [train, test, validation],\n",
    "    keys=['train', 'test', 'validation'],\n",
    "    names=['dataset', 'row']\n",
    ")\n",
    "print(all_data)\n",
    "\n",
    "# 12. Concatenate with names\n",
    "print(\"\\n12. Concatenate with Index Names:\")\n",
    "\n",
    "df_2023 = pd.DataFrame({'value': [100, 200]})\n",
    "df_2024 = pd.DataFrame({'value': [150, 250]})\n",
    "\n",
    "named_concat = pd.concat(\n",
    "    [df_2023, df_2024],\n",
    "    keys=['2023', '2024'],\n",
    "    names=['year', 'id']\n",
    ")\n",
    "print(named_concat)\n",
    "\n",
    "# 13. Comparison: concat vs merge\n",
    "print(\"\\n13. Concat vs Merge Comparison:\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Operation': ['concat', 'merge'],\n",
    "    'Purpose': ['Stack/combine', 'Join on keys'],\n",
    "    'Alignment': ['By index/position', 'By column values'],\n",
    "    'Use_Case': ['Same structure data', 'Related data'],\n",
    "    'Example': ['Monthly reports', 'Orders + Customers']\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Concatenation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939aa4a",
   "metadata": {},
   "source": [
    "## 7. Grouping and Aggregation\n",
    "\n",
    "**What**: Splitting data into groups and applying aggregate functions to each group.\n",
    "\n",
    "**Why**: \n",
    "- Summarize data by categories\n",
    "- Calculate statistics per group\n",
    "- Identify patterns within segments\n",
    "- Generate reports and insights\n",
    "\n",
    "**When to Use**:\n",
    "- Sales by region/product/time period\n",
    "- Average metrics per category\n",
    "- Count records by group\n",
    "- Compare performance across segments\n",
    "\n",
    "**Key Operations**:\n",
    "- **groupby()**: Split data into groups\n",
    "- **agg()**: Apply multiple aggregations\n",
    "- **transform()**: Return same-shaped result\n",
    "- **filter()**: Filter groups based on condition\n",
    "- **apply()**: Custom group operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe86a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. GROUPING AND AGGREGATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GROUPING AND AGGREGATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample dataset\n",
    "sales_data = pd.DataFrame({\n",
    "    'region': ['North', 'South', 'East', 'North', 'South', 'East', 'North', 'South'],\n",
    "    'product': ['Laptop', 'Laptop', 'Phone', 'Phone', 'Tablet', 'Tablet', 'Laptop', 'Phone'],\n",
    "    'sales': [1000, 1200, 800, 900, 500, 550, 1100, 850],\n",
    "    'quantity': [10, 12, 15, 18, 8, 9, 11, 16],\n",
    "    'month': ['Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Mar', 'Mar']\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample Sales Data:\")\n",
    "print(sales_data)\n",
    "\n",
    "# 1. Simple groupby with single aggregation\n",
    "print(\"\\n1. Simple GroupBy with Single Aggregation:\")\n",
    "\n",
    "sales_by_region = sales_data.groupby('region')['sales'].sum()\n",
    "print(\"\\n   Total sales by region:\")\n",
    "print(sales_by_region)\n",
    "\n",
    "# 2. Groupby with multiple aggregations\n",
    "print(\"\\n2. Multiple Aggregations:\")\n",
    "\n",
    "region_stats = sales_data.groupby('region')['sales'].agg(['sum', 'mean', 'count', 'min', 'max'])\n",
    "print(\"\\n   Sales statistics by region:\")\n",
    "print(region_stats)\n",
    "\n",
    "# 3. Groupby multiple columns\n",
    "print(\"\\n3. GroupBy Multiple Columns:\")\n",
    "\n",
    "region_product = sales_data.groupby(['region', 'product'])['sales'].sum()\n",
    "print(\"\\n   Sales by region and product:\")\n",
    "print(region_product)\n",
    "\n",
    "# 4. Aggregation with different functions for different columns\n",
    "print(\"\\n4. Different Aggregations per Column:\")\n",
    "\n",
    "multi_agg = sales_data.groupby('region').agg({\n",
    "    'sales': ['sum', 'mean'],\n",
    "    'quantity': ['sum', 'max']\n",
    "})\n",
    "print(multi_agg)\n",
    "\n",
    "# 5. Named aggregations (cleaner output)\n",
    "print(\"\\n5. Named Aggregations:\")\n",
    "\n",
    "named_agg = sales_data.groupby('region').agg(\n",
    "    total_sales=('sales', 'sum'),\n",
    "    avg_sales=('sales', 'mean'),\n",
    "    total_quantity=('quantity', 'sum'),\n",
    "    max_quantity=('quantity', 'max')\n",
    ")\n",
    "print(named_agg)\n",
    "\n",
    "# 6. GroupBy with transform\n",
    "print(\"\\n6. Transform (Keep Original Shape):\")\n",
    "\n",
    "# Add group mean as new column\n",
    "sales_data['region_avg_sales'] = sales_data.groupby('region')['sales'].transform('mean')\n",
    "print(\"\\n   Original data with region average:\")\n",
    "print(sales_data[['region', 'sales', 'region_avg_sales']])\n",
    "\n",
    "# Calculate percentage of region total\n",
    "sales_data['pct_of_region'] = (\n",
    "    sales_data['sales'] / sales_data.groupby('region')['sales'].transform('sum') * 100\n",
    ")\n",
    "print(\"\\n   Sales as percentage of region total:\")\n",
    "print(sales_data[['region', 'product', 'sales', 'pct_of_region']].round(2))\n",
    "\n",
    "# 7. GroupBy with filter\n",
    "print(\"\\n7. Filter Groups:\")\n",
    "\n",
    "# Keep only regions with total sales > 2000\n",
    "high_sales_regions = sales_data.groupby('region').filter(lambda x: x['sales'].sum() > 2000)\n",
    "print(\"\\n   Regions with total sales > 2000:\")\n",
    "print(high_sales_regions)\n",
    "\n",
    "# 8. GroupBy with custom aggregation function\n",
    "print(\"\\n8. Custom Aggregation Function:\")\n",
    "\n",
    "def sales_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "custom_agg = sales_data.groupby('region')['sales'].agg([\n",
    "    'mean',\n",
    "    ('range', sales_range),\n",
    "    ('cv', lambda x: x.std() / x.mean())  # Coefficient of variation\n",
    "])\n",
    "print(\"\\n   Custom metrics by region:\")\n",
    "print(custom_agg)\n",
    "\n",
    "# 9. GroupBy with apply\n",
    "print(\"\\n9. Apply Custom Function to Groups:\")\n",
    "\n",
    "def top_product(group):\n",
    "    return group.nlargest(1, 'sales')[['product', 'sales']]\n",
    "\n",
    "top_per_region = sales_data.groupby('region').apply(top_product)\n",
    "print(\"\\n   Top product per region:\")\n",
    "print(top_per_region)\n",
    "\n",
    "# 10. GroupBy iteration\n",
    "print(\"\\n10. Iterate Over Groups:\")\n",
    "\n",
    "for region, group_data in sales_data.groupby('region'):\n",
    "    print(f\"\\n   {region}:\")\n",
    "    print(f\"   Total sales: ${group_data['sales'].sum():,}\")\n",
    "    print(f\"   Products: {group_data['product'].nunique()}\")\n",
    "\n",
    "# 11. Multiple levels of grouping\n",
    "print(\"\\n11. Multiple Grouping Levels:\")\n",
    "\n",
    "monthly_summary = sales_data.groupby(['month', 'region']).agg({\n",
    "    'sales': 'sum',\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "print(monthly_summary)\n",
    "\n",
    "# 12. Unstack for better visualization\n",
    "print(\"\\n12. Unstack Grouped Data:\")\n",
    "\n",
    "pivot_summary = sales_data.groupby(['region', 'product'])['sales'].sum().unstack(fill_value=0)\n",
    "print(\"\\n   Sales matrix (Region Ã— Product):\")\n",
    "print(pivot_summary)\n",
    "\n",
    "# 13. GroupBy with size and count\n",
    "print(\"\\n13. Size vs Count:\")\n",
    "\n",
    "print(\"\\n   Size (includes NaN):\")\n",
    "print(sales_data.groupby('region').size())\n",
    "\n",
    "print(\"\\n   Count per column (excludes NaN):\")\n",
    "print(sales_data.groupby('region').count())\n",
    "\n",
    "# 14. Groupby with percentiles\n",
    "print(\"\\n14. Percentile Aggregations:\")\n",
    "\n",
    "percentile_agg = sales_data.groupby('region')['sales'].quantile([0.25, 0.5, 0.75])\n",
    "print(percentile_agg)\n",
    "\n",
    "# 15. Practical example: Sales performance report\n",
    "print(\"\\n15. Practical: Comprehensive Sales Report:\")\n",
    "\n",
    "report = sales_data.groupby('region').agg(\n",
    "    total_sales=('sales', 'sum'),\n",
    "    avg_sale=('sales', 'mean'),\n",
    "    min_sale=('sales', 'min'),\n",
    "    max_sale=('sales', 'max'),\n",
    "    total_quantity=('quantity', 'sum'),\n",
    "    num_transactions=('sales', 'count'),\n",
    "    unique_products=('product', 'nunique')\n",
    ").round(2)\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Add rankings\n",
    "report['sales_rank'] = report['total_sales'].rank(ascending=False)\n",
    "print(\"\\n   With rankings:\")\n",
    "print(report.sort_values('total_sales', ascending=False))\n",
    "\n",
    "print(\"\\n   âœ“ Grouping and aggregation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca03da",
   "metadata": {},
   "source": [
    "## 8. Data Transformation (Apply, Map, Replace)\n",
    "\n",
    "**What**: Applying functions to modify data values in DataFrames and Series.\n",
    "\n",
    "**Why**: \n",
    "- Transform data according to business logic\n",
    "- Create new features from existing columns\n",
    "- Clean and standardize values\n",
    "- Perform element-wise operations\n",
    "\n",
    "**When to Use**:\n",
    "- Creating derived features\n",
    "- Data cleaning and normalization\n",
    "- Category mapping and encoding\n",
    "- Custom calculations\n",
    "\n",
    "**Key Operations**:\n",
    "- **apply()**: Apply function along axis (row/column)\n",
    "- **map()**: Map values element-wise (Series only)\n",
    "- **applymap()**: Apply function to every element (deprecated, use map)\n",
    "- **replace()**: Replace specific values\n",
    "- **Lambda functions**: Quick inline transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. DATA TRANSFORMATION (APPLY, MAP, REPLACE)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA TRANSFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample dataset\n",
    "transform_data = pd.DataFrame({\n",
    "    'name': ['alice', 'bob', 'charlie', 'david'],\n",
    "    'age': [25, 30, 35, 28],\n",
    "    'salary': [50000, 60000, 75000, 55000],\n",
    "    'department': ['sales', 'IT', 'sales', 'IT'],\n",
    "    'performance': ['good', 'excellent', 'average', 'good']\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample Data:\")\n",
    "print(transform_data)\n",
    "\n",
    "# 1. Apply with lambda (column-wise)\n",
    "print(\"\\n1. Apply Lambda to Column:\")\n",
    "\n",
    "transform_data['salary_k'] = transform_data['salary'].apply(lambda x: x / 1000)\n",
    "print(\"\\n   Salary in thousands:\")\n",
    "print(transform_data[['name', 'salary', 'salary_k']])\n",
    "\n",
    "# 2. Apply custom function\n",
    "print(\"\\n2. Apply Custom Function:\")\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age < 25:\n",
    "        return 'Junior'\n",
    "    elif age < 35:\n",
    "        return 'Mid-level'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "transform_data['age_category'] = transform_data['age'].apply(categorize_age)\n",
    "print(transform_data[['name', 'age', 'age_category']])\n",
    "\n",
    "# 3. Apply to multiple columns (row-wise)\n",
    "print(\"\\n3. Apply to Rows (axis=1):\")\n",
    "\n",
    "def total_compensation(row):\n",
    "    bonus = {'good': 5000, 'excellent': 10000, 'average': 2000}\n",
    "    return row['salary'] + bonus.get(row['performance'], 0)\n",
    "\n",
    "transform_data['total_comp'] = transform_data.apply(total_compensation, axis=1)\n",
    "print(transform_data[['name', 'salary', 'performance', 'total_comp']])\n",
    "\n",
    "# 4. Map with dictionary\n",
    "print(\"\\n4. Map with Dictionary:\")\n",
    "\n",
    "dept_mapping = {\n",
    "    'sales': 'Sales Department',\n",
    "    'IT': 'Information Technology',\n",
    "    'HR': 'Human Resources'\n",
    "}\n",
    "\n",
    "transform_data['dept_full'] = transform_data['department'].map(dept_mapping)\n",
    "print(transform_data[['name', 'department', 'dept_full']])\n",
    "\n",
    "# 5. Map with Series\n",
    "print(\"\\n5. Map with Series:\")\n",
    "\n",
    "dept_budget = pd.Series({\n",
    "    'sales': 100000,\n",
    "    'IT': 150000,\n",
    "    'HR': 80000\n",
    "})\n",
    "\n",
    "transform_data['dept_budget'] = transform_data['department'].map(dept_budget)\n",
    "print(transform_data[['name', 'department', 'dept_budget']])\n",
    "\n",
    "# 6. Map with function\n",
    "print(\"\\n6. Map with Function:\")\n",
    "\n",
    "transform_data['name_upper'] = transform_data['name'].map(str.upper)\n",
    "print(transform_data[['name', 'name_upper']])\n",
    "\n",
    "# 7. DataFrame map (element-wise, new in pandas 2.1)\n",
    "print(\"\\n7. DataFrame Map (Element-wise):\")\n",
    "\n",
    "numeric_data = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "# Square all values\n",
    "squared = numeric_data.map(lambda x: x ** 2)\n",
    "print(\"\\n   Original:\")\n",
    "print(numeric_data)\n",
    "print(\"\\n   Squared:\")\n",
    "print(squared)\n",
    "\n",
    "# 8. Replace single value\n",
    "print(\"\\n8. Replace Single Value:\")\n",
    "\n",
    "replaced = transform_data['performance'].replace('average', 'satisfactory')\n",
    "print(\"\\n   Performance ratings:\")\n",
    "print(replaced)\n",
    "\n",
    "# 9. Replace multiple values\n",
    "print(\"\\n9. Replace Multiple Values:\")\n",
    "\n",
    "perf_replace = transform_data['performance'].replace({\n",
    "    'good': 'Good',\n",
    "    'excellent': 'Excellent',\n",
    "    'average': 'Average'\n",
    "})\n",
    "print(perf_replace)\n",
    "\n",
    "# 10. Replace with regex\n",
    "print(\"\\n10. Replace with Regex:\")\n",
    "\n",
    "text_data = pd.Series(['test-123', 'data-456', 'info-789'])\n",
    "cleaned = text_data.replace(r'-\\d+', '', regex=True)\n",
    "print(\"\\n   Original:\", text_data.tolist())\n",
    "print(\"   Cleaned:\", cleaned.tolist())\n",
    "\n",
    "# 11. Conditional replacement (where)\n",
    "print(\"\\n11. Conditional Replacement (where/mask):\")\n",
    "\n",
    "# Replace salary > 60000 with 60000 (cap)\n",
    "capped_salary = transform_data['salary'].where(transform_data['salary'] <= 60000, 60000)\n",
    "print(\"\\n   Capped salary:\")\n",
    "print(pd.DataFrame({'original': transform_data['salary'], 'capped': capped_salary}))\n",
    "\n",
    "# 12. Apply with multiple return values\n",
    "print(\"\\n12. Apply Returning Multiple Values:\")\n",
    "\n",
    "def salary_stats(salary):\n",
    "    return pd.Series({\n",
    "        'tax': salary * 0.25,\n",
    "        'net': salary * 0.75\n",
    "    })\n",
    "\n",
    "tax_net = transform_data['salary'].apply(salary_stats)\n",
    "print(tax_net)\n",
    "\n",
    "# 13. Vectorized operations vs apply\n",
    "print(\"\\n13. Vectorized Operations (Preferred):\")\n",
    "\n",
    "# Using apply (slower)\n",
    "result_apply = transform_data['salary'].apply(lambda x: x * 1.1)\n",
    "\n",
    "# Using vectorized operation (faster)\n",
    "result_vectorized = transform_data['salary'] * 1.1\n",
    "\n",
    "print(\"\\n   Both give same result:\")\n",
    "print(\"   Apply:\", result_apply.head(2).tolist())\n",
    "print(\"   Vectorized:\", result_vectorized.head(2).tolist())\n",
    "print(\"   âœ“ Use vectorized when possible (much faster for large data)\")\n",
    "\n",
    "# 14. Practical: Feature engineering\n",
    "print(\"\\n14. Practical: Feature Engineering:\")\n",
    "\n",
    "# Create multiple derived features\n",
    "transform_data['salary_per_year_of_age'] = transform_data['salary'] / transform_data['age']\n",
    "transform_data['is_high_earner'] = transform_data['salary'] > transform_data['salary'].median()\n",
    "transform_data['dept_code'] = transform_data['department'].astype('category').cat.codes\n",
    "\n",
    "print(transform_data[['name', 'salary_per_year_of_age', 'is_high_earner', 'dept_code']])\n",
    "\n",
    "# 15. Chaining transformations\n",
    "print(\"\\n15. Chaining Transformations:\")\n",
    "\n",
    "result = (transform_data['name']\n",
    "          .str.upper()\n",
    "          .str.replace('A', '@')\n",
    "          .str[:5])\n",
    "\n",
    "print(\"\\n   Chained string transformations:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n   âœ“ Data transformation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b25e9",
   "metadata": {},
   "source": [
    "## 9. Handling Time Series Data\n",
    "\n",
    "**What**: Working with date and time data, including parsing, indexing, and time-based operations.\n",
    "\n",
    "**Why**: \n",
    "- Analyze temporal patterns and trends\n",
    "- Perform time-based calculations\n",
    "- Resample and aggregate time series data\n",
    "- Handle date ranges and periods\n",
    "\n",
    "**When to Use**:\n",
    "- Stock price analysis\n",
    "- Sales forecasting\n",
    "- Event tracking and scheduling\n",
    "- IoT sensor data analysis\n",
    "\n",
    "**Key Operations**:\n",
    "- **pd.to_datetime()**: Convert to datetime\n",
    "- **dt accessor**: Access datetime properties\n",
    "- **date_range()**: Generate date sequences\n",
    "- **resample()**: Change time frequency\n",
    "- **shift()**: Lag/lead time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781cec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. HANDLING TIME SERIES DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HANDLING TIME SERIES DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Create datetime from strings\n",
    "print(\"\\n1. Convert Strings to Datetime:\")\n",
    "\n",
    "date_strings = ['2024-01-01', '2024-02-15', '2024-03-30']\n",
    "dates = pd.to_datetime(date_strings)\n",
    "print(dates)\n",
    "\n",
    "# Different formats\n",
    "mixed_formats = ['01/15/2024', '2024-02-20', 'March 10, 2024']\n",
    "parsed_dates = pd.to_datetime(mixed_formats, format='mixed')\n",
    "print(\"\\n   Mixed formats:\")\n",
    "print(parsed_dates)\n",
    "\n",
    "# 2. DateTime properties\n",
    "print(\"\\n2. Extract DateTime Components:\")\n",
    "\n",
    "sample_dates = pd.date_range('2024-01-01', periods=5, freq='D')\n",
    "df_dates = pd.DataFrame({'date': sample_dates})\n",
    "\n",
    "df_dates['year'] = df_dates['date'].dt.year\n",
    "df_dates['month'] = df_dates['date'].dt.month\n",
    "df_dates['day'] = df_dates['date'].dt.day\n",
    "df_dates['day_name'] = df_dates['date'].dt.day_name()\n",
    "df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "\n",
    "print(df_dates)\n",
    "\n",
    "# 3. Generate date ranges\n",
    "print(\"\\n3. Generate Date Ranges:\")\n",
    "\n",
    "# Daily\n",
    "daily = pd.date_range('2024-01-01', periods=7, freq='D')\n",
    "print(f\"\\n   Daily: {daily[0]} to {daily[-1]}\")\n",
    "\n",
    "# Business days\n",
    "business_days = pd.date_range('2024-01-01', periods=5, freq='B')\n",
    "print(f\"   Business days: {len(business_days)} days\")\n",
    "\n",
    "# Monthly\n",
    "monthly = pd.date_range('2024-01-01', periods=6, freq='MS')  # Month start\n",
    "print(f\"   Monthly: {monthly[0]} to {monthly[-1]}\")\n",
    "\n",
    "# Hourly\n",
    "hourly = pd.date_range('2024-01-01', periods=24, freq='h')\n",
    "print(f\"   Hourly: {len(hourly)} hours\")\n",
    "\n",
    "# 4. Time series with datetime index\n",
    "print(\"\\n4. Time Series with DatetimeIndex:\")\n",
    "\n",
    "ts_data = pd.DataFrame({\n",
    "    'sales': np.random.randint(100, 200, 10)\n",
    "}, index=pd.date_range('2024-01-01', periods=10, freq='D'))\n",
    "\n",
    "print(ts_data.head())\n",
    "print(f\"\\n   Index type: {type(ts_data.index)}\")\n",
    "\n",
    "# 5. Selecting by date\n",
    "print(\"\\n5. Select by Date:\")\n",
    "\n",
    "print(\"\\n   All January data:\")\n",
    "print(ts_data['2024-01'])\n",
    "\n",
    "print(\"\\n   Specific date:\")\n",
    "print(ts_data.loc['2024-01-05'])\n",
    "\n",
    "# 6. Resampling (change frequency)\n",
    "print(\"\\n6. Resample Time Series:\")\n",
    "\n",
    "# Create sample time series\n",
    "ts = pd.DataFrame({\n",
    "    'value': np.random.randn(30)\n",
    "}, index=pd.date_range('2024-01-01', periods=30, freq='D'))\n",
    "\n",
    "# Resample to weekly (sum)\n",
    "weekly = ts.resample('W').sum()\n",
    "print(\"\\n   Daily to Weekly (sum):\")\n",
    "print(weekly.head())\n",
    "\n",
    "# Resample to weekly (mean)\n",
    "weekly_mean = ts.resample('W').mean()\n",
    "print(\"\\n   Daily to Weekly (mean):\")\n",
    "print(weekly_mean.head())\n",
    "\n",
    "# 7. Shifting (lag/lead)\n",
    "print(\"\\n7. Shift Time Series:\")\n",
    "\n",
    "shift_data = pd.DataFrame({\n",
    "    'value': [10, 20, 30, 40, 50]\n",
    "}, index=pd.date_range('2024-01-01', periods=5, freq='D'))\n",
    "\n",
    "shift_data['prev_day'] = shift_data['value'].shift(1)  # Lag\n",
    "shift_data['next_day'] = shift_data['value'].shift(-1)  # Lead\n",
    "\n",
    "print(shift_data)\n",
    "\n",
    "# 8. Calculate changes\n",
    "print(\"\\n8. Calculate Changes:\")\n",
    "\n",
    "shift_data['daily_change'] = shift_data['value'].diff()\n",
    "shift_data['pct_change'] = shift_data['value'].pct_change() * 100\n",
    "\n",
    "print(shift_data)\n",
    "\n",
    "# 9. Rolling window calculations\n",
    "print(\"\\n9. Rolling Window (Moving Average):\")\n",
    "\n",
    "rolling_data = pd.DataFrame({\n",
    "    'value': [10, 15, 12, 18, 20, 17, 22, 25]\n",
    "}, index=pd.date_range('2024-01-01', periods=8, freq='D'))\n",
    "\n",
    "rolling_data['ma_3'] = rolling_data['value'].rolling(window=3).mean()\n",
    "rolling_data['ma_5'] = rolling_data['value'].rolling(window=5).mean()\n",
    "\n",
    "print(rolling_data)\n",
    "\n",
    "# 10. Time zones\n",
    "print(\"\\n10. Time Zones:\")\n",
    "\n",
    "# Create timezone-aware datetime\n",
    "utc_time = pd.date_range('2024-01-01', periods=3, freq='h', tz='UTC')\n",
    "print(\"\\n   UTC times:\")\n",
    "print(utc_time)\n",
    "\n",
    "# Convert to different timezone\n",
    "ny_time = utc_time.tz_convert('America/New_York')\n",
    "print(\"\\n   New York times:\")\n",
    "print(ny_time)\n",
    "\n",
    "# 11. Date arithmetic\n",
    "print(\"\\n11. Date Arithmetic:\")\n",
    "\n",
    "start_date = pd.Timestamp('2024-01-01')\n",
    "print(f\"\\n   Start: {start_date}\")\n",
    "print(f\"   + 10 days: {start_date + pd.Timedelta(days=10)}\")\n",
    "print(f\"   + 2 weeks: {start_date + pd.Timedelta(weeks=2)}\")\n",
    "print(f\"   + 3 months: {start_date + pd.DateOffset(months=3)}\")\n",
    "\n",
    "# 12. Period objects\n",
    "print(\"\\n12. Period Objects:\")\n",
    "\n",
    "periods = pd.period_range('2024-01', periods=12, freq='M')\n",
    "print(\"\\n   Monthly periods:\")\n",
    "print(periods[:6])\n",
    "\n",
    "# 13. Business day operations\n",
    "print(\"\\n13. Business Day Calculations:\")\n",
    "\n",
    "# Add 10 business days\n",
    "from pandas.tseries.offsets import BDay\n",
    "biz_date = pd.Timestamp('2024-01-01') + 10 * BDay()\n",
    "print(f\"\\n   10 business days from 2024-01-01: {biz_date}\")\n",
    "\n",
    "# 14. Practical: Time series analysis\n",
    "print(\"\\n14. Practical: Sales Analysis:\")\n",
    "\n",
    "# Generate sample sales data\n",
    "sales_ts = pd.DataFrame({\n",
    "    'sales': np.random.randint(1000, 2000, 90)\n",
    "}, index=pd.date_range('2024-01-01', periods=90, freq='D'))\n",
    "\n",
    "# Add features\n",
    "sales_ts['day_of_week'] = sales_ts.index.day_name()\n",
    "sales_ts['is_weekend'] = sales_ts.index.dayofweek >= 5\n",
    "sales_ts['week'] = sales_ts.index.isocalendar().week\n",
    "\n",
    "# Weekly summary\n",
    "weekly_summary = sales_ts.resample('W').agg({\n",
    "    'sales': ['sum', 'mean', 'max']\n",
    "})\n",
    "\n",
    "print(\"\\n   Weekly sales summary:\")\n",
    "print(weekly_summary.head())\n",
    "\n",
    "print(\"\\n   âœ“ Time series handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925c321",
   "metadata": {},
   "source": [
    "## 10. Working with MultiIndex\n",
    "\n",
    "**What**: DataFrames with hierarchical (multi-level) row or column indices.\n",
    "\n",
    "**Why**: \n",
    "- Represent higher-dimensional data\n",
    "- Organize complex datasets efficiently\n",
    "- Enable advanced grouping and aggregation\n",
    "- Simplify pivot and cross-tabulation\n",
    "\n",
    "**When to Use**:\n",
    "- Grouped time series data\n",
    "- Multi-dimensional analysis\n",
    "- Hierarchical data structures\n",
    "- Panel data (cross-section + time)\n",
    "\n",
    "**Key Operations**:\n",
    "- **set_index()**: Create MultiIndex\n",
    "- **xs()**: Cross-section selection\n",
    "- **swaplevel()**: Swap index levels\n",
    "- **unstack()**: Pivot level to columns\n",
    "- **reset_index()**: Flatten MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. WORKING WITH MULTIINDEX\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WORKING WITH MULTIINDEX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Create MultiIndex from lists\n",
    "print(\"\\n1. Create MultiIndex from Lists:\")\n",
    "\n",
    "arrays = [\n",
    "    ['A', 'A', 'B', 'B'],\n",
    "    ['one', 'two', 'one', 'two']\n",
    "]\n",
    "index = pd.MultiIndex.from_arrays(arrays, names=['letter', 'number'])\n",
    "df_multi = pd.DataFrame({'value': [10, 20, 30, 40]}, index=index)\n",
    "\n",
    "print(df_multi)\n",
    "\n",
    "# 2. Create MultiIndex from tuples\n",
    "print(\"\\n2. Create MultiIndex from Tuples:\")\n",
    "\n",
    "tuples = [('A', 'one'), ('A', 'two'), ('B', 'one'), ('B', 'two')]\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=['letter', 'number'])\n",
    "df_tuples = pd.DataFrame({'value': [10, 20, 30, 40]}, index=index)\n",
    "\n",
    "print(df_tuples)\n",
    "\n",
    "# 3. Create MultiIndex from product\n",
    "print(\"\\n3. Create MultiIndex from Product:\")\n",
    "\n",
    "# Cartesian product\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [['A', 'B'], ['one', 'two', 'three']],\n",
    "    names=['letter', 'number']\n",
    ")\n",
    "df_product = pd.DataFrame({'value': range(6)}, index=index)\n",
    "\n",
    "print(df_product)\n",
    "\n",
    "# 4. Set MultiIndex from columns\n",
    "print(\"\\n4. Set MultiIndex from Columns:\")\n",
    "\n",
    "df_simple = pd.DataFrame({\n",
    "    'region': ['North', 'North', 'South', 'South'],\n",
    "    'product': ['Laptop', 'Phone', 'Laptop', 'Phone'],\n",
    "    'sales': [1000, 800, 1200, 900]\n",
    "})\n",
    "\n",
    "df_indexed = df_simple.set_index(['region', 'product'])\n",
    "print(df_indexed)\n",
    "\n",
    "# 5. Selecting with MultiIndex\n",
    "print(\"\\n5. Select with MultiIndex:\")\n",
    "\n",
    "# Select outer level\n",
    "print(\"\\n   All North region:\")\n",
    "print(df_indexed.loc['North'])\n",
    "\n",
    "# Select specific tuple\n",
    "print(\"\\n   North + Laptop:\")\n",
    "print(df_indexed.loc[('North', 'Laptop')])\n",
    "\n",
    "# Select with slice\n",
    "print(\"\\n   North, all products:\")\n",
    "print(df_indexed.loc[('North', slice(None)), :])\n",
    "\n",
    "# 6. Cross-section (xs)\n",
    "print(\"\\n6. Cross-Section Selection:\")\n",
    "\n",
    "# Get all 'Laptop' across regions\n",
    "laptops = df_indexed.xs('Laptop', level='product')\n",
    "print(\"\\n   All Laptop sales:\")\n",
    "print(laptops)\n",
    "\n",
    "# 7. Swap levels\n",
    "print(\"\\n7. Swap Index Levels:\")\n",
    "\n",
    "swapped = df_indexed.swaplevel('region', 'product')\n",
    "print(swapped)\n",
    "\n",
    "# Sort by new index\n",
    "print(\"\\n   Sorted by product then region:\")\n",
    "print(swapped.sort_index())\n",
    "\n",
    "# 8. Unstack (pivot level to columns)\n",
    "print(\"\\n8. Unstack MultiIndex:\")\n",
    "\n",
    "unstacked = df_indexed.unstack(level='product')\n",
    "print(\"\\n   Products as columns:\")\n",
    "print(unstacked)\n",
    "\n",
    "# Unstack different level\n",
    "unstacked_region = df_indexed.unstack(level='region')\n",
    "print(\"\\n   Regions as columns:\")\n",
    "print(unstacked_region)\n",
    "\n",
    "# 9. Stack (columns to index)\n",
    "print(\"\\n9. Stack Back:\")\n",
    "\n",
    "stacked = unstacked.stack()\n",
    "print(stacked)\n",
    "\n",
    "# 10. Reset index (flatten)\n",
    "print(\"\\n10. Reset Index to Columns:\")\n",
    "\n",
    "flattened = df_indexed.reset_index()\n",
    "print(flattened)\n",
    "\n",
    "# 11. Aggregation with MultiIndex\n",
    "print(\"\\n11. Aggregation with MultiIndex:\")\n",
    "\n",
    "# Create sample hierarchical data\n",
    "multi_data = pd.DataFrame({\n",
    "    'region': ['North', 'North', 'North', 'South', 'South', 'South'],\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone', 'Tablet'],\n",
    "    'month': ['Jan', 'Jan', 'Jan', 'Jan', 'Jan', 'Jan'],\n",
    "    'sales': [1000, 800, 500, 1200, 900, 550]\n",
    "}).set_index(['region', 'product', 'month'])\n",
    "\n",
    "print(multi_data)\n",
    "\n",
    "# Aggregate at different levels\n",
    "print(\"\\n   Sum by region:\")\n",
    "print(multi_data.sum(level='region'))\n",
    "\n",
    "print(\"\\n   Mean by product:\")\n",
    "print(multi_data.mean(level='product'))\n",
    "\n",
    "# 12. Sorting MultiIndex\n",
    "print(\"\\n12. Sort MultiIndex:\")\n",
    "\n",
    "# Sort by all levels\n",
    "sorted_multi = multi_data.sort_index()\n",
    "print(\"\\n   Sorted by all levels:\")\n",
    "print(sorted_multi)\n",
    "\n",
    "# Sort by specific level\n",
    "sorted_level = multi_data.sort_index(level='product')\n",
    "print(\"\\n   Sorted by product level:\")\n",
    "print(sorted_level)\n",
    "\n",
    "# 13. MultiIndex columns\n",
    "print(\"\\n13. MultiIndex Columns:\")\n",
    "\n",
    "# Create DataFrame with MultiIndex columns\n",
    "col_index = pd.MultiIndex.from_product(\n",
    "    [['Sales', 'Quantity'], ['Q1', 'Q2']],\n",
    "    names=['Metric', 'Quarter']\n",
    ")\n",
    "\n",
    "df_multi_col = pd.DataFrame(\n",
    "    np.random.randint(100, 200, (3, 4)),\n",
    "    index=['Product A', 'Product B', 'Product C'],\n",
    "    columns=col_index\n",
    ")\n",
    "\n",
    "print(df_multi_col)\n",
    "\n",
    "# Select specific column level\n",
    "print(\"\\n   All Q1 data:\")\n",
    "print(df_multi_col.xs('Q1', level='Quarter', axis=1))\n",
    "\n",
    "# 14. Practical: Hierarchical time series\n",
    "print(\"\\n14. Practical: Regional Sales Time Series:\")\n",
    "\n",
    "# Create hierarchical time series\n",
    "dates = pd.date_range('2024-01-01', periods=6, freq='MS')\n",
    "regions = ['North', 'South']\n",
    "products = ['Laptop', 'Phone']\n",
    "\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [dates, regions, products],\n",
    "    names=['date', 'region', 'product']\n",
    ")\n",
    "\n",
    "hierarchical_ts = pd.DataFrame({\n",
    "    'sales': np.random.randint(500, 1500, len(index))\n",
    "}, index=index)\n",
    "\n",
    "print(hierarchical_ts.head(10))\n",
    "\n",
    "# Aggregate by month and region\n",
    "monthly_regional = hierarchical_ts.sum(level=['date', 'region'])\n",
    "print(\"\\n   Monthly sales by region:\")\n",
    "print(monthly_regional.head())\n",
    "\n",
    "print(\"\\n   âœ“ MultiIndex operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82931d",
   "metadata": {},
   "source": [
    "## 11. String Operations\n",
    "\n",
    "**What**: Text manipulation and pattern matching using pandas string accessor.\n",
    "\n",
    "**Why**: \n",
    "- Clean and standardize text data\n",
    "- Extract information from strings\n",
    "- Parse unstructured text\n",
    "- Prepare text for analysis\n",
    "\n",
    "**When to Use**:\n",
    "- Name/address cleaning\n",
    "- Extracting codes or IDs\n",
    "- Text categorization\n",
    "- URL/email parsing\n",
    "\n",
    "**Key Operations**:\n",
    "- **str.lower/upper()**: Case conversion\n",
    "- **str.contains()**: Pattern matching\n",
    "- **str.extract()**: Regex extraction\n",
    "- **str.split()**: Split strings\n",
    "- **str.replace()**: Replace patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 11. STRING OPERATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRING OPERATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample text data\n",
    "string_data = pd.DataFrame({\n",
    "    'name': ['Alice Smith', 'Bob JONES', '  charlie brown', 'Diana Prince  '],\n",
    "    'email': ['alice@example.com', 'BOB@TEST.COM', 'charlie@demo.org', 'diana@company.net'],\n",
    "    'phone': ['(123) 456-7890', '987-654-3210', '555.123.4567', '800-555-1234'],\n",
    "    'code': ['ABC-123', 'XYZ-456', 'DEF-789', 'GHI-012']\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample Text Data:\")\n",
    "print(string_data)\n",
    "\n",
    "# 1. Case conversion\n",
    "print(\"\\n1. Case Conversion:\")\n",
    "\n",
    "string_data['name_lower'] = string_data['name'].str.lower()\n",
    "string_data['name_upper'] = string_data['name'].str.upper()\n",
    "string_data['name_title'] = string_data['name'].str.title()\n",
    "\n",
    "print(string_data[['name', 'name_lower', 'name_upper', 'name_title']].head(2))\n",
    "\n",
    "# 2. Strip whitespace\n",
    "print(\"\\n2. Strip Whitespace:\")\n",
    "\n",
    "cleaned_names = string_data['name'].str.strip()\n",
    "print(\"\\n   Before strip:\")\n",
    "print(string_data['name'].tolist())\n",
    "print(\"\\n   After strip:\")\n",
    "print(cleaned_names.tolist())\n",
    "\n",
    "# 3. Contains (pattern matching)\n",
    "print(\"\\n3. String Contains:\")\n",
    "\n",
    "has_smith = string_data['name'].str.contains('Smith', case=False)\n",
    "print(\"\\n   Names containing 'Smith':\")\n",
    "print(string_data[has_smith]['name'])\n",
    "\n",
    "# Regex pattern\n",
    "has_numbers = string_data['phone'].str.contains(r'\\d{3}', regex=True)\n",
    "print(\"\\n   Phones with 3+ digits:\", has_numbers.all())\n",
    "\n",
    "# 4. Starts with / Ends with\n",
    "print(\"\\n4. Starts With / Ends With:\")\n",
    "\n",
    "starts_with = string_data['code'].str.startswith('ABC')\n",
    "print(\"\\n   Codes starting with ABC:\")\n",
    "print(string_data[starts_with]['code'])\n",
    "\n",
    "ends_with = string_data['email'].str.endswith('.com')\n",
    "print(\"\\n   .com emails:\")\n",
    "print(string_data[ends_with]['email'])\n",
    "\n",
    "# 5. Split strings\n",
    "print(\"\\n5. Split Strings:\")\n",
    "\n",
    "# Split by space\n",
    "name_parts = string_data['name'].str.strip().str.split(' ', expand=True)\n",
    "name_parts.columns = ['first_name', 'last_name']\n",
    "print(name_parts)\n",
    "\n",
    "# Split email\n",
    "email_parts = string_data['email'].str.split('@', expand=True)\n",
    "email_parts.columns = ['username', 'domain']\n",
    "print(\"\\n   Email parts:\")\n",
    "print(email_parts)\n",
    "\n",
    "# 6. Extract with regex\n",
    "print(\"\\n6. Extract with Regex:\")\n",
    "\n",
    "# Extract area code from phone\n",
    "area_codes = string_data['phone'].str.extract(r'(\\d{3})')\n",
    "print(\"\\n   Area codes:\")\n",
    "print(area_codes)\n",
    "\n",
    "# Extract code parts\n",
    "code_parts = string_data['code'].str.extract(r'([A-Z]+)-(\\d+)')\n",
    "code_parts.columns = ['letters', 'numbers']\n",
    "print(\"\\n   Code components:\")\n",
    "print(code_parts)\n",
    "\n",
    "# 7. Replace strings\n",
    "print(\"\\n7. Replace Strings:\")\n",
    "\n",
    "# Simple replace\n",
    "standardized_phone = string_data['phone'].str.replace(r'[().\\s-]', '', regex=True)\n",
    "print(\"\\n   Standardized phones:\")\n",
    "print(standardized_phone)\n",
    "\n",
    "# Replace with pattern\n",
    "masked_email = string_data['email'].str.replace(r'(.{2})[^@]+', r'\\1***', regex=True)\n",
    "print(\"\\n   Masked emails:\")\n",
    "print(masked_email)\n",
    "\n",
    "# 8. Length\n",
    "print(\"\\n8. String Length:\")\n",
    "\n",
    "string_data['name_length'] = string_data['name'].str.len()\n",
    "print(string_data[['name', 'name_length']])\n",
    "\n",
    "# 9. Slice strings\n",
    "print(\"\\n9. Slice Strings:\")\n",
    "\n",
    "# First 3 characters\n",
    "first_three = string_data['name'].str[:3]\n",
    "print(\"\\n   First 3 chars:\", first_three.tolist())\n",
    "\n",
    "# Last 3 characters\n",
    "last_three = string_data['code'].str[-3:]\n",
    "print(\"   Last 3 chars:\", last_three.tolist())\n",
    "\n",
    "# 10. Concatenate strings\n",
    "print(\"\\n10. Concatenate Strings:\")\n",
    "\n",
    "string_data['full_contact'] = (\n",
    "    string_data['name'].str.strip() + \n",
    "    ' <' + string_data['email'] + '>'\n",
    ")\n",
    "print(string_data['full_contact'])\n",
    "\n",
    "# 11. Find and index\n",
    "print(\"\\n11. Find Position:\")\n",
    "\n",
    "at_position = string_data['email'].str.find('@')\n",
    "print(\"\\n   Position of @ in email:\")\n",
    "print(at_position)\n",
    "\n",
    "# 12. Count occurrences\n",
    "print(\"\\n12. Count Pattern Occurrences:\")\n",
    "\n",
    "dash_count = string_data['phone'].str.count('-')\n",
    "print(\"\\n   Number of dashes in phone:\")\n",
    "print(dash_count)\n",
    "\n",
    "# 13. Get specific items after split\n",
    "print(\"\\n13. Get Specific Split Items:\")\n",
    "\n",
    "# Get first name only\n",
    "first_names = string_data['name'].str.strip().str.split().str[0]\n",
    "print(\"\\n   First names:\")\n",
    "print(first_names)\n",
    "\n",
    "# Get domain\n",
    "domains = string_data['email'].str.split('@').str[1]\n",
    "print(\"\\n   Email domains:\")\n",
    "print(domains)\n",
    "\n",
    "# 14. Padding\n",
    "print(\"\\n14. Pad Strings:\")\n",
    "\n",
    "# Pad with zeros\n",
    "padded_codes = code_parts['numbers'].str.pad(width=5, fillchar='0')\n",
    "print(\"\\n   Padded codes:\")\n",
    "print(padded_codes)\n",
    "\n",
    "# 15. Practical: Clean and standardize data\n",
    "print(\"\\n15. Practical: Data Cleaning Pipeline:\")\n",
    "\n",
    "# Comprehensive cleaning\n",
    "cleaned_data = string_data.copy()\n",
    "\n",
    "# Clean names\n",
    "cleaned_data['name_clean'] = (\n",
    "    cleaned_data['name']\n",
    "    .str.strip()\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "# Standardize emails\n",
    "cleaned_data['email_clean'] = (\n",
    "    cleaned_data['email']\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Standardize phones\n",
    "cleaned_data['phone_clean'] = (\n",
    "    cleaned_data['phone']\n",
    "    .str.replace(r'[^0-9]', '', regex=True)\n",
    ")\n",
    "\n",
    "# Extract components\n",
    "cleaned_data['area_code'] = cleaned_data['phone_clean'].str[:3]\n",
    "cleaned_data['code_prefix'] = cleaned_data['code'].str.split('-').str[0]\n",
    "\n",
    "print(cleaned_data[['name_clean', 'email_clean', 'phone_clean', 'area_code', 'code_prefix']])\n",
    "\n",
    "print(\"\\n   âœ“ String operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ab879",
   "metadata": {},
   "source": [
    "## 12. Binning and Discretization\n",
    "\n",
    "**What**: Converting continuous numerical data into categorical bins or discrete intervals.\n",
    "\n",
    "**Why**: \n",
    "- Simplify continuous variables\n",
    "- Create categorical features\n",
    "- Handle outliers\n",
    "- Prepare data for analysis\n",
    "\n",
    "**When to Use**:\n",
    "- Age groups from ages\n",
    "- Price ranges from prices\n",
    "- Risk categories from scores\n",
    "- Quantile-based grouping\n",
    "\n",
    "**Key Operations**:\n",
    "- **pd.cut()**: Bin into fixed intervals\n",
    "- **pd.qcut()**: Bin into quantiles\n",
    "- **Custom bins**: Specify bin edges\n",
    "- **Labels**: Assign category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b793ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 12. BINNING AND DISCRETIZATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BINNING AND DISCRETIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample data\n",
    "binning_data = pd.DataFrame({\n",
    "    'age': [22, 25, 30, 35, 42, 48, 55, 62, 68, 75],\n",
    "    'income': [35000, 45000, 52000, 68000, 75000, 82000, 95000, 105000, 88000, 92000],\n",
    "    'score': [45, 67, 72, 89, 56, 78, 91, 63, 82, 94]\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample Data:\")\n",
    "print(binning_data)\n",
    "\n",
    "# 1. Simple binning with cut()\n",
    "print(\"\\n1. Simple Binning (cut):\")\n",
    "\n",
    "# Create 3 equal-width bins\n",
    "age_bins = pd.cut(binning_data['age'], bins=3)\n",
    "print(\"\\n   Age bins:\")\n",
    "print(age_bins)\n",
    "\n",
    "# Count in each bin\n",
    "print(\"\\n   Counts per bin:\")\n",
    "print(age_bins.value_counts().sort_index())\n",
    "\n",
    "# 2. Custom bin edges\n",
    "print(\"\\n2. Custom Bin Edges:\")\n",
    "\n",
    "# Define specific age groups\n",
    "age_groups = pd.cut(\n",
    "    binning_data['age'],\n",
    "    bins=[0, 30, 50, 70, 100],\n",
    "    labels=['Young', 'Adult', 'Middle-aged', 'Senior']\n",
    ")\n",
    "\n",
    "binning_data['age_group'] = age_groups\n",
    "print(binning_data[['age', 'age_group']])\n",
    "\n",
    "# 3. Right vs Left inclusive\n",
    "print(\"\\n3. Right vs Left Inclusive:\")\n",
    "\n",
    "# Default: right=True (intervals like (20, 30])\n",
    "right_inclusive = pd.cut(binning_data['age'], bins=3, right=True)\n",
    "print(\"\\n   Right inclusive (20, 30]:\")\n",
    "print(right_inclusive[:3])\n",
    "\n",
    "# Left inclusive: [20, 30)\n",
    "left_inclusive = pd.cut(binning_data['age'], bins=3, right=False)\n",
    "print(\"\\n   Left inclusive [20, 30):\")\n",
    "print(left_inclusive[:3])\n",
    "\n",
    "# 4. Quantile-based binning (qcut)\n",
    "print(\"\\n4. Quantile-based Binning (qcut):\")\n",
    "\n",
    "# Divide into quartiles\n",
    "income_quartiles = pd.qcut(binning_data['income'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "binning_data['income_quartile'] = income_quartiles\n",
    "\n",
    "print(binning_data[['income', 'income_quartile']])\n",
    "\n",
    "print(\"\\n   Counts per quartile:\")\n",
    "print(income_quartiles.value_counts().sort_index())\n",
    "\n",
    "# 5. Quantiles with duplicates\n",
    "print(\"\\n5. Quantile Binning with Duplicates:\")\n",
    "\n",
    "# Handle duplicate edges\n",
    "try:\n",
    "    score_deciles = pd.qcut(binning_data['score'], q=10, duplicates='drop')\n",
    "    print(\"   Score deciles created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# 6. Get bin information\n",
    "print(\"\\n6. Bin Information:\")\n",
    "\n",
    "# Include metadata\n",
    "age_cut = pd.cut(binning_data['age'], bins=4, retbins=True)\n",
    "print(\"\\n   Bin edges:\")\n",
    "print(age_cut[1])\n",
    "\n",
    "# 7. Precision control\n",
    "print(\"\\n7. Control Bin Precision:\")\n",
    "\n",
    "# Limit decimal places in bin labels\n",
    "precise_bins = pd.cut(binning_data['income'], bins=3, precision=0)\n",
    "print(\"\\n   Income bins (no decimals):\")\n",
    "print(precise_bins)\n",
    "\n",
    "# 8. Include lowest\n",
    "print(\"\\n8. Include Lowest Value:\")\n",
    "\n",
    "# Include minimum value\n",
    "inclusive_bins = pd.cut(\n",
    "    binning_data['score'],\n",
    "    bins=[40, 60, 80, 100],\n",
    "    include_lowest=True,\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "print(\"\\n   Score categories:\")\n",
    "print(inclusive_bins)\n",
    "\n",
    "# 9. Binning with inf bounds\n",
    "print(\"\\n9. Binning with Infinity:\")\n",
    "\n",
    "# Open-ended bins\n",
    "income_categories = pd.cut(\n",
    "    binning_data['income'],\n",
    "    bins=[0, 50000, 75000, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "binning_data['income_category'] = income_categories\n",
    "\n",
    "print(binning_data[['income', 'income_category']])\n",
    "\n",
    "# 10. Ordered categories\n",
    "print(\"\\n10. Ordered Categories:\")\n",
    "\n",
    "# Make categories ordered\n",
    "score_levels = pd.cut(\n",
    "    binning_data['score'],\n",
    "    bins=[0, 60, 75, 90, 100],\n",
    "    labels=['Fail', 'Pass', 'Good', 'Excellent']\n",
    ")\n",
    "\n",
    "score_cat = pd.Categorical(score_levels, ordered=True)\n",
    "print(\"\\n   Ordered score levels:\")\n",
    "print(score_cat)\n",
    "print(f\"   Is ordered: {score_cat.ordered}\")\n",
    "\n",
    "# 11. Practical: Risk scoring\n",
    "print(\"\\n11. Practical: Risk Scoring:\")\n",
    "\n",
    "# Create risk categories\n",
    "binning_data['risk_score'] = binning_data['score']\n",
    "\n",
    "binning_data['risk_level'] = pd.cut(\n",
    "    binning_data['risk_score'],\n",
    "    bins=[0, 50, 70, 85, 100],\n",
    "    labels=['High Risk', 'Medium Risk', 'Low Risk', 'Very Low Risk'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(binning_data[['score', 'risk_level']])\n",
    "\n",
    "# 12. Multiple binning strategies\n",
    "print(\"\\n12. Compare Binning Methods:\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'value': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "})\n",
    "\n",
    "# Equal width bins\n",
    "comparison['equal_width'] = pd.cut(comparison['value'], bins=3, labels=['Low', 'Mid', 'High'])\n",
    "\n",
    "# Equal frequency bins (quantiles)\n",
    "comparison['equal_freq'] = pd.qcut(comparison['value'], q=3, labels=['Low', 'Mid', 'High'])\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\n   Equal width distribution:\")\n",
    "print(comparison['equal_width'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n   Equal frequency distribution:\")\n",
    "print(comparison['equal_freq'].value_counts().sort_index())\n",
    "\n",
    "# 13. Binning summary statistics\n",
    "print(\"\\n13. Statistics by Bin:\")\n",
    "\n",
    "# Group by bins and calculate statistics\n",
    "bin_stats = binning_data.groupby('age_group').agg({\n",
    "    'age': ['min', 'max', 'count'],\n",
    "    'income': 'mean',\n",
    "    'score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(bin_stats)\n",
    "\n",
    "print(\"\\n   âœ“ Binning and discretization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae33c71",
   "metadata": {},
   "source": [
    "## 13. Window Functions (Rolling, Expanding, EWM)\n",
    "\n",
    "**What**: Calculations over moving or expanding windows of data.\n",
    "\n",
    "**Why**: \n",
    "- Smooth noisy time series data\n",
    "- Calculate moving averages and trends\n",
    "- Identify patterns over time\n",
    "- Remove short-term fluctuations\n",
    "\n",
    "**When to Use**:\n",
    "- Stock price analysis (moving averages)\n",
    "- Weather data smoothing\n",
    "- Sales trend analysis\n",
    "- Anomaly detection\n",
    "\n",
    "**Key Operations**:\n",
    "- **rolling()**: Fixed-size moving window\n",
    "- **expanding()**: Cumulative window\n",
    "- **ewm()**: Exponentially weighted moving\n",
    "- **Window functions**: sum, mean, std, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71766a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 13. WINDOW FUNCTIONS (ROLLING, EXPANDING, EWM)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WINDOW FUNCTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample time series data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=20, freq='D')\n",
    "window_data = pd.DataFrame({\n",
    "    'value': np.random.randint(80, 120, 20) + np.random.randn(20) * 5\n",
    "}, index=dates)\n",
    "\n",
    "print(\"\\n0. Sample Time Series Data:\")\n",
    "print(window_data.head(10))\n",
    "\n",
    "# 1. Rolling window - Mean\n",
    "print(\"\\n1. Rolling Mean (Moving Average):\")\n",
    "\n",
    "window_data['rolling_mean_3'] = window_data['value'].rolling(window=3).mean()\n",
    "window_data['rolling_mean_7'] = window_data['value'].rolling(window=7).mean()\n",
    "\n",
    "print(window_data[['value', 'rolling_mean_3', 'rolling_mean_7']].head(10))\n",
    "\n",
    "# 2. Rolling window - Other aggregations\n",
    "print(\"\\n2. Rolling Aggregations:\")\n",
    "\n",
    "window_data['rolling_std'] = window_data['value'].rolling(window=5).std()\n",
    "window_data['rolling_min'] = window_data['value'].rolling(window=5).min()\n",
    "window_data['rolling_max'] = window_data['value'].rolling(window=5).max()\n",
    "\n",
    "print(window_data[['value', 'rolling_std', 'rolling_min', 'rolling_max']].head(10))\n",
    "\n",
    "# 3. Rolling with min_periods\n",
    "print(\"\\n3. Rolling with Minimum Periods:\")\n",
    "\n",
    "# Calculate even when window not full\n",
    "early_rolling = window_data['value'].rolling(window=5, min_periods=1).mean()\n",
    "print(\"\\n   First 7 values (min_periods=1):\")\n",
    "print(early_rolling.head(7))\n",
    "\n",
    "# 4. Centered rolling window\n",
    "print(\"\\n4. Centered Rolling Window:\")\n",
    "\n",
    "# Center the window (instead of trailing)\n",
    "centered = window_data['value'].rolling(window=5, center=True).mean()\n",
    "print(\"\\n   Centered vs trailing:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'value': window_data['value'],\n",
    "    'trailing': window_data['value'].rolling(5).mean(),\n",
    "    'centered': centered\n",
    "})\n",
    "print(comparison.head(10))\n",
    "\n",
    "# 5. Rolling sum\n",
    "print(\"\\n5. Rolling Sum:\")\n",
    "\n",
    "window_data['rolling_sum_3'] = window_data['value'].rolling(window=3).sum()\n",
    "print(window_data[['value', 'rolling_sum_3']].head(7))\n",
    "\n",
    "# 6. Expanding window (cumulative)\n",
    "print(\"\\n6. Expanding Window:\")\n",
    "\n",
    "window_data['expanding_mean'] = window_data['value'].expanding().mean()\n",
    "window_data['expanding_sum'] = window_data['value'].expanding().sum()\n",
    "\n",
    "print(window_data[['value', 'expanding_mean', 'expanding_sum']].head(10))\n",
    "\n",
    "# 7. Expanding with min_periods\n",
    "print(\"\\n7. Expanding with Min Periods:\")\n",
    "\n",
    "expanding_min = window_data['value'].expanding(min_periods=3).mean()\n",
    "print(\"\\n   First 5 values (min_periods=3):\")\n",
    "print(expanding_min.head(5))\n",
    "\n",
    "# 8. Exponentially Weighted Moving Average\n",
    "print(\"\\n8. Exponentially Weighted Moving (EWM):\")\n",
    "\n",
    "window_data['ewm_mean'] = window_data['value'].ewm(span=5).mean()\n",
    "print(window_data[['value', 'ewm_mean']].head(10))\n",
    "\n",
    "# 9. EWM with different parameters\n",
    "print(\"\\n9. EWM Parameters:\")\n",
    "\n",
    "# Different spans\n",
    "window_data['ewm_short'] = window_data['value'].ewm(span=3).mean()\n",
    "window_data['ewm_long'] = window_data['value'].ewm(span=10).mean()\n",
    "\n",
    "print(window_data[['value', 'ewm_short', 'ewm_long']].head(10))\n",
    "\n",
    "# 10. Rolling correlation\n",
    "print(\"\\n10. Rolling Correlation:\")\n",
    "\n",
    "# Create second series\n",
    "window_data['value2'] = window_data['value'] + np.random.randn(20) * 10\n",
    "\n",
    "# Calculate rolling correlation\n",
    "rolling_corr = window_data['value'].rolling(window=7).corr(window_data['value2'])\n",
    "print(\"\\n   Rolling 7-day correlation:\")\n",
    "print(rolling_corr.head(10))\n",
    "\n",
    "# 11. Rolling apply custom function\n",
    "print(\"\\n11. Rolling Apply Custom Function:\")\n",
    "\n",
    "# Custom function: range (max - min)\n",
    "def window_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "window_data['rolling_range'] = window_data['value'].rolling(window=5).apply(window_range)\n",
    "print(window_data[['value', 'rolling_range']].head(10))\n",
    "\n",
    "# 12. Multiple rolling aggregations\n",
    "print(\"\\n12. Multiple Rolling Aggregations:\")\n",
    "\n",
    "rolling_stats = window_data['value'].rolling(window=7).agg(['mean', 'std', 'min', 'max'])\n",
    "print(rolling_stats.head(10))\n",
    "\n",
    "# 13. Window with datetime offset\n",
    "print(\"\\n13. Rolling Window with Time Offset:\")\n",
    "\n",
    "# 7-day rolling window\n",
    "time_window = window_data['value'].rolling('7D').mean()\n",
    "print(\"\\n   7-day time-based window:\")\n",
    "print(time_window.head(10))\n",
    "\n",
    "# 14. Practical: Moving average crossover\n",
    "print(\"\\n14. Practical: Moving Average Crossover Strategy:\")\n",
    "\n",
    "# Common in trading: short-term vs long-term MA\n",
    "window_data['ma_short'] = window_data['value'].rolling(window=3).mean()\n",
    "window_data['ma_long'] = window_data['value'].rolling(window=7).mean()\n",
    "\n",
    "# Signal: short MA crosses above long MA\n",
    "window_data['signal'] = (window_data['ma_short'] > window_data['ma_long']).astype(int)\n",
    "\n",
    "print(window_data[['value', 'ma_short', 'ma_long', 'signal']].tail(10))\n",
    "\n",
    "# 15. Practical: Smoothing noisy data\n",
    "print(\"\\n15. Practical: Data Smoothing Comparison:\")\n",
    "\n",
    "# Create noisy data\n",
    "noisy_signal = pd.Series(\n",
    "    [100 + i*2 + np.random.randn()*10 for i in range(20)],\n",
    "    index=dates\n",
    ")\n",
    "\n",
    "smoothing = pd.DataFrame({\n",
    "    'original': noisy_signal,\n",
    "    'rolling_3': noisy_signal.rolling(3).mean(),\n",
    "    'rolling_7': noisy_signal.rolling(7).mean(),\n",
    "    'ewm_5': noisy_signal.ewm(span=5).mean()\n",
    "})\n",
    "\n",
    "print(smoothing.head(10))\n",
    "\n",
    "print(\"\\n   Different smoothing techniques:\")\n",
    "print(f\"   Original std: {smoothing['original'].std():.2f}\")\n",
    "print(f\"   Rolling-3 std: {smoothing['rolling_3'].std():.2f}\")\n",
    "print(f\"   Rolling-7 std: {smoothing['rolling_7'].std():.2f}\")\n",
    "print(f\"   EWM-5 std: {smoothing['ewm_5'].std():.2f}\")\n",
    "\n",
    "print(\"\\n   âœ“ Window functions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0579e",
   "metadata": {},
   "source": [
    "## 14. Data Sampling and Resampling\n",
    "\n",
    "**What**: Selecting subsets of data or changing time series frequency.\n",
    "\n",
    "**Why**: \n",
    "- Create train/test splits\n",
    "- Balance datasets\n",
    "- Change time granularity\n",
    "- Reduce data size for testing\n",
    "\n",
    "**When to Use**:\n",
    "- Machine learning data preparation\n",
    "- Time series aggregation/disaggregation\n",
    "- Statistical sampling\n",
    "- Performance testing with smaller datasets\n",
    "\n",
    "**Key Operations**:\n",
    "- **sample()**: Random sampling\n",
    "- **resample()**: Time series frequency conversion\n",
    "- **Stratified sampling**: Maintain class proportions\n",
    "- **Bootstrapping**: Sample with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ff0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 14. DATA SAMPLING AND RESAMPLING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA SAMPLING AND RESAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample dataset\n",
    "sampling_data = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'value': np.random.randint(1, 100, 100),\n",
    "    'score': np.random.randn(100)\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample Dataset:\")\n",
    "print(sampling_data.head())\n",
    "print(f\"   Shape: {sampling_data.shape}\")\n",
    "\n",
    "# 1. Random sampling\n",
    "print(\"\\n1. Random Sampling:\")\n",
    "\n",
    "# Sample 10 random rows\n",
    "random_sample = sampling_data.sample(n=10)\n",
    "print(random_sample)\n",
    "\n",
    "# 2. Fraction-based sampling\n",
    "print(\"\\n2. Fraction-based Sampling:\")\n",
    "\n",
    "# Sample 20% of data\n",
    "frac_sample = sampling_data.sample(frac=0.2)\n",
    "print(f\"\\n   20% sample size: {len(frac_sample)} rows\")\n",
    "print(frac_sample.head())\n",
    "\n",
    "# 3. Sampling with replacement\n",
    "print(\"\\n3. Sampling with Replacement:\")\n",
    "\n",
    "# Can get duplicate rows\n",
    "bootstrap_sample = sampling_data.sample(n=10, replace=True)\n",
    "print(bootstrap_sample)\n",
    "\n",
    "# 4. Stratified sampling\n",
    "print(\"\\n4. Stratified Sampling (Maintain Proportions):\")\n",
    "\n",
    "# Check original distribution\n",
    "print(\"\\n   Original category distribution:\")\n",
    "print(sampling_data['category'].value_counts(normalize=True))\n",
    "\n",
    "# Stratified sample\n",
    "stratified = sampling_data.groupby('category', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.2)\n",
    ")\n",
    "\n",
    "print(f\"\\n   Stratified sample size: {len(stratified)}\")\n",
    "print(\"\\n   Stratified sample distribution:\")\n",
    "print(stratified['category'].value_counts(normalize=True))\n",
    "\n",
    "# 5. Random state for reproducibility\n",
    "print(\"\\n5. Reproducible Sampling:\")\n",
    "\n",
    "sample1 = sampling_data.sample(n=5, random_state=42)\n",
    "sample2 = sampling_data.sample(n=5, random_state=42)\n",
    "\n",
    "print(\"\\n   First sample:\")\n",
    "print(sample1['id'].tolist())\n",
    "print(\"\\n   Second sample (same random_state):\")\n",
    "print(sample2['id'].tolist())\n",
    "print(f\"\\n   Identical: {sample1['id'].tolist() == sample2['id'].tolist()}\")\n",
    "\n",
    "# 6. Sampling by weights\n",
    "print(\"\\n6. Weighted Sampling:\")\n",
    "\n",
    "# Higher probability for higher values\n",
    "weights = sampling_data['value'] / sampling_data['value'].sum()\n",
    "weighted_sample = sampling_data.sample(n=10, weights=weights)\n",
    "\n",
    "print(\"\\n   Weighted sample (biased toward higher values):\")\n",
    "print(weighted_sample[['id', 'value']].sort_values('value', ascending=False))\n",
    "\n",
    "# 7. Time series resampling - Downsampling\n",
    "print(\"\\n7. Time Series Resampling (Downsampling):\")\n",
    "\n",
    "# Create daily time series\n",
    "ts_data = pd.DataFrame({\n",
    "    'value': np.random.randint(50, 150, 30)\n",
    "}, index=pd.date_range('2024-01-01', periods=30, freq='D'))\n",
    "\n",
    "# Resample to weekly (mean)\n",
    "weekly = ts_data.resample('W').mean()\n",
    "print(\"\\n   Daily to Weekly (mean):\")\n",
    "print(weekly.head())\n",
    "\n",
    "# Resample to weekly (sum)\n",
    "weekly_sum = ts_data.resample('W').sum()\n",
    "print(\"\\n   Daily to Weekly (sum):\")\n",
    "print(weekly_sum.head())\n",
    "\n",
    "# 8. Time series resampling - Upsampling\n",
    "print(\"\\n8. Time Series Resampling (Upsampling):\")\n",
    "\n",
    "# Create monthly data\n",
    "monthly_data = pd.DataFrame({\n",
    "    'value': [100, 150, 200]\n",
    "}, index=pd.date_range('2024-01-01', periods=3, freq='MS'))\n",
    "\n",
    "# Upsample to daily (forward fill)\n",
    "daily = monthly_data.resample('D').ffill()\n",
    "print(\"\\n   Monthly to Daily (forward fill):\")\n",
    "print(daily.head(10))\n",
    "\n",
    "# Upsample with interpolation\n",
    "daily_interp = monthly_data.resample('D').interpolate()\n",
    "print(\"\\n   Monthly to Daily (interpolation):\")\n",
    "print(daily_interp.head(10))\n",
    "\n",
    "# 9. Resample with different aggregations\n",
    "print(\"\\n9. Multiple Aggregations in Resample:\")\n",
    "\n",
    "# Create detailed time series\n",
    "detailed_ts = pd.DataFrame({\n",
    "    'sales': np.random.randint(100, 200, 30),\n",
    "    'quantity': np.random.randint(10, 50, 30)\n",
    "}, index=pd.date_range('2024-01-01', periods=30, freq='D'))\n",
    "\n",
    "# Resample with different aggregations per column\n",
    "weekly_agg = detailed_ts.resample('W').agg({\n",
    "    'sales': 'sum',\n",
    "    'quantity': 'mean'\n",
    "})\n",
    "\n",
    "print(weekly_agg.head())\n",
    "\n",
    "# 10. Resample with custom function\n",
    "print(\"\\n10. Resample with Custom Function:\")\n",
    "\n",
    "def price_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "weekly_range = ts_data.resample('W').apply(price_range)\n",
    "print(\"\\n   Weekly value range:\")\n",
    "print(weekly_range.head())\n",
    "\n",
    "# 11. Sampling for train/test split\n",
    "print(\"\\n11. Practical: Train/Test Split:\")\n",
    "\n",
    "# 80/20 split\n",
    "train = sampling_data.sample(frac=0.8, random_state=42)\n",
    "test = sampling_data.drop(train.index)\n",
    "\n",
    "print(f\"\\n   Train size: {len(train)} ({len(train)/len(sampling_data)*100:.0f}%)\")\n",
    "print(f\"   Test size: {len(test)} ({len(test)/len(sampling_data)*100:.0f}%)\")\n",
    "\n",
    "# Verify no overlap\n",
    "print(f\"   No overlap: {len(set(train.index) & set(test.index)) == 0}\")\n",
    "\n",
    "# 12. Bootstrap sampling\n",
    "print(\"\\n12. Bootstrap Sampling:\")\n",
    "\n",
    "# Multiple bootstrap samples\n",
    "bootstrap_means = []\n",
    "for i in range(1000):\n",
    "    boot_sample = sampling_data['value'].sample(n=len(sampling_data), replace=True)\n",
    "    bootstrap_means.append(boot_sample.mean())\n",
    "\n",
    "bootstrap_means = pd.Series(bootstrap_means)\n",
    "print(f\"\\n   Bootstrap mean estimate: {bootstrap_means.mean():.2f}\")\n",
    "print(f\"   95% CI: [{bootstrap_means.quantile(0.025):.2f}, {bootstrap_means.quantile(0.975):.2f}]\")\n",
    "print(f\"   Actual mean: {sampling_data['value'].mean():.2f}\")\n",
    "\n",
    "# 13. Cross-validation folds\n",
    "print(\"\\n13. Create Cross-Validation Folds:\")\n",
    "\n",
    "# Simple K-fold creation\n",
    "n_folds = 5\n",
    "fold_size = len(sampling_data) // n_folds\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    indices = sampling_data.index.tolist()\n",
    "    test_idx = indices[fold * fold_size : (fold + 1) * fold_size]\n",
    "    train_idx = [i for i in indices if i not in test_idx]\n",
    "    \n",
    "    print(f\"   Fold {fold + 1}: Train={len(train_idx)}, Test={len(test_idx)}\")\n",
    "\n",
    "# 14. Time-based resampling periods\n",
    "print(\"\\n14. Different Resampling Periods:\")\n",
    "\n",
    "# Create hourly data\n",
    "hourly_data = pd.DataFrame({\n",
    "    'value': np.random.randint(50, 150, 24 * 7)  # 1 week of hourly data\n",
    "}, index=pd.date_range('2024-01-01', periods=24*7, freq='h'))\n",
    "\n",
    "print(\"\\n   Original: Hourly data\")\n",
    "print(f\"   Period: {hourly_data.index[0]} to {hourly_data.index[-1]}\")\n",
    "\n",
    "# Different frequencies\n",
    "print(\"\\n   Resample to 6-hour:\")\n",
    "print(hourly_data.resample('6h').mean().head())\n",
    "\n",
    "print(\"\\n   Resample to daily:\")\n",
    "print(hourly_data.resample('D').mean().head())\n",
    "\n",
    "print(\"\\n   Resample to business days:\")\n",
    "print(hourly_data.resample('B').mean().head())\n",
    "\n",
    "print(\"\\n   âœ“ Sampling and resampling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c124cd",
   "metadata": {},
   "source": [
    "## 15. Advanced Wrangling Techniques\n",
    "\n",
    "**What**: Complex data manipulation techniques and best practices for efficient workflows.\n",
    "\n",
    "**Why**: \n",
    "- Optimize performance\n",
    "- Write cleaner, more maintainable code\n",
    "- Handle complex data scenarios\n",
    "- Build efficient data pipelines\n",
    "\n",
    "**When to Use**:\n",
    "- Large-scale data processing\n",
    "- Production data pipelines\n",
    "- Complex transformations\n",
    "- Performance-critical applications\n",
    "\n",
    "**Key Techniques**:\n",
    "- **Method chaining**: Chain operations fluently\n",
    "- **pipe()**: Custom transformation functions\n",
    "- **assign()**: Add columns in chains\n",
    "- **query()**: SQL-like filtering\n",
    "- **eval()**: Fast expression evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 15. ADVANCED WRANGLING TECHNIQUES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED WRANGLING TECHNIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample data\n",
    "advanced_data = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'age': [25, 30, 35, 28, 32],\n",
    "    'salary': [50000, 60000, 75000, 55000, 65000],\n",
    "    'department': ['Sales', 'IT', 'Sales', 'IT', 'HR'],\n",
    "    'performance': [8.5, 9.2, 7.8, 8.9, 9.0]\n",
    "})\n",
    "\n",
    "print(\"\\n0. Sample Data:\")\n",
    "print(advanced_data)\n",
    "\n",
    "# 1. Method chaining\n",
    "print(\"\\n1. Method Chaining:\")\n",
    "\n",
    "# Chain multiple operations\n",
    "result = (advanced_data\n",
    "          .query('age > 25')\n",
    "          .assign(bonus=lambda x: x['salary'] * 0.1)\n",
    "          .sort_values('bonus', ascending=False)\n",
    "          .head(3))\n",
    "\n",
    "print(result)\n",
    "\n",
    "# 2. Using pipe() for custom transformations\n",
    "print(\"\\n2. Custom Transformations with pipe():\")\n",
    "\n",
    "def add_salary_category(df):\n",
    "    df['salary_category'] = pd.cut(\n",
    "        df['salary'],\n",
    "        bins=[0, 55000, 70000, 100000],\n",
    "        labels=['Low', 'Medium', 'High']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def add_age_group(df):\n",
    "    df['age_group'] = pd.cut(\n",
    "        df['age'],\n",
    "        bins=[0, 30, 40, 100],\n",
    "        labels=['Young', 'Middle', 'Senior']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Chain custom functions\n",
    "piped_result = (advanced_data\n",
    "                .pipe(add_salary_category)\n",
    "                .pipe(add_age_group))\n",
    "\n",
    "print(piped_result[['name', 'salary_category', 'age_group']])\n",
    "\n",
    "# 3. assign() for adding columns\n",
    "print(\"\\n3. Adding Columns with assign():\")\n",
    "\n",
    "# Add multiple columns at once\n",
    "assigned = advanced_data.assign(\n",
    "    salary_k=lambda x: x['salary'] / 1000,\n",
    "    bonus=lambda x: x['salary'] * 0.1,\n",
    "    total_comp=lambda x: x['salary'] + x['bonus']  # Can reference newly created columns\n",
    ")\n",
    "\n",
    "print(assigned[['name', 'salary', 'salary_k', 'bonus', 'total_comp']].head())\n",
    "\n",
    "# 4. query() for filtering\n",
    "print(\"\\n4. Query Method (SQL-like):\")\n",
    "\n",
    "# Simple query\n",
    "high_performers = advanced_data.query('performance > 8.5')\n",
    "print(\"\\n   High performers:\")\n",
    "print(high_performers)\n",
    "\n",
    "# Complex query\n",
    "filtered = advanced_data.query('age > 27 and salary >= 60000')\n",
    "print(\"\\n   Age > 27 AND salary >= 60000:\")\n",
    "print(filtered)\n",
    "\n",
    "# Query with variables\n",
    "min_age = 30\n",
    "dept = 'IT'\n",
    "var_query = advanced_data.query('age >= @min_age and department == @dept')\n",
    "print(f\"\\n   Age >= {min_age} in {dept}:\")\n",
    "print(var_query)\n",
    "\n",
    "# 5. eval() for fast calculations\n",
    "print(\"\\n5. eval() for Fast Expression Evaluation:\")\n",
    "\n",
    "# Instead of: df['total'] = df['a'] + df['b'] - df['c']\n",
    "calc_data = pd.DataFrame({\n",
    "    'a': [1, 2, 3, 4, 5],\n",
    "    'b': [10, 20, 30, 40, 50],\n",
    "    'c': [5, 10, 15, 20, 25]\n",
    "})\n",
    "\n",
    "calc_data['total'] = calc_data.eval('a + b - c')\n",
    "print(calc_data)\n",
    "\n",
    "# 6. explode() for list-like columns\n",
    "print(\"\\n6. Explode List-like Columns:\")\n",
    "\n",
    "list_data = pd.DataFrame({\n",
    "    'customer': ['A', 'B', 'C'],\n",
    "    'products': [['Laptop', 'Phone'], ['Tablet'], ['Laptop', 'Monitor', 'Keyboard']]\n",
    "})\n",
    "\n",
    "print(\"\\n   Original:\")\n",
    "print(list_data)\n",
    "\n",
    "exploded = list_data.explode('products')\n",
    "print(\"\\n   Exploded:\")\n",
    "print(exploded)\n",
    "\n",
    "# 7. melt with multiple value columns\n",
    "print(\"\\n7. Advanced Melt:\")\n",
    "\n",
    "wide_data = pd.DataFrame({\n",
    "    'product': ['A', 'B'],\n",
    "    'Q1_sales': [100, 150],\n",
    "    'Q1_units': [10, 15],\n",
    "    'Q2_sales': [120, 160],\n",
    "    'Q2_units': [12, 16]\n",
    "})\n",
    "\n",
    "# Melt with value_vars patterns\n",
    "melted = wide_data.melt(\n",
    "    id_vars='product',\n",
    "    value_vars=['Q1_sales', 'Q2_sales'],\n",
    "    var_name='quarter',\n",
    "    value_name='sales'\n",
    ")\n",
    "\n",
    "print(melted)\n",
    "\n",
    "# 8. Combining merge + groupby\n",
    "print(\"\\n8. Merge + GroupBy Pipeline:\")\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4],\n",
    "    'customer_id': [101, 102, 101, 103],\n",
    "    'amount': [100, 150, 200, 120]\n",
    "})\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'tier': ['Gold', 'Silver', 'Gold']\n",
    "})\n",
    "\n",
    "# Complex pipeline\n",
    "result = (orders\n",
    "          .merge(customers, on='customer_id')\n",
    "          .groupby('tier')\n",
    "          .agg({\n",
    "              'amount': ['sum', 'mean', 'count'],\n",
    "              'customer_id': 'nunique'\n",
    "          })\n",
    "          .round(2))\n",
    "\n",
    "print(result)\n",
    "\n",
    "# 9. Using where/mask for conditional assignment\n",
    "print(\"\\n9. Conditional Assignment with where/mask:\")\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'value': [10, 20, 30, 40, 50]\n",
    "})\n",
    "\n",
    "# where: keep values where condition is True\n",
    "data['capped_at_30'] = data['value'].where(data['value'] <= 30, 30)\n",
    "\n",
    "# mask: replace values where condition is True\n",
    "data['floored_at_20'] = data['value'].mask(data['value'] < 20, 20)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# 10. Complex transformations with groupby.transform\n",
    "print(\"\\n10. GroupBy Transform for Complex Features:\")\n",
    "\n",
    "sales_data = pd.DataFrame({\n",
    "    'product': ['A', 'A', 'B', 'B', 'A'],\n",
    "    'sales': [100, 150, 200, 180, 120]\n",
    "})\n",
    "\n",
    "# Add group statistics\n",
    "sales_data['product_mean'] = sales_data.groupby('product')['sales'].transform('mean')\n",
    "sales_data['diff_from_mean'] = sales_data['sales'] - sales_data['product_mean']\n",
    "sales_data['pct_of_total'] = sales_data.groupby('product')['sales'].transform(\n",
    "    lambda x: x / x.sum() * 100\n",
    ")\n",
    "\n",
    "print(sales_data)\n",
    "\n",
    "# 11. pd.crosstab with normalization\n",
    "print(\"\\n11. Advanced CrossTab:\")\n",
    "\n",
    "ct_data = pd.DataFrame({\n",
    "    'region': ['North', 'South', 'North', 'South', 'North'],\n",
    "    'product': ['A', 'A', 'B', 'B', 'A'],\n",
    "    'sales': [100, 150, 200, 180, 120]\n",
    "})\n",
    "\n",
    "# Crosstab with values and normalization\n",
    "crosstab = pd.crosstab(\n",
    "    ct_data['region'],\n",
    "    ct_data['product'],\n",
    "    values=ct_data['sales'],\n",
    "    aggfunc='sum',\n",
    "    normalize='index',  # Row percentages\n",
    "    margins=True\n",
    ") * 100\n",
    "\n",
    "print(\"\\n   Sales percentage by region:\")\n",
    "print(crosstab.round(2))\n",
    "\n",
    "# 12. Efficient string operations\n",
    "print(\"\\n12. Efficient String Vectorization:\")\n",
    "\n",
    "text_data = pd.DataFrame({\n",
    "    'text': ['  Hello World  ', 'PYTHON pandas', 'Data Wrangling']\n",
    "})\n",
    "\n",
    "# Chain string operations\n",
    "text_data['processed'] = (text_data['text']\n",
    "                          .str.strip()\n",
    "                          .str.lower()\n",
    "                          .str.replace(' ', '_'))\n",
    "\n",
    "print(text_data)\n",
    "\n",
    "# 13. Memory optimization\n",
    "print(\"\\n13. Memory Optimization:\")\n",
    "\n",
    "mem_data = pd.DataFrame({\n",
    "    'category': ['A', 'B', 'A', 'C', 'B'] * 20,\n",
    "    'value': range(100)\n",
    "})\n",
    "\n",
    "print(f\"\\n   Original memory: {mem_data.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Convert to category\n",
    "mem_data['category'] = mem_data['category'].astype('category')\n",
    "\n",
    "print(f\"   After category: {mem_data.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# 14. Practical: Complete data pipeline\n",
    "print(\"\\n14. Practical: Complete Data Pipeline:\")\n",
    "\n",
    "# Simulate raw data\n",
    "raw_data = pd.DataFrame({\n",
    "    'customer_name': ['  Alice Smith', 'BOB JONES  ', 'charlie brown'],\n",
    "    'purchase_date': ['2024-01-15', '2024-02-20', '2024-01-30'],\n",
    "    'amount': [100, 200, 150],\n",
    "    'category': ['electronics', 'electronics', 'books']\n",
    "})\n",
    "\n",
    "# Complete pipeline\n",
    "cleaned_data = (raw_data\n",
    "                # Clean names\n",
    "                .assign(customer_name=lambda x: x['customer_name'].str.strip().str.title())\n",
    "                # Parse dates\n",
    "                .assign(purchase_date=lambda x: pd.to_datetime(x['purchase_date']))\n",
    "                # Add derived features\n",
    "                .assign(\n",
    "                    month=lambda x: x['purchase_date'].dt.month,\n",
    "                    year=lambda x: x['purchase_date'].dt.year,\n",
    "                    amount_category=lambda x: pd.cut(x['amount'], bins=[0, 100, 200, 1000], labels=['Low', 'Medium', 'High'])\n",
    "                )\n",
    "                # Optimize memory\n",
    "                .assign(category=lambda x: x['category'].astype('category'))\n",
    "                # Sort\n",
    "                .sort_values('purchase_date')\n",
    "                # Reset index\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "print(cleaned_data)\n",
    "print(f\"\\n   Data types:\\n{cleaned_data.dtypes}\")\n",
    "\n",
    "# 15. Best practices summary\n",
    "print(\"\\n15. Best Practices Summary:\")\n",
    "\n",
    "best_practices = pd.DataFrame({\n",
    "    'Practice': [\n",
    "        'Use method chaining',\n",
    "        'Vectorize operations',\n",
    "        'Use query() for filtering',\n",
    "        'Optimize dtypes (category)',\n",
    "        'Use assign() over direct assignment',\n",
    "        'Prefer apply() to loops',\n",
    "        'Use pipe() for reusable transforms',\n",
    "        'Profile memory usage'\n",
    "    ],\n",
    "    'Benefit': [\n",
    "        'Readable, maintainable code',\n",
    "        '10-100x faster than loops',\n",
    "        'SQL-like readability',\n",
    "        '50-90% memory reduction',\n",
    "        'Works in method chains',\n",
    "        'Faster than Python loops',\n",
    "        'Modular, testable code',\n",
    "        'Identify bottlenecks'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(best_practices.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ“ DATA WRANGLING LEARNING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYou've learned 15 comprehensive data wrangling techniques:\")\n",
    "print(\"1. Data Loading and Inspection\")\n",
    "print(\"2. Filtering and Subsetting\")\n",
    "print(\"3. Sorting and Ranking\")\n",
    "print(\"4. Reshaping Data\")\n",
    "print(\"5. Merging and Joining DataFrames\")\n",
    "print(\"6. Concatenating Data\")\n",
    "print(\"7. Grouping and Aggregation\")\n",
    "print(\"8. Data Transformation (Apply, Map, Replace)\")\n",
    "print(\"9. Handling Time Series Data\")\n",
    "print(\"10. Working with MultiIndex\")\n",
    "print(\"11. String Operations\")\n",
    "print(\"12. Binning and Discretization\")\n",
    "print(\"13. Window Functions (Rolling, Expanding, EWM)\")\n",
    "print(\"14. Data Sampling and Resampling\")\n",
    "print(\"15. Advanced Wrangling Techniques\")\n",
    "print(\"\\nNext steps: Practice these techniques on real datasets!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-step-by-step-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
