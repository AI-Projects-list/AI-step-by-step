{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f106f5",
   "metadata": {},
   "source": [
    "# 1. SparkSession - Entry Point\n",
    "\n",
    "## üìñ What is SparkSession?\n",
    "\n",
    "**SparkSession** is the **unified entry point** for all Spark functionality in PySpark 2.0+. It replaced the older SparkContext, SQLContext, and HiveContext.\n",
    "\n",
    "**Key Features:**\n",
    "- Single entry point for DataFrame, SQL, and Streaming APIs\n",
    "- Automatically creates SparkContext internally\n",
    "- Supports Hive integration\n",
    "- Configuration management\n",
    "- Catalog access (tables, databases, functions)\n",
    "\n",
    "**Structure:**\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"key\", \"value\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "## üéØ Why Use SparkSession?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Unified Interface** - One object for all Spark operations\n",
    "2. **Simplified API** - Easier than managing multiple contexts\n",
    "3. **Built-in Optimization** - Catalyst optimizer for DataFrames\n",
    "4. **Distributed Computing** - Process TBs of data across clusters\n",
    "5. **Lazy Evaluation** - Optimizes entire query plan before execution\n",
    "6. **Fault Tolerance** - Automatic recovery from failures\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Overhead** - Too heavy for small datasets (< 1 GB)\n",
    "2. **Learning Curve** - Different from pandas\n",
    "3. **Memory Requirements** - Needs significant RAM\n",
    "4. **Setup Complexity** - Cluster configuration can be complex\n",
    "\n",
    "## ‚è±Ô∏è When to Use SparkSession\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Big Data Processing (> 10 GB)**\n",
    "- Example: Process 100 GB of log files\n",
    "- Why: Pandas loads everything in memory, Spark distributes\n",
    "- Benefit: Can process data larger than single machine RAM\n",
    "\n",
    "**2. Distributed Computing Needed**\n",
    "- Example: Analyze 1 TB of customer transactions\n",
    "- Why: Split work across 10+ machines\n",
    "- Speed: Linear scaling with cluster size\n",
    "\n",
    "**3. ETL Pipelines**\n",
    "- Example: Daily data warehouse updates\n",
    "- Why: Read from multiple sources, transform, load to warehouse\n",
    "- Tools: Integrates with HDFS, S3, Kafka, databases\n",
    "\n",
    "**4. Machine Learning at Scale**\n",
    "- Example: Train model on 50 million records\n",
    "- Why: MLlib distributed algorithms\n",
    "- Benefit: Training on full dataset, not samples\n",
    "\n",
    "**5. Real-Time Stream Processing**\n",
    "- Example: Process sensor data from 10,000 devices\n",
    "- Why: Structured Streaming handles high throughput\n",
    "- Use case: IoT, fraud detection, monitoring\n",
    "\n",
    "**6. SQL on Big Data**\n",
    "- Example: Run SQL queries on 500 GB Parquet files\n",
    "- Why: SparkSQL with query optimization\n",
    "- Benefit: Familiar SQL interface for big data\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Small Data (< 1 GB)**\n",
    "- Problem: Overhead > benefit\n",
    "- Better: Use pandas (faster, simpler)\n",
    "- Why: Spark startup time not worth it\n",
    "\n",
    "**2. Need Interactive Analysis**\n",
    "- Problem: Lazy evaluation delays feedback\n",
    "- Better: Jupyter + pandas for exploration\n",
    "- Why: Immediate results matter more\n",
    "\n",
    "**3. Complex Custom Logic**\n",
    "- Problem: UDFs slow, hard to optimize\n",
    "- Better: Use built-in Spark functions when possible\n",
    "- Why: Python UDFs bypass Catalyst optimizer\n",
    "\n",
    "**4. Low-Latency Required (< 100ms)**\n",
    "- Problem: Spark adds latency\n",
    "- Better: Redis, Cassandra, in-memory databases\n",
    "- Why: Spark designed for throughput, not latency\n",
    "\n",
    "**5. No Cluster Available**\n",
    "- Problem: Local mode slower than pandas for small data\n",
    "- Better: pandas or Dask for single machine\n",
    "- Why: Spark shines with distributed infrastructure\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Architecture:**\n",
    "1. **Driver Program** - Your Python script with SparkSession\n",
    "2. **Cluster Manager** - YARN, Mesos, Kubernetes, or Standalone\n",
    "3. **Executors** - Worker processes on cluster nodes\n",
    "4. **Tasks** - Units of work sent to executors\n",
    "\n",
    "**Execution Flow:**\n",
    "1. Create SparkSession\n",
    "2. Define transformations (lazy)\n",
    "3. Call action (trigger execution)\n",
    "4. Catalyst optimizer creates execution plan\n",
    "5. Tasks distributed to executors\n",
    "6. Results collected to driver\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Netflix** - Recommendation engine (billions of events)\n",
    "2. **Uber** - Ride analytics, surge pricing\n",
    "3. **Airbnb** - Search ranking, pricing optimization\n",
    "4. **LinkedIn** - User analytics, job recommendations\n",
    "5. **Financial Services** - Fraud detection, risk analysis\n",
    "6. **Healthcare** - Patient data analysis, drug discovery\n",
    "7. **E-commerce** - Customer behavior, inventory optimization\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Always use SparkSession (not SparkContext) for PySpark 2.0+  \n",
    "‚úÖ Call `.getOrCreate()` to reuse existing session  \n",
    "‚úÖ Set meaningful appName for monitoring  \n",
    "‚úÖ Configure memory: `.config(\"spark.executor.memory\", \"4g\")`  \n",
    "‚úÖ Stop session when done: `spark.stop()`  \n",
    "‚úÖ Use `.master(\"local[*]\")` for local testing  \n",
    "‚úÖ Access SparkContext: `spark.sparkContext`  \n",
    "‚úÖ Lazy evaluation - nothing happens until action called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c05b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARKSESSION - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PYSPARK SPARKSESSION - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# NOTE: Install PySpark first: pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum, count, max, min\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import time\n",
    "\n",
    "# 1. CREATE SPARKSESSION - BASIC\n",
    "print(\"\\n1. CREATING SPARKSESSION (BASIC)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simple creation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Tutorial\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì SparkSession created successfully!\")\n",
    "print(f\"  App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark Version: {spark.version}\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# 2. CREATE SPARKSESSION - ADVANCED CONFIGURATION\n",
    "print(\"\\n2. CREATING SPARKSESSION (ADVANCED CONFIGURATION)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Stop existing session\n",
    "spark.stop()\n",
    "\n",
    "# Create with detailed configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced PySpark Tutorial\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì Advanced SparkSession created!\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Master: {spark.sparkContext.master} (local mode, all cores)\")\n",
    "print(f\"  Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"  Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# 3. CREATE SAMPLE DATAFRAME\n",
    "print(\"\\n3. CREATING SAMPLE DATAFRAME\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Sample data - E-commerce sales\n",
    "data = [\n",
    "    (\"TX001\", \"Laptop\", \"Electronics\", 1200, 2, \"2024-01-15\", \"New York\"),\n",
    "    (\"TX002\", \"Mouse\", \"Accessories\", 25, 5, \"2024-01-15\", \"London\"),\n",
    "    (\"TX003\", \"Keyboard\", \"Accessories\", 75, 3, \"2024-01-16\", \"Paris\"),\n",
    "    (\"TX004\", \"Monitor\", \"Electronics\", 300, 1, \"2024-01-16\", \"Tokyo\"),\n",
    "    (\"TX005\", \"Laptop\", \"Electronics\", 1200, 1, \"2024-01-17\", \"New York\"),\n",
    "    (\"TX006\", \"Headphones\", \"Accessories\", 150, 2, \"2024-01-17\", \"London\"),\n",
    "    (\"TX007\", \"Webcam\", \"Electronics\", 80, 4, \"2024-01-18\", \"Berlin\"),\n",
    "    (\"TX008\", \"Mouse\", \"Accessories\", 25, 10, \"2024-01-18\", \"Paris\"),\n",
    "    (\"TX009\", \"Monitor\", \"Electronics\", 300, 2, \"2024-01-19\", \"Tokyo\"),\n",
    "    (\"TX010\", \"Keyboard\", \"Accessories\", 75, 5, \"2024-01-19\", \"New York\")\n",
    "]\n",
    "\n",
    "columns = [\"transaction_id\", \"product\", \"category\", \"price\", \"quantity\", \"date\", \"city\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(f\"‚úì DataFrame created with {df.count()} rows\")\n",
    "print(f\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# 4. BASIC DATAFRAME OPERATIONS\n",
    "print(\"\\n4. BASIC DATAFRAME OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Select columns\n",
    "print(\"Select specific columns:\")\n",
    "df.select(\"product\", \"price\", \"quantity\").show(3)\n",
    "\n",
    "# Filter data\n",
    "print(\"\\nFilter: Electronics only\")\n",
    "df.filter(col(\"category\") == \"Electronics\").show(3)\n",
    "\n",
    "# Add calculated column\n",
    "print(\"\\nAdd 'revenue' column (price * quantity):\")\n",
    "df_with_revenue = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "df_with_revenue.select(\"product\", \"price\", \"quantity\", \"revenue\").show(5)\n",
    "\n",
    "# 5. AGGREGATIONS\n",
    "print(\"\\n5. AGGREGATION OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"Overall Statistics:\")\n",
    "df_with_revenue.agg(\n",
    "    count(\"*\").alias(\"total_transactions\"),\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "    max(\"revenue\").alias(\"max_revenue\"),\n",
    "    min(\"revenue\").alias(\"min_revenue\")\n",
    ").show()\n",
    "\n",
    "# Group by category\n",
    "print(\"\\nRevenue by Category:\")\n",
    "category_stats = df_with_revenue.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    avg(\"revenue\").alias(\"avg_revenue\")\n",
    ").orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "category_stats.show()\n",
    "\n",
    "# Group by city\n",
    "print(\"\\nRevenue by City:\")\n",
    "city_stats = df_with_revenue.groupBy(\"city\").agg(\n",
    "    sum(\"revenue\").alias(\"total_revenue\")\n",
    ").orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "city_stats.show()\n",
    "\n",
    "# 6. SQL QUERIES\n",
    "print(\"\\n6. SQL QUERIES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Register as temporary view\n",
    "df_with_revenue.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "print(\"SQL Query: Top 5 products by revenue\")\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        category,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        COUNT(*) as transactions\n",
    "    FROM sales\n",
    "    GROUP BY product, category\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()\n",
    "\n",
    "# Complex SQL query\n",
    "print(\"\\nSQL Query: Products with revenue > $500\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        ROUND(AVG(price), 2) as avg_price\n",
    "    FROM sales\n",
    "    GROUP BY product\n",
    "    HAVING SUM(revenue) > 500\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 7. PERFORMANCE DEMONSTRATION\n",
    "print(\"\\n7. PERFORMANCE DEMONSTRATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create larger dataset\n",
    "print(\"Creating larger dataset for performance test...\")\n",
    "large_data = [(f\"TX{i}\", f\"Product{i%10}\", \"Category\" + str(i%3), \n",
    "               i%1000 + 10, i%5 + 1, f\"2024-01-{(i%28)+1:02d}\", \n",
    "               [\"NYC\", \"London\", \"Paris\", \"Tokyo\"][i%4]) \n",
    "              for i in range(10000)]\n",
    "\n",
    "large_df = spark.createDataFrame(large_data, columns)\n",
    "large_df = large_df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "print(f\"‚úì Created DataFrame with {large_df.count():,} rows\")\n",
    "\n",
    "# Benchmark aggregation\n",
    "print(\"\\nBenchmarking aggregation on 10,000 rows...\")\n",
    "start_time = time.time()\n",
    "\n",
    "result = large_df.groupBy(\"category\").agg(\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    count(\"*\").alias(\"count\")\n",
    ").collect()  # Action triggers execution\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Aggregation completed in {elapsed_time:.3f} seconds\")\n",
    "print(f\"\\nResults:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['category']}: ${row['total_revenue']:,.2f} ({row['count']:,} transactions)\")\n",
    "\n",
    "# 8. LAZY EVALUATION DEMONSTRATION\n",
    "print(\"\\n8. LAZY EVALUATION DEMONSTRATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"Defining transformations (no execution yet)...\")\n",
    "\n",
    "# These are all transformations (lazy)\n",
    "step1 = df_with_revenue.filter(col(\"category\") == \"Electronics\")\n",
    "step2 = step1.select(\"product\", \"revenue\")\n",
    "step3 = step2.groupBy(\"product\").agg(sum(\"revenue\").alias(\"total\"))\n",
    "step4 = step3.orderBy(col(\"total\").desc())\n",
    "\n",
    "print(\"‚úì Transformations defined (not executed yet)\")\n",
    "print(\"  - Filter Electronics\")\n",
    "print(\"  - Select columns\")\n",
    "print(\"  - Group by product\")\n",
    "print(\"  - Order by revenue\")\n",
    "\n",
    "print(\"\\nCalling action (.show()) - triggers execution...\")\n",
    "step4.show()\n",
    "print(\"‚úì Execution completed!\")\n",
    "\n",
    "# 9. ACCESSING SPARKSESSION PROPERTIES\n",
    "print(\"\\n9. ACCESSING SPARKSESSION PROPERTIES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"SparkSession Information:\")\n",
    "print(f\"  Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark Version: {spark.version}\")\n",
    "print(f\"  Master URL: {spark.sparkContext.master}\")\n",
    "print(f\"  Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "print(f\"\\nCatalog Information:\")\n",
    "print(f\"  Current Database: {spark.catalog.currentDatabase()}\")\n",
    "print(f\"  Tables in current database:\")\n",
    "for table in spark.catalog.listTables():\n",
    "    print(f\"    - {table.name} ({table.tableType})\")\n",
    "\n",
    "# 10. READING AND WRITING DATA\n",
    "print(\"\\n10. READING AND WRITING DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Write to CSV\n",
    "output_path = \"sales_output.csv\"\n",
    "print(f\"Writing DataFrame to CSV: {output_path}\")\n",
    "df_with_revenue.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "print(f\"‚úì Data written successfully\")\n",
    "\n",
    "# Read from CSV\n",
    "print(f\"\\nReading back from CSV...\")\n",
    "df_read = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(output_path)\n",
    "print(f\"‚úì Data read successfully: {df_read.count()} rows\")\n",
    "df_read.show(3)\n",
    "\n",
    "# Write to Parquet (columnar format, more efficient)\n",
    "parquet_path = \"sales_output.parquet\"\n",
    "print(f\"\\nWriting DataFrame to Parquet: {parquet_path}\")\n",
    "df_with_revenue.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "print(f\"‚úì Parquet data written successfully\")\n",
    "\n",
    "# 11. CLEANUP\n",
    "print(\"\\n11. CLEANUP\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Clean up output files\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "    print(f\"‚úì Removed {output_path}\")\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    shutil.rmtree(parquet_path)\n",
    "    print(f\"‚úì Removed {parquet_path}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "print(\"\\nStopping SparkSession...\")\n",
    "spark.stop()\n",
    "print(\"‚úì SparkSession stopped\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì SparkSession is the unified entry point for PySpark\")\n",
    "print(\"‚úì Use for big data (> 10 GB) and distributed computing\")\n",
    "print(\"‚úì Supports DataFrames, SQL, Streaming, and MLlib\")\n",
    "print(\"‚úì Lazy evaluation - transformations build execution plan\")\n",
    "print(\"‚úì Actions trigger actual computation\")\n",
    "print(\"‚úì Configure memory, partitions, and other settings\")\n",
    "print(\"‚úì Always call spark.stop() when done\")\n",
    "print(\"‚úì Use local mode for testing, cluster mode for production\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34293c43",
   "metadata": {},
   "source": [
    "# 2. SparkContext\n",
    "\n",
    "## üìñ What is SparkContext?\n",
    "\n",
    "**SparkContext** is the **low-level** connection to the Spark cluster. It's automatically created by SparkSession but can be accessed for RDD operations and cluster configuration.\n",
    "\n",
    "**Key Features:**\n",
    "- Creates RDDs (Resilient Distributed Datasets)\n",
    "- Broadcasts variables to cluster\n",
    "- Creates accumulators\n",
    "- Access to cluster configuration\n",
    "- Job scheduling and monitoring\n",
    "\n",
    "**Access Pattern:**\n",
    "```python\n",
    "sc = spark.sparkContext  # Get from SparkSession\n",
    "```\n",
    "\n",
    "## üéØ Why Use SparkContext?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Low-Level Control** - Direct access to RDD API\n",
    "2. **Performance Tuning** - Configure partitions, parallelism\n",
    "3. **Broadcast Variables** - Efficiently share read-only data\n",
    "4. **Accumulators** - Shared counters across cluster\n",
    "5. **Legacy Code** - Support for older Spark applications\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **No Optimization** - Manual query planning required\n",
    "2. **Verbose** - More code than DataFrame API\n",
    "3. **Type Safety** - No schema enforcement\n",
    "4. **Limited SQL** - Can't use SQL directly\n",
    "\n",
    "## ‚è±Ô∏è When to Use SparkContext\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Working with Unstructured Data**\n",
    "- Example: Raw text files, logs without schema\n",
    "- Why: RDDs better for unstructured data\n",
    "- Use case: `sc.textFile(\"logs.txt\")`\n",
    "\n",
    "**2. Need Low-Level Transformations**\n",
    "- Example: Custom partitioning logic\n",
    "- Why: Full control over data distribution\n",
    "- Benefit: Optimize for specific use case\n",
    "\n",
    "**3. Broadcasting Large Variables**\n",
    "- Example: Share 1 GB lookup table across cluster\n",
    "- Why: Broadcast once instead of sending with each task\n",
    "- Performance: 10-100x faster for joins\n",
    "\n",
    "**4. Shared Counters Needed**\n",
    "- Example: Count errors across all tasks\n",
    "- Why: Accumulators thread-safe and efficient\n",
    "- Use case: Monitoring, debugging\n",
    "\n",
    "**5. Graph Processing**\n",
    "- Example: Social network analysis with GraphX\n",
    "- Why: GraphX built on RDDs\n",
    "- Alternative: NetworkX for small graphs\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Structured/Tabular Data**\n",
    "- Problem: No schema, no optimization\n",
    "- Better: Use DataFrames\n",
    "- Why: Catalyst optimizer 2-10x faster\n",
    "\n",
    "**2. SQL Queries Needed**\n",
    "- Problem: Can't use SQL on RDDs\n",
    "- Better: DataFrames with SQL API\n",
    "- Why: Familiar SQL syntax\n",
    "\n",
    "**3. New Projects**\n",
    "- Problem: RDD API is legacy\n",
    "- Better: Start with DataFrames\n",
    "- Why: More features, better performance\n",
    "\n",
    "**4. Type Safety Needed**\n",
    "- Problem: RDDs don't enforce schema\n",
    "- Better: DataFrames or Datasets\n",
    "- Why: Catch errors at compile time\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**RDD Workflow:**\n",
    "1. Create RDD from data source\n",
    "2. Apply transformations (map, filter, etc.)\n",
    "3. Call action (collect, count, save)\n",
    "4. Spark builds DAG (Directed Acyclic Graph)\n",
    "5. Distributes tasks to executors\n",
    "6. Results returned to driver\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Transformations** - Lazy operations (map, filter, flatMap)\n",
    "- **Actions** - Trigger execution (collect, count, save)\n",
    "- **Lineage** - Track transformations for fault tolerance\n",
    "- **Partitions** - Data split across cluster\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Log Analysis** - Parse unstructured log files\n",
    "2. **Text Mining** - Natural language processing\n",
    "3. **Clickstream Analysis** - User behavior tracking\n",
    "4. **Graph Analytics** - Social networks, fraud detection\n",
    "5. **Sensor Data** - IoT time series processing\n",
    "6. **Legacy Systems** - Migrate old RDD code\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Access via `spark.sparkContext` (don't create directly)  \n",
    "‚úÖ Use for unstructured data and low-level operations  \n",
    "‚úÖ DataFrames preferred for structured data  \n",
    "‚úÖ Broadcast variables for large read-only data  \n",
    "‚úÖ Accumulators for shared counters  \n",
    "‚úÖ RDD lineage provides fault tolerance  \n",
    "‚úÖ Partitions controlled via `sc.parallelize(data, numPartitions)`  \n",
    "‚úÖ Convert RDD to DataFrame: `rdd.toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARKCONTEXT - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PYSPARK SPARKCONTEXT - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import re\n",
    "\n",
    "# 1. CREATE SPARKSESSION AND ACCESS SPARKCONTEXT\n",
    "print(\"\\n1. CREATING SPARKSESSION AND ACCESSING SPARKCONTEXT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkContext Tutorial\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"‚úì SparkContext accessed successfully\")\n",
    "print(f\"  Application ID: {sc.applicationId}\")\n",
    "print(f\"  App Name: {sc.appName}\")\n",
    "print(f\"  Master: {sc.master}\")\n",
    "print(f\"  Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"  Default Min Partitions: {sc.defaultMinPartitions}\")\n",
    "\n",
    "# 2. CREATING RDDs\n",
    "print(\"\\n2. CREATING RDDs (RESILIENT DISTRIBUTED DATASETS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Method 1: Parallelize a collection\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers, numSlices=4)  # 4 partitions\n",
    "\n",
    "print(f\"Method 1: Parallelize collection\")\n",
    "print(f\"  Data: {numbers}\")\n",
    "print(f\"  RDD Partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"  First 5 elements: {numbers_rdd.take(5)}\")\n",
    "\n",
    "# Method 2: From text data\n",
    "text_data = [\n",
    "    \"Apache Spark is a unified analytics engine\",\n",
    "    \"Spark provides high-level APIs in Java, Scala, Python and R\",\n",
    "    \"It also supports SQL queries, streaming data, machine learning and graph processing\"\n",
    "]\n",
    "text_rdd = sc.parallelize(text_data)\n",
    "\n",
    "print(f\"\\nMethod 2: Text data\")\n",
    "print(f\"  Number of lines: {text_rdd.count()}\")\n",
    "print(f\"  First line: {text_rdd.first()}\")\n",
    "\n",
    "# 3. RDD TRANSFORMATIONS\n",
    "print(\"\\n3. RDD TRANSFORMATIONS (LAZY)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# map - Transform each element\n",
    "print(\"map - Square each number:\")\n",
    "squared = numbers_rdd.map(lambda x: x ** 2)\n",
    "print(f\"  Original: {numbers_rdd.collect()}\")\n",
    "print(f\"  Squared: {squared.collect()}\")\n",
    "\n",
    "# filter - Keep elements matching condition\n",
    "print(\"\\nfilter - Keep even numbers only:\")\n",
    "evens = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
    "print(f\"  Even numbers: {evens.collect()}\")\n",
    "\n",
    "# flatMap - Map then flatten\n",
    "print(\"\\nflatMap - Split text into words:\")\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
    "print(f\"  Total words: {words_rdd.count()}\")\n",
    "print(f\"  First 10 words: {words_rdd.take(10)}\")\n",
    "\n",
    "# distinct - Remove duplicates\n",
    "print(\"\\ndistinct - Unique words:\")\n",
    "unique_words = words_rdd.map(lambda w: w.lower()).distinct()\n",
    "print(f\"  Unique words count: {unique_words.count()}\")\n",
    "print(f\"  Sample words: {unique_words.take(10)}\")\n",
    "\n",
    "# 4. RDD ACTIONS (TRIGGER EXECUTION)\n",
    "print(\"\\n4. RDD ACTIONS (TRIGGER EXECUTION)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# collect - Return all elements to driver\n",
    "print(\"collect - Get all elements:\")\n",
    "all_numbers = numbers_rdd.collect()\n",
    "print(f\"  {all_numbers}\")\n",
    "\n",
    "# count - Number of elements\n",
    "print(f\"\\ncount - Total elements: {numbers_rdd.count()}\")\n",
    "\n",
    "# first - First element\n",
    "print(f\"\\nfirst - First element: {numbers_rdd.first()}\")\n",
    "\n",
    "# take - First N elements\n",
    "print(f\"\\ntake(3) - First 3 elements: {numbers_rdd.take(3)}\")\n",
    "\n",
    "# reduce - Aggregate elements\n",
    "print(\"\\nreduce - Sum all numbers:\")\n",
    "total = numbers_rdd.reduce(lambda a, b: a + b)\n",
    "print(f\"  Sum: {total}\")\n",
    "\n",
    "# fold - Like reduce with initial value\n",
    "print(\"\\nfold - Sum with initial value 100:\")\n",
    "total_with_init = numbers_rdd.fold(100, lambda a, b: a + b)\n",
    "print(f\"  Sum: {total_with_init}\")\n",
    "\n",
    "# 5. WORD COUNT EXAMPLE (CLASSIC MAPREDUCE)\n",
    "print(\"\\n5. WORD COUNT EXAMPLE (CLASSIC MAPREDUCE)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Sample text\n",
    "text = [\n",
    "    \"spark is fast\",\n",
    "    \"spark is powerful\",\n",
    "    \"python is easy\",\n",
    "    \"spark and python together\"\n",
    "]\n",
    "\n",
    "text_rdd = sc.parallelize(text)\n",
    "\n",
    "# Word count pipeline\n",
    "word_counts = text_rdd \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Word Count Results:\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# 6. PAIR RDD OPERATIONS\n",
    "print(\"\\n6. PAIR RDD OPERATIONS (KEY-VALUE PAIRS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create pair RDD - Sales by category\n",
    "sales_data = [\n",
    "    (\"Electronics\", 1200),\n",
    "    (\"Books\", 50),\n",
    "    (\"Electronics\", 800),\n",
    "    (\"Clothing\", 150),\n",
    "    (\"Books\", 30),\n",
    "    (\"Electronics\", 500),\n",
    "    (\"Clothing\", 200)\n",
    "]\n",
    "\n",
    "sales_rdd = sc.parallelize(sales_data)\n",
    "\n",
    "# reduceByKey - Sum by category\n",
    "print(\"reduceByKey - Total sales by category:\")\n",
    "total_by_category = sales_rdd.reduceByKey(add)\n",
    "for category, total in total_by_category.collect():\n",
    "    print(f\"  {category}: ${total}\")\n",
    "\n",
    "# groupByKey - Group values by key\n",
    "print(\"\\ngroupByKey - All sales by category:\")\n",
    "grouped = sales_rdd.groupByKey()\n",
    "for category, sales in grouped.collect():\n",
    "    print(f\"  {category}: {list(sales)}\")\n",
    "\n",
    "# mapValues - Transform only values\n",
    "print(\"\\nmapValues - Add 10% tax:\")\n",
    "with_tax = sales_rdd.mapValues(lambda x: x * 1.10)\n",
    "print(f\"  Sample: {with_tax.take(3)}\")\n",
    "\n",
    "# 7. BROADCAST VARIABLES\n",
    "print(\"\\n7. BROADCAST VARIABLES (EFFICIENT DATA SHARING)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Large lookup table to share across cluster\n",
    "category_names = {\n",
    "    \"E\": \"Electronics\",\n",
    "    \"B\": \"Books\",\n",
    "    \"C\": \"Clothing\",\n",
    "    \"F\": \"Food\"\n",
    "}\n",
    "\n",
    "# Broadcast the dictionary\n",
    "broadcast_categories = sc.broadcast(category_names)\n",
    "\n",
    "print(f\"Broadcast variable created\")\n",
    "print(f\"  Size: {len(broadcast_categories.value)} categories\")\n",
    "print(f\"  Value: {broadcast_categories.value}\")\n",
    "\n",
    "# Use broadcast variable\n",
    "category_codes = [(\"E\", 100), (\"B\", 50), (\"C\", 75), (\"E\", 150)]\n",
    "codes_rdd = sc.parallelize(category_codes)\n",
    "\n",
    "# Map codes to names using broadcast variable\n",
    "with_names = codes_rdd.map(\n",
    "    lambda x: (broadcast_categories.value[x[0]], x[1])\n",
    ")\n",
    "\n",
    "print(\"\\nUsing broadcast variable to map codes:\")\n",
    "for category, value in with_names.collect():\n",
    "    print(f\"  {category}: ${value}\")\n",
    "\n",
    "# 8. ACCUMULATORS (SHARED COUNTERS)\n",
    "print(\"\\n8. ACCUMULATORS (SHARED COUNTERS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create accumulator\n",
    "error_count = sc.accumulator(0)\n",
    "valid_count = sc.accumulator(0)\n",
    "\n",
    "# Data with some invalid entries\n",
    "data_with_errors = [\"10\", \"20\", \"invalid\", \"30\", \"error\", \"40\", \"50\"]\n",
    "data_rdd = sc.parallelize(data_with_errors)\n",
    "\n",
    "def parse_number(value):\n",
    "    try:\n",
    "        num = int(value)\n",
    "        valid_count.add(1)\n",
    "        return num\n",
    "    except ValueError:\n",
    "        error_count.add(1)\n",
    "        return None\n",
    "\n",
    "# Process data (accumulators updated)\n",
    "parsed = data_rdd.map(parse_number).filter(lambda x: x is not None)\n",
    "result = parsed.collect()  # Trigger action\n",
    "\n",
    "print(f\"Processing complete:\")\n",
    "print(f\"  Valid numbers: {valid_count.value}\")\n",
    "print(f\"  Errors: {error_count.value}\")\n",
    "print(f\"  Parsed values: {result}\")\n",
    "\n",
    "# 9. PARTITIONING\n",
    "print(\"\\n9. PARTITIONING CONTROL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create RDD with specific partitions\n",
    "data = list(range(1, 101))  # 1 to 100\n",
    "rdd_2_partitions = sc.parallelize(data, 2)\n",
    "rdd_4_partitions = sc.parallelize(data, 4)\n",
    "rdd_8_partitions = sc.parallelize(data, 8)\n",
    "\n",
    "print(f\"Same data with different partitions:\")\n",
    "print(f\"  2 partitions: {rdd_2_partitions.getNumPartitions()}\")\n",
    "print(f\"  4 partitions: {rdd_4_partitions.getNumPartitions()}\")\n",
    "print(f\"  8 partitions: {rdd_8_partitions.getNumPartitions()}\")\n",
    "\n",
    "# repartition - Increase partitions (shuffle)\n",
    "repartitioned = rdd_2_partitions.repartition(8)\n",
    "print(f\"\\nAfter repartition(8): {repartitioned.getNumPartitions()} partitions\")\n",
    "\n",
    "# coalesce - Decrease partitions (no shuffle)\n",
    "coalesced = rdd_8_partitions.coalesce(2)\n",
    "print(f\"After coalesce(2): {coalesced.getNumPartitions()} partitions\")\n",
    "\n",
    "# 10. RDD PERSISTENCE\n",
    "print(\"\\n10. RDD PERSISTENCE (CACHING)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create expensive computation\n",
    "expensive_rdd = sc.parallelize(range(1000000)).map(lambda x: x ** 2)\n",
    "\n",
    "print(\"Without caching:\")\n",
    "import time\n",
    "start = time.time()\n",
    "count1 = expensive_rdd.count()\n",
    "time1 = time.time() - start\n",
    "print(f\"  First count: {time1:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "count2 = expensive_rdd.count()\n",
    "time2 = time.time() - start\n",
    "print(f\"  Second count: {time2:.3f} seconds (recomputed)\")\n",
    "\n",
    "# Cache the RDD\n",
    "expensive_rdd.cache()\n",
    "\n",
    "print(\"\\nWith caching:\")\n",
    "start = time.time()\n",
    "count3 = expensive_rdd.count()\n",
    "time3 = time.time() - start\n",
    "print(f\"  First count (cached): {time3:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "count4 = expensive_rdd.count()\n",
    "time4 = time.time() - start\n",
    "print(f\"  Second count (from cache): {time4:.3f} seconds (faster!)\")\n",
    "\n",
    "# Unpersist when done\n",
    "expensive_rdd.unpersist()\n",
    "\n",
    "# 11. CLEANUP\n",
    "print(\"\\n11. CLEANUP\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Unpersist broadcast variable\n",
    "broadcast_categories.unpersist()\n",
    "print(\"‚úì Broadcast variable unpersisted\")\n",
    "\n",
    "# Stop SparkContext (via SparkSession)\n",
    "spark.stop()\n",
    "print(\"‚úì SparkSession stopped\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì SparkContext provides low-level RDD API\")\n",
    "print(\"‚úì Access via spark.sparkContext (don't create directly)\")\n",
    "print(\"‚úì RDDs good for unstructured data, custom logic\")\n",
    "print(\"‚úì Transformations are lazy (map, filter, flatMap)\")\n",
    "print(\"‚úì Actions trigger execution (collect, count, reduce)\")\n",
    "print(\"‚úì Broadcast variables for efficient data sharing\")\n",
    "print(\"‚úì Accumulators for shared counters\")\n",
    "print(\"‚úì Control partitioning with parallelize, repartition, coalesce\")\n",
    "print(\"‚úì Cache expensive computations with .cache()\")\n",
    "print(\"‚úì Use DataFrames for structured data (preferred)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
