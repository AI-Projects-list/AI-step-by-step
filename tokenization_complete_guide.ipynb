{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1e5489",
   "metadata": {},
   "source": [
    "# 1. What is Tokenization?\n",
    "\n",
    "## üìñ What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking down text into smaller units called \"tokens\" - these can be words, subwords, characters, or even sentences.\n",
    "\n",
    "**Token Examples:**\n",
    "- **Text**: \"Hello, world!\"\n",
    "- **Word tokens**: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "- **Subword tokens**: [\"Hello\", \",\", \"wo\", \"##rld\", \"!\"]\n",
    "- **Character tokens**: [\"H\", \"e\", \"l\", \"l\", \"o\", \",\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\", \"!\"]\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Input**: Raw text string\n",
    "- **Output**: List of tokens + metadata (positions, IDs, attention masks)\n",
    "- **Reversible**: Can reconstruct original text from tokens\n",
    "- **Language-dependent**: Different rules for different languages\n",
    "\n",
    "## üéØ Why Use Tokenization?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Machine-Readable Format** - Convert text to numerical IDs for ML models\n",
    "2. **Vocabulary Management** - Handle millions of words with fixed vocabulary\n",
    "3. **OOV Handling** - Subword tokenization handles unknown words\n",
    "4. **Language Agnostic** - Works across different languages\n",
    "5. **Feature Engineering** - Tokens become features for ML\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Information Loss** - May lose subtle nuances\n",
    "2. **Context Dependency** - Same word, different meanings\n",
    "3. **Language Specific** - Needs different strategies per language\n",
    "4. **Ambiguity** - \"New York\" vs [\"New\", \"York\"]\n",
    "\n",
    "## ‚è±Ô∏è When to Use Tokenization\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Text Classification**\n",
    "- Example: Spam detection, sentiment analysis\n",
    "- Why: Need to convert text to features\n",
    "- Benefit: Model can learn from word patterns\n",
    "\n",
    "**2. Named Entity Recognition (NER)**\n",
    "- Example: Extract names, dates, locations\n",
    "- Why: Need word-level or subword-level processing\n",
    "- Use case: \"Apple Inc. in Cupertino\" ‚Üí [\"Apple\", \"Inc.\", \"Cupertino\"]\n",
    "\n",
    "**3. Machine Translation**\n",
    "- Example: English to French translation\n",
    "- Why: Need to map words/subwords across languages\n",
    "- Benefit: Subword tokens handle morphology\n",
    "\n",
    "**4. Question Answering**\n",
    "- Example: BERT for SQuAD dataset\n",
    "- Why: Need to align question and context tokens\n",
    "- Use case: Find answer span in token positions\n",
    "\n",
    "**5. Text Generation**\n",
    "- Example: GPT-3, ChatGPT\n",
    "- Why: Generate text token by token\n",
    "- Use case: Predict next token given previous tokens\n",
    "\n",
    "**6. Search and Information Retrieval**\n",
    "- Example: Google Search, Elasticsearch\n",
    "- Why: Index documents by tokens\n",
    "- Benefit: Fast keyword matching\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Simple String Matching**\n",
    "- Problem: Overhead for exact matches\n",
    "- Better: Use string.find() or regex\n",
    "- Why: Tokenization adds unnecessary complexity\n",
    "\n",
    "**2. Character-Level Tasks Only**\n",
    "- Problem: Word tokenization loses information\n",
    "- Better: Use character embeddings directly\n",
    "- Why: Some tasks need character-level features\n",
    "\n",
    "**3. Very Short Text**\n",
    "- Problem: Single word doesn't need tokenization\n",
    "- Better: Process directly\n",
    "- Why: \"Yes\" ‚Üí [\"Yes\"] adds no value\n",
    "\n",
    "**4. Binary/Non-Text Data**\n",
    "- Problem: Tokenization is for text\n",
    "- Better: Use appropriate encoding (images ‚Üí pixels)\n",
    "- Why: Not designed for non-linguistic data\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Tokenization Pipeline:**\n",
    "1. **Normalization**: Lowercase, remove accents, etc.\n",
    "2. **Pre-tokenization**: Split by whitespace/punctuation\n",
    "3. **Tokenization**: Apply algorithm (BPE, WordPiece, etc.)\n",
    "4. **Post-processing**: Add special tokens ([CLS], [SEP])\n",
    "5. **Conversion**: Map tokens to IDs from vocabulary\n",
    "6. **Padding/Truncation**: Make sequences same length\n",
    "\n",
    "**Example Flow:**\n",
    "```\n",
    "Text: \"I love NLP!\"\n",
    "  ‚Üì Normalize\n",
    "\"i love nlp!\"\n",
    "  ‚Üì Pre-tokenize\n",
    "[\"i\", \"love\", \"nlp\", \"!\"]\n",
    "  ‚Üì Tokenize (subword)\n",
    "[\"i\", \"love\", \"nl\", \"##p\", \"!\"]\n",
    "  ‚Üì Add special tokens\n",
    "[\"[CLS]\", \"i\", \"love\", \"nl\", \"##p\", \"!\", \"[SEP]\"]\n",
    "  ‚Üì Convert to IDs\n",
    "[101, 1045, 2293, 17953, 2361, 999, 102]\n",
    "```\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Search Engines** - Google, Bing tokenize queries and documents\n",
    "2. **Chatbots** - ChatGPT, Alexa tokenize user input\n",
    "3. **Translation** - Google Translate, DeepL\n",
    "4. **Sentiment Analysis** - Twitter sentiment, product reviews\n",
    "5. **Email Filtering** - Gmail spam detection\n",
    "6. **Content Moderation** - Facebook, YouTube content filtering\n",
    "7. **Voice Assistants** - Siri, Alexa process transcribed speech\n",
    "8. **Legal Tech** - Contract analysis, e-discovery\n",
    "9. **Healthcare** - Clinical notes processing, medical coding\n",
    "10. **Finance** - News sentiment analysis, fraud detection\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Tokenization is the **first step** in all NLP pipelines  \n",
    "‚úÖ Choice of tokenizer affects model performance significantly  \n",
    "‚úÖ **Subword tokenization** (BPE, WordPiece) dominates modern NLP  \n",
    "‚úÖ Always use **same tokenizer** for training and inference  \n",
    "‚úÖ **Special tokens** ([CLS], [SEP], [PAD]) are task-specific  \n",
    "‚úÖ Fast tokenizers (Rust-based) are 10x faster than Python  \n",
    "‚úÖ Vocabulary size is a tradeoff: larger = better coverage, slower  \n",
    "‚úÖ **Context matters**: \"bank\" (river) vs \"bank\" (financial)  \n",
    "‚úÖ Multilingual models use shared subword vocabulary  \n",
    "‚úÖ Save tokenizer config with model for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION - COMPLETE INTRODUCTION\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOKENIZATION FUNDAMENTALS - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# NOTE: Install required libraries first\n",
    "# pip install nltk spacy transformers\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1. BASIC TOKENIZATION CONCEPTS\n",
    "print(\"\\n1. BASIC TOKENIZATION CONCEPTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing is amazing.\"\n",
    "print(f\"Original text: {text}\")\n",
    "\n",
    "# Method 1: Simple split by whitespace\n",
    "tokens_whitespace = text.split()\n",
    "print(f\"\\nWhitespace tokenization: {tokens_whitespace}\")\n",
    "print(f\"  Token count: {len(tokens_whitespace)}\")\n",
    "print(f\"  Problem: Punctuation attached to words!\")\n",
    "\n",
    "# Method 2: Split by whitespace and punctuation\n",
    "tokens_punct = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "print(f\"\\nPunctuation-aware tokenization: {tokens_punct}\")\n",
    "print(f\"  Token count: {len(tokens_punct)}\")\n",
    "print(f\"  Better: Punctuation separated!\")\n",
    "\n",
    "# Method 3: Split only words (ignore punctuation)\n",
    "tokens_words = re.findall(r'\\w+', text)\n",
    "print(f\"\\nWord-only tokenization: {tokens_words}\")\n",
    "print(f\"  Token count: {len(tokens_words)}\")\n",
    "print(f\"  Issue: Lost punctuation information!\")\n",
    "\n",
    "# 2. CHARACTER-LEVEL TOKENIZATION\n",
    "print(\"\\n2. CHARACTER-LEVEL TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_short = \"Hello\"\n",
    "char_tokens = list(text_short)\n",
    "print(f\"Text: {text_short}\")\n",
    "print(f\"Character tokens: {char_tokens}\")\n",
    "print(f\"Token count: {len(char_tokens)}\")\n",
    "print(f\"\\nUse case: Language modeling, spell checking, OCR\")\n",
    "\n",
    "# Reconstruct text from characters\n",
    "reconstructed = ''.join(char_tokens)\n",
    "print(f\"Reconstructed: {reconstructed}\")\n",
    "print(f\"Match: {reconstructed == text_short}\")\n",
    "\n",
    "# 3. SENTENCE TOKENIZATION\n",
    "print(\"\\n3. SENTENCE TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "paragraph = \"\"\"Natural Language Processing is exciting. It enables computers to understand text.\n",
    "Machine learning models need tokenization. This is a fundamental step!\"\"\"\n",
    "\n",
    "print(f\"Original paragraph:\\n{paragraph}\")\n",
    "\n",
    "# Simple sentence split (basic)\n",
    "sentences_basic = paragraph.split('.')\n",
    "sentences_basic = [s.strip() for s in sentences_basic if s.strip()]\n",
    "print(f\"\\nBasic sentence split:\")\n",
    "for i, sent in enumerate(sentences_basic, 1):\n",
    "    print(f\"  {i}. {sent}\")\n",
    "\n",
    "# Better: Regex-based sentence tokenization\n",
    "sentences_regex = re.split(r'[.!?]+', paragraph)\n",
    "sentences_regex = [s.strip() for s in sentences_regex if s.strip()]\n",
    "print(f\"\\nRegex sentence split:\")\n",
    "for i, sent in enumerate(sentences_regex, 1):\n",
    "    print(f\"  {i}. {sent}\")\n",
    "\n",
    "# 4. TOKENIZATION WITH NLTK\n",
    "print(\"\\n4. TOKENIZATION WITH NLTK\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    # Download required data (run once)\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('punkt_tab')\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    \n",
    "    text_nltk = \"Dr. Smith works at N.Y.U. He earned $50,000 last year!\"\n",
    "    print(f\"Text: {text_nltk}\")\n",
    "    \n",
    "    # Word tokenization\n",
    "    tokens_nltk = word_tokenize(text_nltk)\n",
    "    print(f\"\\nNLTK word tokens: {tokens_nltk}\")\n",
    "    print(f\"  Handles: abbreviations (Dr., N.Y.U.), currency ($), punctuation\")\n",
    "    \n",
    "    # Sentence tokenization\n",
    "    paragraph_nltk = \"I love NLP. It's amazing! Do you agree? Yes, I do.\"\n",
    "    sentences_nltk = sent_tokenize(paragraph_nltk)\n",
    "    print(f\"\\nNLTK sentence tokens:\")\n",
    "    for i, sent in enumerate(sentences_nltk, 1):\n",
    "        print(f\"  {i}. {sent}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"NLTK not installed. Install with: pip install nltk\")\n",
    "except LookupError:\n",
    "    print(\"NLTK data not downloaded. Run: nltk.download('punkt')\")\n",
    "\n",
    "# 5. TOKENIZATION WITH SPACY\n",
    "print(\"\\n5. TOKENIZATION WITH SPACY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    # Load English model (run once: python -m spacy download en_core_web_sm)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    text_spacy = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
    "    print(f\"Text: {text_spacy}\")\n",
    "    \n",
    "    # Process text\n",
    "    doc = nlp(text_spacy)\n",
    "    \n",
    "    # Extract tokens with attributes\n",
    "    print(f\"\\nspaCy tokens with attributes:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text:15s} | POS: {token.pos_:10s} | Lemma: {token.lemma_:15s} | Stop: {token.is_stop}\")\n",
    "    \n",
    "    # Just token text\n",
    "    tokens_spacy = [token.text for token in doc]\n",
    "    print(f\"\\nToken list: {tokens_spacy}\")\n",
    "    \n",
    "    # Sentence segmentation\n",
    "    text_sents = \"I love NLP. It's powerful. What do you think?\"\n",
    "    doc_sents = nlp(text_sents)\n",
    "    print(f\"\\nspaCy sentences:\")\n",
    "    for i, sent in enumerate(doc_sents.sents, 1):\n",
    "        print(f\"  {i}. {sent.text}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"spaCy not installed. Install with: pip install spacy\")\n",
    "except OSError:\n",
    "    print(\"spaCy model not downloaded. Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# 6. SUBWORD TOKENIZATION (SIMPLE BPE CONCEPT)\n",
    "print(\"\\n6. SUBWORD TOKENIZATION CONCEPT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simulate simple subword tokenization\n",
    "text_rare = \"unhappiness\"\n",
    "print(f\"Word: {text_rare}\")\n",
    "print(f\"\\nProblem: Rare word, might not be in vocabulary\")\n",
    "print(f\"\\nSolution: Split into subwords\")\n",
    "print(f\"  Subwords: ['un', 'happiness'] or ['un', 'happi', 'ness']\")\n",
    "print(f\"  Benefit: Common subwords likely in vocabulary\")\n",
    "print(f\"  Meaning: Preserved through subword composition\")\n",
    "\n",
    "# Manual demonstration\n",
    "subwords = ['un', 'happi', 'ness']\n",
    "print(f\"\\nSubword tokens: {subwords}\")\n",
    "print(f\"  'un' ‚Üí negative prefix\")\n",
    "print(f\"  'happi' ‚Üí root word (happy)\")\n",
    "print(f\"  'ness' ‚Üí noun suffix\")\n",
    "\n",
    "# 7. TRANSFORMER TOKENIZATION (BERT)\n",
    "print(\"\\n7. TRANSFORMER TOKENIZATION (BERT)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    from transformers import BertTokenizer\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    text_bert = \"Tokenization is fundamental for NLP!\"\n",
    "    print(f\"Text: {text_bert}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text_bert)\n",
    "    print(f\"\\nBERT tokens: {tokens}\")\n",
    "    print(f\"  Note: ## prefix indicates subword continuation\")\n",
    "    \n",
    "    # Convert to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"\\nToken IDs: {token_ids}\")\n",
    "    \n",
    "    # Full encoding (includes special tokens)\n",
    "    encoding = tokenizer(text_bert, \n",
    "                        add_special_tokens=True,\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True)\n",
    "    \n",
    "    print(f\"\\nFull encoding:\")\n",
    "    print(f\"  Input IDs: {encoding['input_ids'].tolist()}\")\n",
    "    print(f\"  Attention mask: {encoding['attention_mask'].tolist()}\")\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded = tokenizer.decode(encoding['input_ids'][0])\n",
    "    print(f\"\\nDecoded text: {decoded}\")\n",
    "    print(f\"  [CLS] = Classification token (start)\")\n",
    "    print(f\"  [SEP] = Separator token (end)\")\n",
    "    \n",
    "    # Vocabulary info\n",
    "    print(f\"\\nVocabulary size: {tokenizer.vocab_size:,}\")\n",
    "    print(f\"Max length: {tokenizer.model_max_length:,} tokens\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Transformers not installed. Install with: pip install transformers\")\n",
    "\n",
    "# 8. TOKENIZATION STATISTICS\n",
    "print(\"\\n8. TOKENIZATION STATISTICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "corpus = \"\"\"Machine learning is a subset of artificial intelligence. \n",
    "It enables computers to learn from data. Deep learning is a subset of machine learning.\n",
    "Neural networks are the foundation of deep learning.\"\"\"\n",
    "\n",
    "print(f\"Corpus:\\n{corpus}\")\n",
    "\n",
    "# Simple tokenization\n",
    "tokens_stats = re.findall(r'\\w+', corpus.lower())\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Total tokens: {len(tokens_stats)}\")\n",
    "print(f\"  Unique tokens: {len(set(tokens_stats))}\")\n",
    "\n",
    "# Token frequency\n",
    "token_freq = Counter(tokens_stats)\n",
    "print(f\"\\nTop 10 most frequent tokens:\")\n",
    "for token, count in token_freq.most_common(10):\n",
    "    print(f\"  {token:15s}: {count} times\")\n",
    "\n",
    "# Average token length\n",
    "avg_length = sum(len(t) for t in tokens_stats) / len(tokens_stats)\n",
    "print(f\"\\nAverage token length: {avg_length:.2f} characters\")\n",
    "\n",
    "# 9. PRACTICAL EXAMPLE: TEXT PREPROCESSING PIPELINE\n",
    "print(\"\\n9. PRACTICAL EXAMPLE: TEXT PREPROCESSING PIPELINE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    \"\"\"Complete preprocessing and tokenization pipeline\"\"\"\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Step 2: Remove special characters (keep alphanumeric and spaces)\n",
    "    text_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', text_lower)\n",
    "    \n",
    "    # Step 3: Tokenize\n",
    "    tokens = text_clean.split()\n",
    "    \n",
    "    # Step 4: Remove stop words (simplified list)\n",
    "    stop_words = {'is', 'a', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}\n",
    "    tokens_filtered = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return tokens_filtered\n",
    "\n",
    "sample_text = \"Natural Language Processing is the key to understanding text!\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "\n",
    "processed_tokens = preprocess_and_tokenize(sample_text)\n",
    "print(f\"Processed tokens: {processed_tokens}\")\n",
    "print(f\"\\nSteps applied:\")\n",
    "print(f\"  1. Lowercased\")\n",
    "print(f\"  2. Removed punctuation\")\n",
    "print(f\"  3. Split into words\")\n",
    "print(f\"  4. Removed stop words\")\n",
    "\n",
    "# 10. COMPARISON OF TOKENIZATION METHODS\n",
    "print(\"\\n10. COMPARISON OF TOKENIZATION METHODS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "test_text = \"Don't split contractions! Email: user@example.com\"\n",
    "print(f\"Test text: {test_text}\")\n",
    "\n",
    "# Method 1: Split\n",
    "method1 = test_text.split()\n",
    "print(f\"\\nMethod 1 (split): {method1}\")\n",
    "print(f\"  Problem: Punctuation attached, email not handled\")\n",
    "\n",
    "# Method 2: Regex word boundaries\n",
    "method2 = re.findall(r'\\b\\w+\\b', test_text)\n",
    "print(f\"\\nMethod 2 (regex \\\\b\\\\w+\\\\b): {method2}\")\n",
    "print(f\"  Problem: Lost punctuation, split contractions\")\n",
    "\n",
    "# Method 3: Regex with punctuation\n",
    "method3 = re.findall(r\"\\w+(?:[-']\\w+)*|[^\\w\\s]\", test_text)\n",
    "print(f\"\\nMethod 3 (regex with contractions): {method3}\")\n",
    "print(f\"  Better: Keeps contractions, separates punctuation\")\n",
    "\n",
    "# Method 4: NLTK (if available)\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    method4 = word_tokenize(test_text)\n",
    "    print(f\"\\nMethod 4 (NLTK): {method4}\")\n",
    "    print(f\"  Best: Handles contractions, emails, punctuation intelligently\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Tokenization converts text into processable units (tokens)\")\n",
    "print(\"‚úì Different methods: whitespace, regex, NLTK, spaCy, transformers\")\n",
    "print(\"‚úì Character-level: Fine-grained, large vocab\")\n",
    "print(\"‚úì Word-level: Intuitive, OOV problems\")\n",
    "print(\"‚úì Subword-level: Best of both worlds (BPE, WordPiece)\")\n",
    "print(\"‚úì NLTK: Good for traditional NLP tasks\")\n",
    "print(\"‚úì spaCy: Fast, production-ready, includes POS tagging\")\n",
    "print(\"‚úì Transformers (BERT, GPT): State-of-the-art, subword tokenization\")\n",
    "print(\"‚úì Choose tokenizer based on: task, language, vocabulary size\")\n",
    "print(\"‚úì Always use same tokenizer for training and inference!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8da7ba",
   "metadata": {},
   "source": [
    "# 2. Whitespace Tokenization\n",
    "\n",
    "## üìñ What is Whitespace Tokenization?\n",
    "\n",
    "**Whitespace tokenization** splits text into tokens based on whitespace characters (spaces, tabs, newlines).\n",
    "\n",
    "**Method:**\n",
    "```python\n",
    "tokens = text.split()  # Split on any whitespace\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Input: `\"Hello world! How are you?\"`\n",
    "- Output: `[\"Hello\", \"world!\", \"How\", \"are\", \"you?\"]`\n",
    "\n",
    "**Key Features:**\n",
    "- Simplest tokenization method\n",
    "- Fast and memory efficient\n",
    "- Language agnostic\n",
    "- Preserves punctuation with words\n",
    "\n",
    "## üéØ Why Use Whitespace Tokenization?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Simplicity** - One line of code: `text.split()`\n",
    "2. **Speed** - Fastest tokenization method\n",
    "3. **No Dependencies** - Built-in Python function\n",
    "4. **Predictable** - Easy to understand behavior\n",
    "5. **Memory Efficient** - Minimal overhead\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Punctuation Issues** - \"world!\" includes punctuation\n",
    "2. **Contractions** - \"don't\" stays as one token\n",
    "3. **No Special Handling** - URLs, emails treated naively\n",
    "4. **Language Limitations** - Fails for languages without spaces (Chinese, Japanese)\n",
    "\n",
    "## ‚è±Ô∏è When to Use Whitespace Tokenization\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Quick Prototyping**\n",
    "- Example: Initial data exploration\n",
    "- Why: Get quick token count, basic stats\n",
    "- Benefit: No setup required\n",
    "\n",
    "**2. Pre-cleaned Text**\n",
    "- Example: Text already processed (punctuation removed)\n",
    "- Why: Whitespace split is sufficient\n",
    "- Use case: \"hello world how are you\" ‚Üí [\"hello\", \"world\", \"how\", \"are\", \"you\"]\n",
    "\n",
    "**3. Simple Bag-of-Words Models**\n",
    "- Example: Basic text classification\n",
    "- Why: Don't need sophisticated tokenization\n",
    "- Performance: Good enough for simple tasks\n",
    "\n",
    "**4. Code/Log Processing**\n",
    "- Example: Parse log files already space-separated\n",
    "- Why: Structure already well-defined\n",
    "- Use case: \"2024-12-11 ERROR Failed\" ‚Üí [\"2024-12-11\", \"ERROR\", \"Failed\"]\n",
    "\n",
    "**5. Performance-Critical Applications**\n",
    "- Example: Process millions of documents quickly\n",
    "- Why: 10-100x faster than complex tokenizers\n",
    "- Benefit: Low latency, high throughput\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Need Punctuation Separation**\n",
    "- Problem: \"world!\" vs \"world\" treated differently\n",
    "- Better: Use NLTK or regex tokenization\n",
    "- Why: Punctuation affects meaning and frequency counts\n",
    "\n",
    "**2. Handling Contractions**\n",
    "- Problem: \"don't\" should be [\"do\", \"n't\"] or [\"do\", \"not\"]\n",
    "- Better: Use TreeBank tokenizer\n",
    "- Why: Contractions need special handling\n",
    "\n",
    "**3. Production NLP Systems**\n",
    "- Problem: Too naive for real-world text\n",
    "- Better: Use spaCy or transformers\n",
    "- Why: Need robust handling of edge cases\n",
    "\n",
    "**4. Languages Without Spaces**\n",
    "- Problem: Chinese, Japanese have no word boundaries\n",
    "- Better: Use language-specific tokenizers (Jieba for Chinese)\n",
    "- Why: Whitespace tokenization fails completely\n",
    "\n",
    "**5. Academic/Research Work**\n",
    "- Problem: Not reproducible, not standard\n",
    "- Better: Use established tokenizers (NLTK, spaCy)\n",
    "- Why: Need consistency with published benchmarks\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Algorithm:**\n",
    "1. Scan text left to right\n",
    "2. When whitespace found, split\n",
    "3. Collect non-whitespace sequences as tokens\n",
    "4. Return list of tokens\n",
    "\n",
    "**Whitespace Characters:**\n",
    "- Space: ` `\n",
    "- Tab: `\\t`\n",
    "- Newline: `\\n`\n",
    "- Carriage return: `\\r`\n",
    "\n",
    "**Time Complexity:** O(n) where n is text length  \n",
    "**Space Complexity:** O(n) for token storage\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Log Analysis** - Parse server logs, error messages\n",
    "2. **Data Exploration** - Quick token counts for EDA\n",
    "3. **Simple Search** - Basic keyword search engines\n",
    "4. **Word Clouds** - Generate visualizations quickly\n",
    "5. **Code Parsing** - Split code into identifiers\n",
    "6. **CSV Processing** - Split tab/space-separated values\n",
    "7. **Command Line Parsing** - Split shell commands\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Fastest tokenization method available  \n",
    "‚úÖ Use `.split()` with no args (splits on any whitespace)  \n",
    "‚úÖ Use `.split(' ')` to split only on spaces  \n",
    "‚úÖ Handles multiple consecutive spaces automatically  \n",
    "‚úÖ Removes leading/trailing whitespace  \n",
    "‚úÖ Good for **initial exploration**, not production  \n",
    "‚úÖ Combine with `.lower()` for case-insensitive matching  \n",
    "‚úÖ Consider `.strip()` to remove edge whitespace first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaaed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHITESPACE TOKENIZATION - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WHITESPACE TOKENIZATION - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# 1. BASIC WHITESPACE TOKENIZATION\n",
    "print(\"\\n1. BASIC WHITESPACE TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text = \"Hello world! How are you today?\"\n",
    "print(f\"Text: {text}\")\n",
    "\n",
    "# Simple split\n",
    "tokens = text.split()\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"\\nObservation: Punctuation attached to words (world!, today?)\")\n",
    "\n",
    "# 2. DIFFERENT WHITESPACE CHARACTERS\n",
    "print(\"\\n2. HANDLING DIFFERENT WHITESPACE TYPES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Multiple spaces\n",
    "text_spaces = \"Hello    world     with    spaces\"\n",
    "print(f\"Text with multiple spaces: '{text_spaces}'\")\n",
    "tokens_spaces = text_spaces.split()\n",
    "print(f\"Tokens: {tokens_spaces}\")\n",
    "print(f\"Note: Multiple spaces treated as single separator\")\n",
    "\n",
    "# Tabs\n",
    "text_tabs = \"Hello\\tworld\\twith\\ttabs\"\n",
    "print(f\"\\nText with tabs: '{text_tabs}'\")\n",
    "tokens_tabs = text_tabs.split()\n",
    "print(f\"Tokens: {tokens_tabs}\")\n",
    "\n",
    "# Newlines\n",
    "text_newlines = \"Hello\\nworld\\nwith\\nnewlines\"\n",
    "print(f\"\\nText with newlines: '{text_newlines}'\")\n",
    "tokens_newlines = text_newlines.split()\n",
    "print(f\"Tokens: {tokens_newlines}\")\n",
    "\n",
    "# Mixed whitespace\n",
    "text_mixed = \"Hello\\t  world\\n  with   mixed\\r\\nwhitespace\"\n",
    "print(f\"\\nText with mixed whitespace: '{text_mixed}'\")\n",
    "tokens_mixed = text_mixed.split()\n",
    "print(f\"Tokens: {tokens_mixed}\")\n",
    "print(f\"Note: .split() handles all whitespace types automatically\")\n",
    "\n",
    "# 3. SPLIT WITH SPECIFIC SEPARATOR\n",
    "print(\"\\n3. SPLIT WITH SPECIFIC SEPARATOR\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_sep = \"apple,banana,orange,grape\"\n",
    "print(f\"CSV text: {text_sep}\")\n",
    "\n",
    "# Split on comma\n",
    "tokens_comma = text_sep.split(',')\n",
    "print(f\"Split on comma: {tokens_comma}\")\n",
    "\n",
    "# Split only on single space (not tab, newline)\n",
    "text_space = \"hello world\\twith\\ttabs\"\n",
    "print(f\"\\nText: '{text_space}'\")\n",
    "tokens_space_only = text_space.split(' ')\n",
    "print(f\"Split on space only: {tokens_space_only}\")\n",
    "print(f\"Note: Tabs preserved in tokens\")\n",
    "\n",
    "# 4. LEADING/TRAILING WHITESPACE\n",
    "print(\"\\n4. LEADING/TRAILING WHITESPACE HANDLING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_edges = \"   Hello world   \"\n",
    "print(f\"Text with edge spaces: '{text_edges}'\")\n",
    "\n",
    "# Without strip\n",
    "tokens_no_strip = text_edges.split()\n",
    "print(f\"Tokens: {tokens_no_strip}\")\n",
    "print(f\"Note: .split() automatically removes leading/trailing whitespace\")\n",
    "\n",
    "# Explicit strip (redundant but clear)\n",
    "tokens_strip = text_edges.strip().split()\n",
    "print(f\"With .strip(): {tokens_strip}\")\n",
    "print(f\"Same result!\")\n",
    "\n",
    "# 5. CASE SENSITIVITY\n",
    "print(\"\\n5. CASE SENSITIVITY IN TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_case = \"Python python PYTHON PyThOn\"\n",
    "print(f\"Text: {text_case}\")\n",
    "\n",
    "# Case-sensitive\n",
    "tokens_case = text_case.split()\n",
    "print(f\"\\nCase-sensitive tokens: {tokens_case}\")\n",
    "print(f\"Unique tokens: {len(set(tokens_case))}\")\n",
    "\n",
    "# Case-insensitive\n",
    "tokens_lower = text_case.lower().split()\n",
    "print(f\"\\nLowercase tokens: {tokens_lower}\")\n",
    "print(f\"Unique tokens: {len(set(tokens_lower))}\")\n",
    "print(f\"Note: All variants become 'python'\")\n",
    "\n",
    "# 6. PERFORMANCE COMPARISON\n",
    "print(\"\\n6. PERFORMANCE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Generate large text\n",
    "large_text = \" \".join([\"word\"] * 100000)  # 100K words\n",
    "print(f\"Text size: {len(large_text):,} characters, ~100,000 words\")\n",
    "\n",
    "# Whitespace tokenization\n",
    "start = time.time()\n",
    "tokens_ws = large_text.split()\n",
    "time_ws = time.time() - start\n",
    "print(f\"\\nWhitespace tokenization: {time_ws:.6f} seconds\")\n",
    "print(f\"Tokens: {len(tokens_ws):,}\")\n",
    "\n",
    "# Compare with list comprehension (slower)\n",
    "start = time.time()\n",
    "tokens_loop = []\n",
    "current = \"\"\n",
    "for char in large_text:\n",
    "    if char == ' ':\n",
    "        if current:\n",
    "            tokens_loop.append(current)\n",
    "            current = \"\"\n",
    "    else:\n",
    "        current += char\n",
    "if current:\n",
    "    tokens_loop.append(current)\n",
    "time_loop = time.time() - start\n",
    "print(f\"\\nManual loop: {time_loop:.6f} seconds\")\n",
    "print(f\"Speedup: {time_loop/time_ws:.1f}x faster with .split()\")\n",
    "\n",
    "# 7. HANDLING PUNCTUATION\n",
    "print(\"\\n7. PUNCTUATION HANDLING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_punct = \"Hello, world! How are you? I'm fine.\"\n",
    "print(f\"Text: {text_punct}\")\n",
    "\n",
    "tokens_punct = text_punct.split()\n",
    "print(f\"\\nTokens: {tokens_punct}\")\n",
    "print(f\"\\nIssues:\")\n",
    "print(f\"  - 'Hello,' includes comma\")\n",
    "print(f\"  - 'world!' includes exclamation\")\n",
    "print(f\"  - 'you?' includes question mark\")\n",
    "print(f\"  - 'I'm' is single token (contraction)\")\n",
    "\n",
    "# Remove punctuation manually\n",
    "import string\n",
    "text_no_punct = text_punct.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens_clean = text_no_punct.split()\n",
    "print(f\"\\nAfter removing punctuation: {tokens_clean}\")\n",
    "print(f\"Note: 'I'm' became 'Im' (lost apostrophe)\")\n",
    "\n",
    "# 8. FREQUENCY ANALYSIS\n",
    "print(\"\\n8. TOKEN FREQUENCY ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_freq = \"\"\"Machine learning is amazing. Machine learning transforms data.\n",
    "Data is the new oil. Machine learning needs data.\"\"\"\n",
    "\n",
    "print(f\"Text:\\n{text_freq}\")\n",
    "\n",
    "# Tokenize and normalize\n",
    "tokens_freq = text_freq.lower().split()\n",
    "print(f\"\\nTotal tokens: {len(tokens_freq)}\")\n",
    "\n",
    "# Count frequencies\n",
    "token_counts = Counter(tokens_freq)\n",
    "print(f\"Unique tokens: {len(token_counts)}\")\n",
    "\n",
    "print(f\"\\nTop 5 most frequent tokens:\")\n",
    "for token, count in token_counts.most_common(5):\n",
    "    print(f\"  {token:15s}: {count} times\")\n",
    "\n",
    "# 9. PRACTICAL EXAMPLE: LOG FILE PARSING\n",
    "print(\"\\n9. PRACTICAL EXAMPLE: LOG FILE PARSING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "log_entries = [\n",
    "    \"2024-12-11 10:30:45 INFO User login successful\",\n",
    "    \"2024-12-11 10:31:12 ERROR Database connection failed\",\n",
    "    \"2024-12-11 10:31:45 WARNING Disk space low\",\n",
    "    \"2024-12-11 10:32:01 INFO User logout\"\n",
    "]\n",
    "\n",
    "print(\"Log entries:\")\n",
    "for entry in log_entries:\n",
    "    print(f\"  {entry}\")\n",
    "\n",
    "print(f\"\\nParsed logs:\")\n",
    "for entry in log_entries:\n",
    "    tokens = entry.split()\n",
    "    date = tokens[0]\n",
    "    time = tokens[1]\n",
    "    level = tokens[2]\n",
    "    message = ' '.join(tokens[3:])\n",
    "    \n",
    "    print(f\"  Date: {date}, Time: {time}, Level: {level}, Message: {message}\")\n",
    "\n",
    "# Count log levels\n",
    "levels = [entry.split()[2] for entry in log_entries]\n",
    "level_counts = Counter(levels)\n",
    "print(f\"\\nLog level summary:\")\n",
    "for level, count in level_counts.items():\n",
    "    print(f\"  {level}: {count}\")\n",
    "\n",
    "# 10. LIMITATIONS DEMONSTRATION\n",
    "print(\"\\n10. LIMITATIONS OF WHITESPACE TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "examples = [\n",
    "    (\"don't\", \"Contraction not split\"),\n",
    "    (\"user@example.com\", \"Email kept as one token\"),\n",
    "    (\"http://example.com\", \"URL kept as one token\"),\n",
    "    (\"$100.50\", \"Currency symbol attached\"),\n",
    "    (\"Dr. Smith\", \"Abbreviation period attached\"),\n",
    "    (\"New York\", \"Multi-word entity split\")\n",
    "]\n",
    "\n",
    "print(\"Problematic cases:\")\n",
    "for text, issue in examples:\n",
    "    tokens = text.split()\n",
    "    print(f\"  '{text}' ‚Üí {tokens}\")\n",
    "    print(f\"    Issue: {issue}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Whitespace tokenization: text.split()\")\n",
    "print(\"‚úì Fastest tokenization method (built-in Python)\")\n",
    "print(\"‚úì Handles all whitespace types (space, tab, newline)\")\n",
    "print(\"‚úì Automatically removes leading/trailing whitespace\")\n",
    "print(\"‚úì Good for: quick exploration, simple tasks, pre-cleaned text\")\n",
    "print(\"‚úì Limitations: punctuation attached, no special handling\")\n",
    "print(\"‚úì Use .lower() for case-insensitive matching\")\n",
    "print(\"‚úì Combine with punctuation removal for cleaner tokens\")\n",
    "print(\"‚úì Not suitable for: production NLP, complex text, contractions\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
