{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28fedcf0",
   "metadata": {},
   "source": [
    "# 1. Tokenization\n",
    "\n",
    "## üìñ What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking down text into smaller units called **tokens**. These tokens can be words, sentences, subwords, or characters.\n",
    "\n",
    "**Types of Tokenization:**\n",
    "```\n",
    "1. Word Tokenization\n",
    "   Input: \"Hello, how are you?\"\n",
    "   Output: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"]\n",
    "\n",
    "2. Sentence Tokenization\n",
    "   Input: \"Hello! How are you? I'm fine.\"\n",
    "   Output: [\"Hello!\", \"How are you?\", \"I'm fine.\"]\n",
    "\n",
    "3. Subword Tokenization (BPE, WordPiece)\n",
    "   Input: \"unhappiness\"\n",
    "   Output: [\"un\", \"##happiness\"] or [\"un\", \"happy\", \"ness\"]\n",
    "\n",
    "4. Character Tokenization\n",
    "   Input: \"Hello\"\n",
    "   Output: [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Vocabulary**: Set of all unique tokens\n",
    "- **OOV (Out-of-Vocabulary)**: Words not in vocabulary\n",
    "- **Subword Tokenization**: Handles OOV by breaking into smaller parts\n",
    "- **BPE (Byte Pair Encoding)**: Iteratively merges frequent character pairs\n",
    "- **WordPiece**: Used by BERT, similar to BPE\n",
    "- **SentencePiece**: Language-agnostic tokenization\n",
    "\n",
    "## üéØ Why Use Tokenization?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **First Step in NLP** - Required for all text processing\n",
    "2. **Standardization** - Converts text into processable units\n",
    "3. **Feature Extraction** - Tokens become features for ML models\n",
    "4. **Handles Multiple Languages** - Works across languages\n",
    "5. **Vocabulary Control** - Limits model size via subword tokenization\n",
    "\n",
    "### **Challenges:**\n",
    "1. **Language-Specific Rules** - English ‚â† Chinese ‚â† Arabic\n",
    "2. **Ambiguity** - \"New York\" (1 token or 2?)\n",
    "3. **Special Cases** - Contractions (\"don't\"), hyphenated words\n",
    "4. **OOV Problem** - New/rare words not in vocabulary\n",
    "\n",
    "## ‚è±Ô∏è When to Use Different Tokenization Types\n",
    "\n",
    "### ‚úÖ **Word Tokenization - Use When:**\n",
    "\n",
    "**1. Traditional ML Models**\n",
    "- Example: Naive Bayes, Logistic Regression for text classification\n",
    "- Why: Simple features, interpretable\n",
    "- Models expect word-level features\n",
    "\n",
    "**2. Bag-of-Words / TF-IDF**\n",
    "- Example: Document similarity, search engines\n",
    "- Why: Word counts are meaningful\n",
    "- Each word becomes a dimension\n",
    "\n",
    "**3. Small, Controlled Vocabulary**\n",
    "- Example: Domain-specific chatbot (banking terms)\n",
    "- Why: Limited set of words, no rare terms\n",
    "- Vocabulary < 10K words\n",
    "\n",
    "**4. Keyword Extraction**\n",
    "- Example: SEO analysis, document tagging\n",
    "- Why: Whole words are meaningful units\n",
    "- \"Machine learning\" better than \"machine\" + \"learning\"\n",
    "\n",
    "### ‚úÖ **Sentence Tokenization - Use When:**\n",
    "\n",
    "**1. Summarization**\n",
    "- Example: Extract 3 most important sentences\n",
    "- Why: Sentences are atomic units of meaning\n",
    "- Preserve complete thoughts\n",
    "\n",
    "**2. Translation**\n",
    "- Example: Machine translation systems\n",
    "- Why: Translate sentence-by-sentence\n",
    "- Context stays within sentence boundaries\n",
    "\n",
    "**3. Question Answering**\n",
    "- Example: Find sentence containing answer\n",
    "- Why: Answers typically span 1-2 sentences\n",
    "- Granular enough for precise answers\n",
    "\n",
    "### ‚úÖ **Subword Tokenization - Use When:**\n",
    "\n",
    "**1. Transformer Models (BERT, GPT)**\n",
    "- Example: Using pre-trained BERT\n",
    "- Why: These models use subword tokenization\n",
    "- WordPiece/BPE is built-in\n",
    "\n",
    "**2. Handling OOV Words**\n",
    "- Example: Social media text (\"coooool\", \"lololol\")\n",
    "- Why: Break rare words into known subwords\n",
    "- \"unhappiness\" ‚Üí \"un\" + \"happiness\"\n",
    "\n",
    "**3. Morphologically Rich Languages**\n",
    "- Example: German, Finnish, Turkish\n",
    "- Why: Words have many inflections\n",
    "- \"un√ºbersetzbarkeiten\" ‚Üí multiple subwords\n",
    "\n",
    "**4. Large Vocabulary Control**\n",
    "- Example: Multilingual models\n",
    "- Why: Keep vocab size manageable (30K-50K)\n",
    "- Balance between word and character level\n",
    "\n",
    "### ‚ùå **Don't Use Word Tokenization When:**\n",
    "\n",
    "**1. High OOV Rate**\n",
    "- Problem: Medical texts with rare terms\n",
    "- Better: Subword tokenization\n",
    "- Why: Word tokenization fails on unknowns\n",
    "\n",
    "**2. Character-Level Features Matter**\n",
    "- Problem: Spell checking, DNA sequences\n",
    "- Better: Character tokenization\n",
    "- Why: Need to see individual characters\n",
    "\n",
    "**3. Using Pre-trained Transformers**\n",
    "- Problem: Want to use BERT embeddings\n",
    "- Better: Use BERT's tokenizer (WordPiece)\n",
    "- Why: Mismatch causes errors\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Word Tokenization Algorithm:**\n",
    "1. Split on whitespace\n",
    "2. Handle punctuation (keep or remove)\n",
    "3. Apply language-specific rules\n",
    "4. Return list of word tokens\n",
    "\n",
    "**Subword Tokenization (BPE) Algorithm:**\n",
    "1. Start with character vocabulary\n",
    "2. Find most frequent character pair\n",
    "3. Merge pair into new token\n",
    "4. Repeat until vocab size reached\n",
    "5. Example: \"low\" + \"est\" ‚Üí \"lowest\" (learned merge)\n",
    "\n",
    "**Sentence Tokenization:**\n",
    "1. Detect sentence boundaries (. ! ?)\n",
    "2. Handle abbreviations (Dr., Mr., etc.)\n",
    "3. Consider context (\"Ph.D.\" not sentence end)\n",
    "4. Return list of sentences\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Search Engines** - Tokenize queries and documents (Google, Bing)\n",
    "2. **Chatbots** - Tokenize user messages (Siri, Alexa)\n",
    "3. **Translation** - Sentence tokenization (Google Translate)\n",
    "4. **Sentiment Analysis** - Word tokenization (Twitter sentiment)\n",
    "5. **Text Classification** - Spam detection, category classification\n",
    "6. **Information Extraction** - Named entity recognition\n",
    "7. **Question Answering** - Tokenize questions and contexts\n",
    "8. **Text Summarization** - Sentence tokenization\n",
    "9. **Code Analysis** - Tokenize programming languages\n",
    "10. **Speech Recognition** - Tokenize transcribed text\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ **Use pre-built tokenizers** (NLTK, spaCy, transformers)  \n",
    "‚úÖ **Match tokenizer to model** - BERT needs WordPiece  \n",
    "‚úÖ **Consider language** - Chinese/Japanese need special tokenizers  \n",
    "‚úÖ **Handle contractions** - \"don't\" ‚Üí \"do\" + \"n't\" or \"don't\"?  \n",
    "‚úÖ **Preserve important tokens** - \"New York\" as single entity  \n",
    "‚úÖ **Lowercase after tokenization** - Preserve \"Apple\" vs \"apple\"  \n",
    "‚úÖ **Subword for production** - Handles OOV gracefully  \n",
    "‚úÖ **Sentence boundaries matter** - Use robust sentence tokenizer  \n",
    "‚úÖ **Benchmark speed** - Tokenization can be bottleneck  \n",
    "‚úÖ **Version tokenizers** - Changes affect downstream models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOKENIZATION - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is amazing! It's a field of AI that focuses on \n",
    "the interaction between computers and humans. Dr. Smith published a paper on this \n",
    "topic in 2023. He said, \"NLP will revolutionize technology.\" Key techniques include:\n",
    "tokenization, stemming, and lemmatization. Visit www.nlp-tutorial.com for more info.\n",
    "\"\"\"\n",
    "\n",
    "# 1. WORD TOKENIZATION\n",
    "print(\"\\n1. WORD TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Method 1: Simple split (naive)\n",
    "simple_tokens = sample_text.split()\n",
    "print(\"Simple split (naive):\")\n",
    "print(f\"Tokens: {simple_tokens[:10]}\")\n",
    "print(f\"Total tokens: {len(simple_tokens)}\")\n",
    "print(\"Issues: Punctuation attached, doesn't handle special cases\\n\")\n",
    "\n",
    "# Method 2: Regex-based\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', sample_text)\n",
    "print(\"Regex-based (\\\\b\\\\w+\\\\b):\")\n",
    "print(f\"Tokens: {regex_tokens[:10]}\")\n",
    "print(f\"Total tokens: {len(regex_tokens)}\")\n",
    "print(\"Issues: Removes all punctuation, loses contractions\\n\")\n",
    "\n",
    "# Method 3: NLTK word_tokenize (recommended)\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(\"NLTK word_tokenize (recommended):\")\n",
    "print(f\"Tokens: {nltk_tokens[:15]}\")\n",
    "print(f\"Total tokens: {len(nltk_tokens)}\")\n",
    "print(\"Benefits: Handles punctuation, contractions, special cases\\n\")\n",
    "\n",
    "# Method 4: spaCy (if installed)\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        doc = nlp(sample_text)\n",
    "        spacy_tokens = [token.text for token in doc]\n",
    "        print(\"spaCy tokenizer:\")\n",
    "        print(f\"Tokens: {spacy_tokens[:15]}\")\n",
    "        print(f\"Total tokens: {len(spacy_tokens)}\")\n",
    "        print(\"Benefits: Includes POS tags, dependencies, named entities\\n\")\n",
    "    except OSError:\n",
    "        print(\"spaCy model not installed. Run: python -m spacy download en_core_web_sm\\n\")\n",
    "except ImportError:\n",
    "    print(\"spaCy not installed. Run: pip install spacy\\n\")\n",
    "\n",
    "# 2. SENTENCE TOKENIZATION\n",
    "print(\"\\n2. SENTENCE TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(f\"Number of sentences: {len(sentences)}\\n\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence.strip()}\")\n",
    "\n",
    "print(\"\\nHandles complex cases:\")\n",
    "complex_text = \"Dr. Smith earned his Ph.D. in 2020. He works at A.I. Corp. The company is great!\"\n",
    "complex_sentences = sent_tokenize(complex_text)\n",
    "for i, sent in enumerate(complex_sentences, 1):\n",
    "    print(f\"  {i}. {sent}\")\n",
    "\n",
    "# 3. SUBWORD TOKENIZATION (BPE SIMULATION)\n",
    "print(\"\\n3. SUBWORD TOKENIZATION (BPE-STYLE)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simple BPE demonstration\n",
    "def simple_bpe_tokenize(word, max_subword_len=3):\n",
    "    \"\"\"Simple subword tokenization (BPE-style)\"\"\"\n",
    "    if len(word) <= max_subword_len:\n",
    "        return [word]\n",
    "    \n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        # Try to take max_subword_len characters\n",
    "        end = min(i + max_subword_len, len(word))\n",
    "        tokens.append(word[i:end])\n",
    "        i = end\n",
    "    return tokens\n",
    "\n",
    "test_words = ['unhappiness', 'unbelievable', 'preprocessing', 'tokenization']\n",
    "print(\"Subword tokenization examples:\\n\")\n",
    "for word in test_words:\n",
    "    subwords = simple_bpe_tokenize(word, max_subword_len=4)\n",
    "    print(f\"  {word:15s} ‚Üí {subwords}\")\n",
    "\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"  ‚úì Handles OOV words (e.g., 'supercalifragilistic')\")\n",
    "print(\"  ‚úì Smaller vocabulary size\")\n",
    "print(\"  ‚úì Captures morphology (prefixes/suffixes)\")\n",
    "\n",
    "# 4. TRANSFORMER TOKENIZATION (BERT-STYLE)\n",
    "print(\"\\n4. TRANSFORMER TOKENIZATION (BERT/GPT)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    from transformers import BertTokenizer, GPT2Tokenizer\n",
    "    \n",
    "    # BERT tokenizer (WordPiece)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    test_sentence = \"Tokenization is preprocessing text.\"\n",
    "    \n",
    "    bert_tokens = bert_tokenizer.tokenize(test_sentence)\n",
    "    print(\"BERT (WordPiece) tokenization:\")\n",
    "    print(f\"  Input: {test_sentence}\")\n",
    "    print(f\"  Tokens: {bert_tokens}\")\n",
    "    print(f\"  Token IDs: {bert_tokenizer.convert_tokens_to_ids(bert_tokens)}\")\n",
    "    \n",
    "    # GPT-2 tokenizer (BPE)\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(test_sentence)\n",
    "    print(\"\\nGPT-2 (BPE) tokenization:\")\n",
    "    print(f\"  Input: {test_sentence}\")\n",
    "    print(f\"  Tokens: {gpt2_tokens}\")\n",
    "    print(f\"  Token IDs: {gpt2_tokenizer.convert_tokens_to_ids(gpt2_tokens)}\")\n",
    "    \n",
    "    # Handling OOV with subword tokenization\n",
    "    oov_word = \"supercalifragilisticexpialidocious\"\n",
    "    print(f\"\\nHandling OOV word: '{oov_word}'\")\n",
    "    print(f\"  BERT tokens: {bert_tokenizer.tokenize(oov_word)}\")\n",
    "    print(f\"  GPT-2 tokens: {gpt2_tokenizer.tokenize(oov_word)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"transformers library not installed.\")\n",
    "    print(\"Run: pip install transformers\")\n",
    "\n",
    "# 5. CHARACTER TOKENIZATION\n",
    "print(\"\\n5. CHARACTER TOKENIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text = \"Hello!\"\n",
    "char_tokens = list(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Character tokens: {char_tokens}\")\n",
    "print(f\"Total characters: {len(char_tokens)}\")\n",
    "print(\"\\nUse cases:\")\n",
    "print(\"  ‚úì Spell checking\")\n",
    "print(\"  ‚úì Character-level language models\")\n",
    "print(\"  ‚úì DNA/protein sequence analysis\")\n",
    "print(\"  ‚úì Handwriting recognition\")\n",
    "\n",
    "# 6. TOKENIZATION COMPARISON\n",
    "print(\"\\n6. TOKENIZATION METHOD COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "comparison_text = \"She's reading Dr. Johnson's book about AI.\"\n",
    "\n",
    "print(f\"Input: \\\"{comparison_text}\\\"\\n\")\n",
    "\n",
    "methods = {\n",
    "    'Simple split': comparison_text.split(),\n",
    "    'Regex \\\\w+': re.findall(r'\\w+', comparison_text),\n",
    "    'NLTK word_tokenize': word_tokenize(comparison_text),\n",
    "    'Character-level': list(comparison_text.replace(' ', ''))\n",
    "}\n",
    "\n",
    "for method, tokens in methods.items():\n",
    "    print(f\"{method:20s}: {tokens}\")\n",
    "\n",
    "# 7. VOCABULARY BUILDING\n",
    "print(\"\\n7. VOCABULARY BUILDING FROM TOKENS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"I love learning about NLP\"\n",
    "]\n",
    "\n",
    "# Tokenize all documents\n",
    "all_tokens = []\n",
    "for doc in corpus:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set(all_tokens))\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "print(f\"Corpus: {len(corpus)} documents\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nVocabulary: {vocab}\")\n",
    "print(f\"\\nTop 5 most frequent tokens:\")\n",
    "for token, freq in token_freq.most_common(5):\n",
    "    print(f\"  '{token}': {freq} times\")\n",
    "\n",
    "# 8. PRACTICAL APPLICATION: PREPROCESSING PIPELINE\n",
    "print(\"\\n8. PRACTICAL APPLICATION: TEXT PREPROCESSING PIPELINE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def preprocess_text(text, lowercase=True, remove_punct=False):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    if lowercase:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punct:\n",
    "        tokens = [t for t in tokens if t.isalnum()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "demo_text = \"Hello! This is a TEST sentence. It's quite simple.\"\n",
    "\n",
    "print(f\"Original text: {demo_text}\\n\")\n",
    "print(f\"Default: {preprocess_text(demo_text)}\")\n",
    "print(f\"No lowercase: {preprocess_text(demo_text, lowercase=False)}\")\n",
    "print(f\"Remove punct: {preprocess_text(demo_text, remove_punct=True)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Tokenization breaks text into processable units\")\n",
    "print(\"‚úì Word tokenization: Traditional ML (Naive Bayes, TF-IDF)\")\n",
    "print(\"‚úì Sentence tokenization: Summarization, translation\")\n",
    "print(\"‚úì Subword tokenization: Transformers (BERT, GPT), handles OOV\")\n",
    "print(\"‚úì Character tokenization: Spell checking, char-level models\")\n",
    "print(\"‚úì Use NLTK/spaCy for robust tokenization\")\n",
    "print(\"‚úì Match tokenizer to downstream model\")\n",
    "print(\"‚úì BERT uses WordPiece, GPT uses BPE\")\n",
    "print(\"‚úì Build vocabulary from tokenized corpus\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63949385",
   "metadata": {},
   "source": [
    "# 2. Lowercasing & Case Normalization\n",
    "\n",
    "## üìñ What is Lowercasing?\n",
    "\n",
    "**Lowercasing** is the process of converting all text characters to lowercase to reduce vocabulary size and treat words like \"Apple\" and \"apple\" as the same token.\n",
    "\n",
    "**Types of Case Normalization:**\n",
    "```\n",
    "1. Full Lowercasing\n",
    "   Input: \"Apple Inc. Makes iPhones\"\n",
    "   Output: \"apple inc. makes iphones\"\n",
    "\n",
    "2. Selective Lowercasing (Preserve Acronyms)\n",
    "   Input: \"NASA launched the Artemis mission\"\n",
    "   Output: \"NASA launched the artemis mission\"\n",
    "\n",
    "3. Title Case\n",
    "   Input: \"natural language processing\"\n",
    "   Output: \"Natural Language Processing\"\n",
    "\n",
    "4. Sentence Case\n",
    "   Input: \"HELLO WORLD. HOW ARE YOU?\"\n",
    "   Output: \"Hello world. How are you?\"\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Case-Insensitive Matching**: \"Apple\" = \"apple\" = \"APPLE\"\n",
    "- **Vocabulary Reduction**: \"The\" and \"the\" become single token\n",
    "- **Information Loss**: \"Apple\" (company) vs \"apple\" (fruit)\n",
    "- **Language-Specific**: Turkish ƒ∞/i, German √ü\n",
    "\n",
    "## üéØ Why Use Lowercasing?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Smaller Vocabulary** - Reduces unique tokens by 30-50%\n",
    "2. **Better Generalization** - Model sees \"car\" and \"Car\" as same\n",
    "3. **Simpler Matching** - Case-insensitive search\n",
    "4. **Consistent Features** - No duplicate features for case variants\n",
    "5. **Faster Training** - Smaller vocab = faster model\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Loses Information** - \"Apple Inc.\" vs \"apple fruit\"\n",
    "2. **Acronym Confusion** - \"US\" (country) vs \"us\" (pronoun)\n",
    "3. **Named Entity Issues** - \"Paris\" (city) vs \"paris\" (word)\n",
    "4. **Sentiment Loss** - \"AMAZING!!!\" vs \"amazing\" (intensity lost)\n",
    "\n",
    "## ‚è±Ô∏è When to Use Lowercasing\n",
    "\n",
    "### ‚úÖ **Use Lowercasing When:**\n",
    "\n",
    "**1. Text Classification (Sentiment, Spam)**\n",
    "- Example: Email spam detection\n",
    "- Why: Case doesn't affect spam/not-spam\n",
    "- \"FREE MONEY\" and \"free money\" both spam\n",
    "\n",
    "**2. Search Engines**\n",
    "- Example: Google search\n",
    "- Why: Users type queries in any case\n",
    "- \"Python tutorial\" = \"python tutorial\" = \"PYTHON TUTORIAL\"\n",
    "\n",
    "**3. Bag-of-Words / TF-IDF**\n",
    "- Example: Document similarity\n",
    "- Why: \"Machine\" and \"machine\" should count together\n",
    "- Reduces vocabulary size\n",
    "\n",
    "**4. Small Datasets**\n",
    "- Example: 500 training samples\n",
    "- Why: Not enough data to learn case patterns\n",
    "- \"The\" appears 50 times, \"the\" appears 200 times ‚Üí merge\n",
    "\n",
    "**5. Language Models (Informal Text)**\n",
    "- Example: Twitter sentiment analysis\n",
    "- Why: Social media text has inconsistent casing\n",
    "- \"lol\", \"LOL\", \"Lol\" all mean same thing\n",
    "\n",
    "**6. Keyword Matching**\n",
    "- Example: Filter support tickets by keywords\n",
    "- Why: Keywords may appear in any case\n",
    "- Match \"refund\", \"Refund\", \"REFUND\"\n",
    "\n",
    "### ‚ùå **Don't Use Lowercasing When:**\n",
    "\n",
    "**1. Named Entity Recognition (NER)**\n",
    "- Problem: Detect people, places, organizations\n",
    "- Better: Keep original case\n",
    "- Why: \"Apple\" (company) vs \"apple\" (fruit)\n",
    "- Example: \"Washington\" (person/city) vs \"washington\" (word)\n",
    "\n",
    "**2. Part-of-Speech Tagging**\n",
    "- Problem: Tag word types (noun, verb, etc.)\n",
    "- Better: Preserve case\n",
    "- Why: \"Polish\" (adjective) vs \"polish\" (verb)\n",
    "- Example: \"Turkey\" (noun) vs \"turkey\" (noun, different meaning)\n",
    "\n",
    "**3. Machine Translation**\n",
    "- Problem: Translate German to English\n",
    "- Better: Keep case\n",
    "- Why: German nouns always capitalized\n",
    "- Example: \"Die Katze\" (the cat) - \"Die\" is article, not verb\n",
    "\n",
    "**4. Case Carries Meaning**\n",
    "- Problem: Acronyms, proper nouns\n",
    "- Better: Selective lowercasing\n",
    "- Why: \"US\" ‚â† \"us\", \"IT\" (info tech) ‚â† \"it\"\n",
    "\n",
    "**5. Sentiment Analysis (Intensity)**\n",
    "- Problem: Detect emotion strength\n",
    "- Better: Preserve case as feature\n",
    "- Why: \"LOVE IT!!!\" stronger than \"love it\"\n",
    "\n",
    "**6. Question Answering**\n",
    "- Problem: Answer \"Who is the president?\"\n",
    "- Better: Keep case\n",
    "- Why: Answer is proper noun (\"Joe Biden\")\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Simple Lowercasing:**\n",
    "```python\n",
    "text.lower()  # Python built-in\n",
    "```\n",
    "\n",
    "**Selective Lowercasing (Preserve Acronyms):**\n",
    "```python\n",
    "def selective_lowercase(text):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(\n",
    "        token if token.isupper() and len(token) > 1  # Keep acronyms\n",
    "        else token.lower()\n",
    "        for token in tokens\n",
    "    )\n",
    "```\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Search Engines** - Case-insensitive search (Google, Bing)\n",
    "2. **Email Filters** - Spam detection\n",
    "3. **Chatbots** - User intent classification\n",
    "4. **Sentiment Analysis** - Product reviews (when case doesn't matter)\n",
    "5. **Text Classification** - Topic categorization\n",
    "6. **Information Retrieval** - Document search\n",
    "7. **Autocomplete** - Suggest terms regardless of case\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ **Lowercase AFTER tokenization** - Preserve \"U.S.\" before lowercasing  \n",
    "‚úÖ **Consider task requirements** - NER needs case, spam detection doesn't  \n",
    "‚úÖ **Test both ways** - A/B test with and without lowercasing  \n",
    "‚úÖ **Preserve acronyms** - \"USA\", \"NASA\", \"FBI\" if important  \n",
    "‚úÖ **Language-specific** - Handle German √ü, Turkish ƒ∞  \n",
    "‚úÖ **Check pre-trained models** - BERT has cased/uncased versions  \n",
    "‚úÖ **Document decision** - Note if model is case-sensitive  \n",
    "‚úÖ **Vocabulary size impact** - Measure before/after  \n",
    "‚úÖ **Sentiment intensity** - Keep ALL CAPS if analyzing emotion  \n",
    "‚úÖ **Combine with other preprocessing** - Stop words, stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fe185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOWERCASING & CASE NORMALIZATION - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOWERCASING & CASE NORMALIZATION - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample texts demonstrating case importance\n",
    "sample_texts = [\n",
    "    \"Apple Inc. makes iPhones in California.\",\n",
    "    \"I love eating apples from the farmers market.\",\n",
    "    \"NASA launched the Artemis mission.\",\n",
    "    \"The US president visited Paris, France.\",\n",
    "    \"AMAZING product!!! HIGHLY RECOMMENDED!!!\",\n",
    "    \"The IT department uses IT infrastructure.\"\n",
    "]\n",
    "\n",
    "# 1. BASIC LOWERCASING\n",
    "print(\"\\n1. BASIC LOWERCASING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for text in sample_texts[:3]:\n",
    "    lowercased = text.lower()\n",
    "    print(f\"Original:   {text}\")\n",
    "    print(f\"Lowercased: {lowercased}\")\n",
    "    print()\n",
    "\n",
    "# 2. VOCABULARY SIZE COMPARISON\n",
    "print(\"\\n2. VOCABULARY SIZE IMPACT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "corpus = \" \".join(sample_texts)\n",
    "\n",
    "# Original vocabulary\n",
    "original_tokens = corpus.split()\n",
    "original_vocab = set(original_tokens)\n",
    "\n",
    "# Lowercased vocabulary\n",
    "lowercased_tokens = [t.lower() for t in original_tokens]\n",
    "lowercased_vocab = set(lowercased_tokens)\n",
    "\n",
    "print(f\"Original vocabulary size: {len(original_vocab)}\")\n",
    "print(f\"Lowercased vocabulary size: {len(lowercased_vocab)}\")\n",
    "print(f\"Reduction: {len(original_vocab) - len(lowercased_vocab)} tokens\")\n",
    "print(f\"Percentage: {(1 - len(lowercased_vocab)/len(original_vocab))*100:.1f}% smaller\")\n",
    "\n",
    "print(f\"\\nOriginal vocab (first 10): {sorted(list(original_vocab))[:10]}\")\n",
    "print(f\"Lowercased vocab (first 10): {sorted(list(lowercased_vocab))[:10]}\")\n",
    "\n",
    "# 3. INFORMATION LOSS DEMONSTRATION\n",
    "print(\"\\n3. INFORMATION LOSS EXAMPLES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "ambiguous_cases = [\n",
    "    (\"Apple Inc.\", \"apple inc.\", \"Company vs fruit\"),\n",
    "    (\"US president\", \"us president\", \"Country vs pronoun\"),\n",
    "    (\"Paris, France\", \"paris, france\", \"City vs common word\"),\n",
    "    (\"IT department\", \"it department\", \"Information Tech vs pronoun\"),\n",
    "    (\"Polish the car\", \"polish the car\", \"Nationality vs verb\"),\n",
    "    (\"READ THIS!!!\", \"read this!!!\", \"Emphasis lost\")\n",
    "]\n",
    "\n",
    "print(\"Case matters for meaning:\\n\")\n",
    "for original, lowercased, explanation in ambiguous_cases:\n",
    "    print(f\"  Original:   '{original}'\")\n",
    "    print(f\"  Lowercased: '{lowercased}'\")\n",
    "    print(f\"  Issue: {explanation}\\n\")\n",
    "\n",
    "# 4. SELECTIVE LOWERCASING (PRESERVE ACRONYMS)\n",
    "print(\"\\n4. SELECTIVE LOWERCASING (PRESERVE ACRONYMS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def selective_lowercase(text):\n",
    "    \"\"\"\n",
    "    Lowercase text but preserve acronyms (all caps, 2+ letters)\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    result = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Remove punctuation for checking\n",
    "        word = re.sub(r'[^A-Za-z]', '', token)\n",
    "        \n",
    "        # Keep if acronym (all uppercase, 2+ letters)\n",
    "        if word.isupper() and len(word) >= 2:\n",
    "            result.append(token)\n",
    "        else:\n",
    "            result.append(token.lower())\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "test_texts = [\n",
    "    \"NASA launched the Artemis mission to the Moon.\",\n",
    "    \"The FBI and CIA work for the US government.\",\n",
    "    \"I work in IT at IBM and use SQL daily.\",\n",
    "    \"The CEO of Apple Inc. announced new iPhones.\"\n",
    "]\n",
    "\n",
    "print(\"Preserving acronyms:\\n\")\n",
    "for text in test_texts:\n",
    "    print(f\"Original:  {text}\")\n",
    "    print(f\"Selective: {selective_lowercase(text)}\")\n",
    "    print(f\"Full low:  {text.lower()}\")\n",
    "    print()\n",
    "\n",
    "# 5. CASE-SENSITIVE VS CASE-INSENSITIVE COMPARISON\n",
    "print(\"\\n5. CASE-SENSITIVE VS CASE-INSENSITIVE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "documents = [\n",
    "    \"Apple makes great products. I love Apple.\",\n",
    "    \"I ate an apple today. The apple was delicious.\",\n",
    "    \"Apple Inc. is a technology company.\"\n",
    "]\n",
    "\n",
    "# Case-sensitive word count\n",
    "case_sensitive_counts = Counter()\n",
    "for doc in documents:\n",
    "    words = doc.split()\n",
    "    case_sensitive_counts.update(words)\n",
    "\n",
    "# Case-insensitive word count\n",
    "case_insensitive_counts = Counter()\n",
    "for doc in documents:\n",
    "    words = [w.lower() for w in doc.split()]\n",
    "    case_insensitive_counts.update(words)\n",
    "\n",
    "print(\"Case-sensitive counts:\")\n",
    "for word in ['Apple', 'apple']:\n",
    "    print(f\"  '{word}': {case_sensitive_counts.get(word, 0)}\")\n",
    "\n",
    "print(\"\\nCase-insensitive counts:\")\n",
    "print(f\"  'apple': {case_insensitive_counts['apple']}\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  Case-sensitive: Distinguishes 'Apple' (company) from 'apple' (fruit)\")\n",
    "print(\"  Case-insensitive: Treats both as same word (higher count)\")\n",
    "\n",
    "# 6. IMPACT ON MACHINE LEARNING\n",
    "print(\"\\n6. IMPACT ON MACHINE LEARNING FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_docs = [\n",
    "    \"The Quick Brown Fox\",\n",
    "    \"the quick brown fox\",\n",
    "    \"THE QUICK BROWN FOX\"\n",
    "]\n",
    "\n",
    "# Case-sensitive vectorizer\n",
    "vec_case_sensitive = CountVectorizer(lowercase=False)\n",
    "X_case_sensitive = vec_case_sensitive.fit_transform(sample_docs)\n",
    "\n",
    "# Case-insensitive vectorizer\n",
    "vec_case_insensitive = CountVectorizer(lowercase=True)\n",
    "X_case_insensitive = vec_case_insensitive.fit_transform(sample_docs)\n",
    "\n",
    "print(\"Case-sensitive features:\")\n",
    "print(f\"  Vocabulary size: {len(vec_case_sensitive.vocabulary_)}\")\n",
    "print(f\"  Features: {sorted(vec_case_sensitive.vocabulary_.keys())}\")\n",
    "\n",
    "print(\"\\nCase-insensitive features:\")\n",
    "print(f\"  Vocabulary size: {len(vec_case_insensitive.vocabulary_)}\")\n",
    "print(f\"  Features: {sorted(vec_case_insensitive.vocabulary_.keys())}\")\n",
    "\n",
    "print(\"\\nImpact:\")\n",
    "print(f\"  Case-sensitive: 12 features (The, the, THE, Quick, quick, QUICK, ...)\")\n",
    "print(f\"  Case-insensitive: 4 features (the, quick, brown, fox)\")\n",
    "print(f\"  ‚úì 3x smaller feature space!\")\n",
    "\n",
    "# 7. BEST PRACTICES\n",
    "print(\"\\n7. BEST PRACTICES & RECOMMENDATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "recommendations = {\n",
    "    \"Text Classification (Spam, Sentiment)\": \"‚úì Use lowercasing\",\n",
    "    \"Named Entity Recognition (NER)\": \"‚úó Keep original case\",\n",
    "    \"Part-of-Speech Tagging\": \"‚úó Keep original case\",\n",
    "    \"Machine Translation\": \"‚úó Keep original case\",\n",
    "    \"Search Engines\": \"‚úì Use lowercasing\",\n",
    "    \"Bag-of-Words / TF-IDF\": \"‚úì Use lowercasing\",\n",
    "    \"Question Answering\": \"‚ö†Ô∏è Selective (preserve proper nouns)\",\n",
    "    \"Sentiment with Intensity\": \"‚ö†Ô∏è Consider keeping ALL CAPS\",\n",
    "    \"Chatbots (Intent Classification)\": \"‚úì Use lowercasing\",\n",
    "    \"Code Analysis\": \"‚úó Keep original case (camelCase matters)\"\n",
    "}\n",
    "\n",
    "print(\"Task-specific recommendations:\\n\")\n",
    "for task, recommendation in recommendations.items():\n",
    "    print(f\"  {task:40s} ‚Üí {recommendation}\")\n",
    "\n",
    "# 8. PRACTICAL FUNCTION\n",
    "print(\"\\n8. PRACTICAL PREPROCESSING FUNCTION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def normalize_case(text, strategy='lowercase'):\n",
    "    \"\"\"\n",
    "    Normalize text case based on strategy\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        strategy: 'lowercase', 'uppercase', 'titlecase', 'preserve_acronyms'\n",
    "    \"\"\"\n",
    "    if strategy == 'lowercase':\n",
    "        return text.lower()\n",
    "    \n",
    "    elif strategy == 'uppercase':\n",
    "        return text.upper()\n",
    "    \n",
    "    elif strategy == 'titlecase':\n",
    "        return text.title()\n",
    "    \n",
    "    elif strategy == 'preserve_acronyms':\n",
    "        return selective_lowercase(text)\n",
    "    \n",
    "    else:\n",
    "        return text  # No change\n",
    "\n",
    "demo_text = \"NASA and the FBI work with US agencies.\"\n",
    "\n",
    "print(f\"Original: {demo_text}\\n\")\n",
    "for strategy in ['lowercase', 'uppercase', 'titlecase', 'preserve_acronyms']:\n",
    "    result = normalize_case(demo_text, strategy)\n",
    "    print(f\"{strategy:20s}: {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Lowercasing reduces vocabulary by 30-50%\")\n",
    "print(\"‚úì Use for: Text classification, search, bag-of-words\")\n",
    "print(\"‚úó Avoid for: NER, POS tagging, machine translation\")\n",
    "print(\"‚úì Lowercase AFTER tokenization to preserve 'U.S.'\")\n",
    "print(\"‚úì Consider preserving acronyms (NASA, FBI, IT)\")\n",
    "print(\"‚úì Test with and without lowercasing\")\n",
    "print(\"‚úì Check pre-trained models (BERT cased vs uncased)\")\n",
    "print(\"‚úì Document your case normalization strategy\")\n",
    "print(\"‚úì Information loss: 'Apple Inc.' vs 'apple fruit'\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
