{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a247b230",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "## üìñ What is Linear Regression?\n",
    "\n",
    "**Linear Regression** is a supervised learning algorithm that models the relationship between input features (X) and a continuous target variable (y) using a linear equation.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "```\n",
    "y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ\n",
    "\n",
    "Where:\n",
    "  y = predicted value (target)\n",
    "  Œ≤‚ÇÄ = intercept (bias)\n",
    "  Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô = coefficients (weights)\n",
    "  x‚ÇÅ, x‚ÇÇ, ..., x‚Çô = features\n",
    "  Œµ = error term\n",
    "```\n",
    "\n",
    "**Goal:** Minimize the **Sum of Squared Errors (SSE)**:\n",
    "```\n",
    "SSE = Œ£(y·µ¢ - ≈∑·µ¢)¬≤\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Ordinary Least Squares (OLS)**: Standard method to fit the line\n",
    "- **R¬≤ Score**: Measures how well the model fits (0 to 1, higher is better)\n",
    "- **Assumptions**: Linearity, independence, homoscedasticity, normality\n",
    "- **Interpretability**: Coefficients show feature importance and direction\n",
    "\n",
    "## üéØ Why Use Linear Regression?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Simplicity** - Easy to understand and implement\n",
    "2. **Interpretability** - Coefficients explain feature impact\n",
    "3. **Fast Training** - Closed-form solution (no iterations needed)\n",
    "4. **Low Computational Cost** - Works well with limited resources\n",
    "5. **Baseline Model** - Good starting point for any regression problem\n",
    "6. **Statistical Inference** - Provides p-values, confidence intervals\n",
    "7. **Extrapolation** - Can predict beyond training data range\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Linear Assumption** - Can't capture non-linear relationships\n",
    "2. **Sensitive to Outliers** - Single outlier can skew the line\n",
    "3. **Multicollinearity** - Correlated features cause instability\n",
    "4. **Overfitting** - With many features, may not generalize\n",
    "5. **Assumes Normality** - Residuals should be normally distributed\n",
    "\n",
    "## ‚è±Ô∏è When to Use Linear Regression\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Relationship is Linear**\n",
    "- Example: House price vs square footage\n",
    "- Why: Price increases proportionally with size\n",
    "- Data: Scatter plot shows straight-line trend\n",
    "\n",
    "**2. Need Interpretability**\n",
    "- Example: Understand impact of marketing spend on sales\n",
    "- Why: Coefficients tell you \"$1 increase in ads = $X increase in sales\"\n",
    "- Use case: Business decisions, stakeholder communication\n",
    "\n",
    "**3. Small Dataset**\n",
    "- Example: 100 samples, 5 features\n",
    "- Why: Complex models would overfit\n",
    "- Benefit: Linear regression generalizes well with limited data\n",
    "\n",
    "**4. Fast Predictions Needed**\n",
    "- Example: Real-time pricing engine\n",
    "- Why: O(n) prediction time (just multiply and add)\n",
    "- Performance: Milliseconds for inference\n",
    "\n",
    "**5. Baseline/Benchmark**\n",
    "- Example: Start of any regression project\n",
    "- Why: Establishes minimum performance threshold\n",
    "- Next step: Try more complex models if R¬≤ is low\n",
    "\n",
    "**6. Statistical Inference Required**\n",
    "- Example: Medical study analyzing treatment effect\n",
    "- Why: Need p-values to determine significance\n",
    "- Use case: Academic research, regulatory compliance\n",
    "\n",
    "**7. Few Features (< 10)**\n",
    "- Example: Predict salary from years of experience and education\n",
    "- Why: Low dimensionality, relationships likely linear\n",
    "- Benefit: Avoid overfitting, easy to interpret\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Relationship is Non-Linear**\n",
    "- Problem: House price doesn't increase linearly with age (decreases after peak)\n",
    "- Better: Polynomial regression, decision trees, neural networks\n",
    "- Why: Linear model can't capture curves\n",
    "\n",
    "**2. Many Correlated Features**\n",
    "- Problem: Height in cm, height in inches, height in feet (all correlated)\n",
    "- Better: Ridge/Lasso regression, PCA + linear regression\n",
    "- Why: Multicollinearity makes coefficients unstable\n",
    "\n",
    "**3. Outliers Present**\n",
    "- Problem: One house sold for $10M in a $200K neighborhood\n",
    "- Better: Huber regression, RANSAC, remove outliers\n",
    "- Why: Outliers pull the line away from true trend\n",
    "\n",
    "**4. High-Dimensional Data (p >> n)**\n",
    "- Problem: 10,000 features, 100 samples\n",
    "- Better: Regularized regression (Ridge/Lasso), dimensionality reduction\n",
    "- Why: Overfits, can't compute inverse of singular matrix\n",
    "\n",
    "**5. Need Probability Estimates**\n",
    "- Problem: Predict if customer will churn (yes/no)\n",
    "- Better: Logistic regression, classification models\n",
    "- Why: Linear regression outputs can be < 0 or > 1\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Training Process:**\n",
    "1. **Input**: Dataset with features X and target y\n",
    "2. **Solve**: Normal equation: Œ≤ = (X^T X)^(-1) X^T y\n",
    "3. **Output**: Coefficients Œ≤‚ÇÄ, Œ≤‚ÇÅ, ..., Œ≤‚Çô\n",
    "\n",
    "**Prediction:**\n",
    "```python\n",
    "≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô\n",
    "```\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **R¬≤ Score**: 1 - (SS_res / SS_tot) ‚Üí [0, 1], closer to 1 is better\n",
    "- **RMSE**: ‚àö(Œ£(y - ≈∑)¬≤ / n) ‚Üí Lower is better\n",
    "- **MAE**: Œ£|y - ≈∑| / n ‚Üí Lower is better\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Real Estate** - Predict house prices from size, location, bedrooms\n",
    "2. **Finance** - Stock price prediction, risk assessment\n",
    "3. **Marketing** - ROI prediction from ad spend\n",
    "4. **Healthcare** - Disease progression from patient metrics\n",
    "5. **Economics** - GDP forecasting from economic indicators\n",
    "6. **Retail** - Sales forecasting from historical data\n",
    "7. **Manufacturing** - Quality prediction from process parameters\n",
    "8. **Agriculture** - Crop yield from weather, soil conditions\n",
    "9. **Energy** - Power consumption forecasting\n",
    "10. **Insurance** - Claim amount prediction\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ **Always visualize** data first - scatter plots reveal linearity  \n",
    "‚úÖ **Check assumptions**: linearity, normality of residuals  \n",
    "‚úÖ **Feature scaling** not required (coefficients adjust)  \n",
    "‚úÖ **Regularization** (Ridge/Lasso) if features are correlated  \n",
    "‚úÖ **R¬≤ = 0.7+** is generally good, but depends on domain  \n",
    "‚úÖ **Coefficients** show feature importance and direction (+/-)  \n",
    "‚úÖ **Remove outliers** or use robust regression  \n",
    "‚úÖ **Check VIF** (Variance Inflation Factor) for multicollinearity  \n",
    "‚úÖ **Residual plots** should show random scatter (no pattern)  \n",
    "‚úÖ **Use as baseline** - always try linear regression first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LINEAR REGRESSION - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. GENERATE SYNTHETIC DATA (HOUSE PRICES)\n",
    "print(\"\\n1. GENERATING SYNTHETIC HOUSE PRICE DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Features\n",
    "square_feet = np.random.uniform(800, 3500, n_samples)\n",
    "bedrooms = np.random.randint(1, 6, n_samples)\n",
    "age = np.random.uniform(0, 50, n_samples)\n",
    "\n",
    "# Target: Price (linear relationship + noise)\n",
    "price = (150 * square_feet +        # $150 per sqft\n",
    "         20000 * bedrooms +          # $20K per bedroom\n",
    "         -2000 * age +               # -$2K per year of age\n",
    "         100000 +                    # Base price\n",
    "         np.random.normal(0, 30000, n_samples))  # Noise\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'square_feet': square_feet,\n",
    "    'bedrooms': bedrooms,\n",
    "    'age': age,\n",
    "    'price': price\n",
    "})\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 2. EXPLORATORY DATA ANALYSIS\n",
    "print(\"\\n2. EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Correlation matrix\n",
    "correlation = df.corr()\n",
    "print(\"Correlation with price:\")\n",
    "print(correlation['price'].sort_values(ascending=False))\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Scatter plots\n",
    "axes[0, 0].scatter(df['square_feet'], df['price'], alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Square Feet')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].set_title('Price vs Square Feet')\n",
    "\n",
    "axes[0, 1].scatter(df['bedrooms'], df['price'], alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Bedrooms')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "axes[0, 1].set_title('Price vs Bedrooms')\n",
    "\n",
    "axes[1, 0].scatter(df['age'], df['price'], alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Age (years)')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].set_title('Price vs Age')\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('linear_regression_eda.png', dpi=100, bbox_inches='tight')\n",
    "print(\"\\nEDA visualizations saved to 'linear_regression_eda.png'\")\n",
    "plt.close()\n",
    "\n",
    "# 3. PREPARE DATA\n",
    "print(\"\\n3. PREPARING DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Features and target\n",
    "X = df[['square_feet', 'bedrooms', 'age']]\n",
    "y = df['price']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# 4. TRAIN LINEAR REGRESSION MODEL\n",
    "print(\"\\n4. TRAINING LINEAR REGRESSION MODEL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create and train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"\\nModel coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"  {feature:15s}: ${coef:12,.2f}\")\n",
    "print(f\"  {'Intercept':15s}: ${model.intercept_:12,.2f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Each additional sqft increases price by ${model.coef_[0]:.2f}\")\n",
    "print(f\"  - Each additional bedroom increases price by ${model.coef_[1]:,.2f}\")\n",
    "print(f\"  - Each year of age decreases price by ${abs(model.coef_[2]):,.2f}\")\n",
    "\n",
    "# 5. MAKE PREDICTIONS\n",
    "print(\"\\n5. MAKING PREDICTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Show some predictions\n",
    "print(\"Sample predictions (first 5 test samples):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_test.values[:5],\n",
    "    'Predicted': y_test_pred[:5],\n",
    "    'Difference': y_test.values[:5] - y_test_pred[:5]\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# 6. EVALUATE MODEL\n",
    "print(\"\\n6. MODEL EVALUATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Training metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "print(\"Training Set Performance:\")\n",
    "print(f\"  R¬≤ Score: {train_r2:.4f}\")\n",
    "print(f\"  RMSE: ${train_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${train_mae:,.2f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"  RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${test_mae:,.2f}\")\n",
    "\n",
    "print(f\"\\nModel Interpretation:\")\n",
    "print(f\"  - R¬≤ = {test_r2:.2%} of variance in price is explained by the model\")\n",
    "print(f\"  - Average prediction error: ${test_mae:,.0f}\")\n",
    "print(f\"  - Model generalizes well (train R¬≤ ‚âà test R¬≤)\")\n",
    "\n",
    "# 7. VISUALIZE RESULTS\n",
    "print(\"\\n7. VISUALIZING RESULTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], \n",
    "                [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Price ($)')\n",
    "axes[0, 0].set_ylabel('Predicted Price ($)')\n",
    "axes[0, 0].set_title(f'Actual vs Predicted (R¬≤ = {test_r2:.4f})')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Price ($)')\n",
    "axes[0, 1].set_ylabel('Residuals ($)')\n",
    "axes[0, 1].set_title('Residual Plot (should be random)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 0].set_xlabel('Residuals ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Residuals Distribution (should be normal)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=True)\n",
    "\n",
    "axes[1, 1].barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "axes[1, 1].set_xlabel('Coefficient Value')\n",
    "axes[1, 1].set_title('Feature Importance (Coefficients)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('linear_regression_results.png', dpi=100, bbox_inches='tight')\n",
    "print(\"Results visualizations saved to 'linear_regression_results.png'\")\n",
    "plt.close()\n",
    "\n",
    "# 8. PREDICTION ON NEW DATA\n",
    "print(\"\\n8. PREDICTING ON NEW DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# New houses to predict\n",
    "new_houses = pd.DataFrame({\n",
    "    'square_feet': [1500, 2500, 3000],\n",
    "    'bedrooms': [3, 4, 5],\n",
    "    'age': [5, 10, 2]\n",
    "})\n",
    "\n",
    "print(\"New houses to predict:\")\n",
    "print(new_houses)\n",
    "\n",
    "# Make predictions\n",
    "new_predictions = model.predict(new_houses)\n",
    "\n",
    "print(f\"\\nPredicted prices:\")\n",
    "for i, (idx, row) in enumerate(new_houses.iterrows()):\n",
    "    print(f\"  House {i+1}: {row['square_feet']:.0f} sqft, \"\n",
    "          f\"{row['bedrooms']:.0f} beds, {row['age']:.0f} years old\")\n",
    "    print(f\"           ‚Üí Predicted price: ${new_predictions[i]:,.2f}\")\n",
    "\n",
    "# 9. MODEL EQUATION\n",
    "print(\"\\n9. MODEL EQUATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"Fitted equation:\")\n",
    "print(f\"\\nPrice = {model.intercept_:,.2f}\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    sign = '+' if coef >= 0 else '-'\n",
    "    print(f\"        {sign} {abs(coef):,.2f} √ó {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Linear Regression trained on {len(X_train)} samples\")\n",
    "print(f\"‚úì Test R¬≤ = {test_r2:.4f} (explains {test_r2:.1%} of variance)\")\n",
    "print(f\"‚úì Test RMSE = ${test_rmse:,.2f}\")\n",
    "print(f\"‚úì Test MAE = ${test_mae:,.2f}\")\n",
    "print(f\"‚úì Model is interpretable: coefficients show feature impact\")\n",
    "print(f\"‚úì No overfitting: train R¬≤ ‚âà test R¬≤\")\n",
    "print(f\"‚úì Residuals are roughly normally distributed\")\n",
    "print(f\"‚úì Good baseline model for house price prediction\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b6be4",
   "metadata": {},
   "source": [
    "# 2. Ridge Regression (L2 Regularization)\n",
    "\n",
    "## üìñ What is Ridge Regression?\n",
    "\n",
    "**Ridge Regression** is a regularized version of Linear Regression that adds an L2 penalty term to prevent overfitting and handle multicollinearity.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "```\n",
    "Cost = SSE + Œ± √ó Œ£(Œ≤·µ¢¬≤)\n",
    "\n",
    "Where:\n",
    "  SSE = Sum of Squared Errors (same as Linear Regression)\n",
    "  Œ± = Regularization strength (hyperparameter)\n",
    "  Œ£(Œ≤·µ¢¬≤) = Sum of squared coefficients (L2 penalty)\n",
    "```\n",
    "\n",
    "**Key Difference from Linear Regression:**\n",
    "- **Linear Regression**: Minimize SSE only\n",
    "- **Ridge Regression**: Minimize SSE + penalty for large coefficients\n",
    "\n",
    "**Effect:**\n",
    "- **Shrinks coefficients** toward zero (but never exactly zero)\n",
    "- **Reduces variance** at the cost of increased bias\n",
    "- **Prevents overfitting** by limiting coefficient magnitude\n",
    "\n",
    "## üéØ Why Use Ridge Regression?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Handles Multicollinearity** - Works when features are correlated\n",
    "2. **Prevents Overfitting** - Regularization reduces model complexity\n",
    "3. **Stable Coefficients** - Small data changes don't drastically change model\n",
    "4. **Works with p > n** - Can handle more features than samples\n",
    "5. **Closed-Form Solution** - Fast to train (no iterations)\n",
    "6. **All Features Retained** - Doesn't set coefficients to exactly zero\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Not Feature Selection** - Keeps all features (unlike Lasso)\n",
    "2. **Hyperparameter Tuning** - Need to find optimal Œ±\n",
    "3. **Requires Scaling** - Features must be on same scale\n",
    "4. **Less Interpretable** - Coefficients are shrunk, harder to interpret\n",
    "\n",
    "## ‚è±Ô∏è When to Use Ridge Regression\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Multicollinearity Present**\n",
    "- Example: Height in cm and height in inches (perfectly correlated)\n",
    "- Why: Linear regression coefficients become unstable\n",
    "- Solution: Ridge shrinks correlated coefficients\n",
    "\n",
    "**2. Many Features (High Dimensionality)**\n",
    "- Example: Gene expression data (20,000 genes, 100 samples)\n",
    "- Why: Linear regression overfits badly\n",
    "- Benefit: Regularization prevents overfitting\n",
    "\n",
    "**3. All Features are Relevant**\n",
    "- Example: Image pixels (all contribute to recognition)\n",
    "- Why: Don't want to remove features (Lasso would)\n",
    "- Use Ridge: Keeps all features, just shrinks coefficients\n",
    "\n",
    "**4. Overfitting Detected**\n",
    "- Example: Train R¬≤ = 0.99, Test R¬≤ = 0.60\n",
    "- Why: Model memorized training data\n",
    "- Solution: Ridge adds penalty for complexity\n",
    "\n",
    "**5. Small Dataset**\n",
    "- Example: 50 samples, 20 features\n",
    "- Why: Linear regression would overfit\n",
    "- Benefit: Ridge generalizes better\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Need Feature Selection**\n",
    "- Problem: Want to identify most important 10 features from 100\n",
    "- Better: Lasso regression (sets coefficients to exactly zero)\n",
    "- Why: Ridge shrinks all coefficients but never eliminates features\n",
    "\n",
    "**2. Features Not Scaled**\n",
    "- Problem: Feature 1 in [0, 1], Feature 2 in [0, 1000]\n",
    "- Better: Scale features first, then use Ridge\n",
    "- Why: Penalty affects large-scale features more\n",
    "\n",
    "**3. No Multicollinearity**\n",
    "- Problem: All features are independent\n",
    "- Better: Standard Linear Regression\n",
    "- Why: No need for regularization if no overfitting\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**Training:**\n",
    "```python\n",
    "Œ≤_ridge = (X^T X + Œ± I)^(-1) X^T y\n",
    "\n",
    "Where:\n",
    "  Œ± = regularization parameter\n",
    "  I = identity matrix\n",
    "```\n",
    "\n",
    "**Œ± (Alpha) Selection:**\n",
    "- **Œ± = 0**: Ridge = Linear Regression (no penalty)\n",
    "- **Œ± = small (0.1)**: Light regularization\n",
    "- **Œ± = large (100)**: Heavy regularization, coefficients ‚Üí 0\n",
    "- **Œ± = ‚àû**: All coefficients ‚Üí 0\n",
    "\n",
    "**Find Optimal Œ±:**\n",
    "- Cross-validation (try Œ± = 0.001, 0.01, 0.1, 1, 10, 100)\n",
    "- Choose Œ± with best validation performance\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Genomics** - Predict disease from gene expression (20K features)\n",
    "2. **Finance** - Stock prediction with correlated economic indicators\n",
    "3. **Image Processing** - Pixel-based predictions (correlated pixels)\n",
    "4. **Marketing** - Sales prediction with correlated channels\n",
    "5. **Climate Science** - Weather prediction (correlated sensors)\n",
    "6. **Medical Diagnosis** - Predict from correlated patient metrics\n",
    "7. **Text Analysis** - Sentiment with high-dimensional word features\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ **Always scale features** before Ridge (StandardScaler)  \n",
    "‚úÖ **Use cross-validation** to find optimal Œ±  \n",
    "‚úÖ **Œ± = 1.0** is good starting point  \n",
    "‚úÖ **Ridge keeps all features**, Lasso does feature selection  \n",
    "‚úÖ **Check multicollinearity** with VIF before using Ridge  \n",
    "‚úÖ **RidgeCV** automatically finds best Œ± via cross-validation  \n",
    "‚úÖ **Compare with Linear Regression** to see if regularization helps  \n",
    "‚úÖ **Larger Œ±** = more regularization = simpler model  \n",
    "‚úÖ **Works well** when most features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29eda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REGRESSION - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RIDGE REGRESSION (L2 REGULARIZATION) - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1. GENERATE DATA WITH MULTICOLLINEARITY\n",
    "print(\"\\n1. GENERATING DATA WITH CORRELATED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create correlated features (multicollinearity)\n",
    "x1 = np.random.randn(n_samples)\n",
    "x2 = x1 + np.random.randn(n_samples) * 0.1  # Highly correlated with x1\n",
    "x3 = x1 + np.random.randn(n_samples) * 0.1  # Also correlated with x1\n",
    "x4 = np.random.randn(n_samples)  # Independent\n",
    "x5 = np.random.randn(n_samples)  # Independent\n",
    "\n",
    "# Target variable\n",
    "y = 3*x1 + 2*x2 + 1.5*x3 + 4*x4 + 2*x5 + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Create DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'feature1': x1,\n",
    "    'feature2': x2,  # Correlated with feature1\n",
    "    'feature3': x3,  # Correlated with feature1\n",
    "    'feature4': x4,\n",
    "    'feature5': x5\n",
    "})\n",
    "\n",
    "print(\"Dataset created with multicollinearity:\")\n",
    "print(X.head())\n",
    "\n",
    "# Check correlations\n",
    "print(f\"\\nCorrelation matrix:\")\n",
    "correlations = X.corr()\n",
    "print(correlations)\n",
    "print(f\"\\nNote: feature1, feature2, feature3 are highly correlated!\")\n",
    "\n",
    "# 2. SPLIT AND SCALE DATA\n",
    "print(\"\\n2. SPLITTING AND SCALING DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features (IMPORTANT for Ridge!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeatures scaled to mean=0, std=1\")\n",
    "print(f\"Train mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"Train std: {X_train_scaled.std(axis=0)}\")\n",
    "\n",
    "# 3. COMPARE LINEAR REGRESSION VS RIDGE\n",
    "print(\"\\n3. COMPARING LINEAR REGRESSION VS RIDGE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Linear Regression (baseline)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# Ridge Regression (Œ±=1.0)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Compare coefficients\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Linear Reg': lr.coef_,\n",
    "    'Ridge (Œ±=1)': ridge.coef_\n",
    "})\n",
    "print(\"\\nCoefficient comparison:\")\n",
    "print(coef_comparison)\n",
    "print(f\"\\nObservation: Ridge coefficients are shrunk toward zero\")\n",
    "\n",
    "# Compare performance\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"Linear Regression R¬≤: {r2_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Ridge Regression R¬≤:  {r2_score(y_test, y_pred_ridge):.4f}\")\n",
    "\n",
    "# 4. FIND OPTIMAL ALPHA WITH CROSS-VALIDATION\n",
    "print(\"\\n4. FINDING OPTIMAL ALPHA WITH CROSS-VALIDATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Try different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "# RidgeCV automatically finds best alpha\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Alphas tested: {alphas}\")\n",
    "print(f\"\\nBest alpha: {ridge_cv.alpha_}\")\n",
    "\n",
    "# Train Ridge with best alpha\n",
    "best_ridge = Ridge(alpha=ridge_cv.alpha_)\n",
    "best_ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_best = best_ridge.predict(X_test_scaled)\n",
    "\n",
    "# 5. EVALUATE BEST MODEL\n",
    "print(\"\\n5. EVALUATING BEST RIDGE MODEL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred_best)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae = mean_absolute_error(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Ridge (Œ±={ridge_cv.alpha_}) Performance:\")\n",
    "print(f\"  R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE: {mae:.4f}\")\n",
    "\n",
    "print(f\"\\nCoefficients with best alpha:\")\n",
    "for feature, coef in zip(X.columns, best_ridge.coef_):\n",
    "    print(f\"  {feature}: {coef:8.4f}\")\n",
    "\n",
    "# 6. VISUALIZE REGULARIZATION PATH\n",
    "print(\"\\n6. VISUALIZING REGULARIZATION PATH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Train Ridge with different alphas\n",
    "alphas_range = np.logspace(-3, 3, 100)\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas_range:\n",
    "    ridge_temp = Ridge(alpha=alpha)\n",
    "    ridge_temp.fit(X_train_scaled, y_train)\n",
    "    coefs.append(ridge_temp.coef_)\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "\n",
    "# Plot regularization path\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Coefficients vs alpha\n",
    "for i, feature in enumerate(X.columns):\n",
    "    axes[0].plot(alphas_range, coefs[:, i], label=feature)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (regularization strength)')\n",
    "axes[0].set_ylabel('Coefficient value')\n",
    "axes[0].set_title('Ridge Regularization Path')\n",
    "axes[0].legend()\n",
    "axes[0].axvline(ridge_cv.alpha_, color='red', linestyle='--', \n",
    "                label=f'Best Œ±={ridge_cv.alpha_:.3f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1].scatter(y_test, y_pred_best, alpha=0.6)\n",
    "axes[1].plot([y_test.min(), y_test.max()], \n",
    "             [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "axes[1].set_title(f'Ridge Predictions (Œ±={ridge_cv.alpha_:.3f}, R¬≤={r2:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ridge_regression_results.png', dpi=100, bbox_inches='tight')\n",
    "print(\"Visualizations saved to 'ridge_regression_results.png'\")\n",
    "plt.close()\n",
    "\n",
    "# 7. COEFFICIENT MAGNITUDE COMPARISON\n",
    "print(\"\\n7. COEFFICIENT MAGNITUDE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Linear Reg': lr.coef_,\n",
    "    'Ridge (Œ±=0.1)': Ridge(alpha=0.1).fit(X_train_scaled, y_train).coef_,\n",
    "    'Ridge (Œ±=1)': Ridge(alpha=1.0).fit(X_train_scaled, y_train).coef_,\n",
    "    'Ridge (Œ±=10)': Ridge(alpha=10.0).fit(X_train_scaled, y_train).coef_,\n",
    "    'Ridge (Œ±=100)': Ridge(alpha=100.0).fit(X_train_scaled, y_train).coef_\n",
    "})\n",
    "\n",
    "print(\"Coefficient shrinkage with increasing alpha:\")\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\nAs Œ± increases, coefficients shrink toward zero\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Ridge Regression adds L2 penalty to prevent overfitting\")\n",
    "print(f\"‚úì Best alpha found via CV: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"‚úì Test R¬≤ = {r2:.4f}\")\n",
    "print(f\"‚úì Handles multicollinearity (correlated features)\")\n",
    "print(f\"‚úì Shrinks coefficients but never sets to exactly zero\")\n",
    "print(f\"‚úì Feature scaling is REQUIRED before Ridge\")\n",
    "print(f\"‚úì Use RidgeCV to automatically find best alpha\")\n",
    "print(f\"‚úì Larger alpha = more regularization = simpler model\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
