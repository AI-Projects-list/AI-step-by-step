{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86109b",
   "metadata": {},
   "source": [
    "# 1. Tensors - The Foundation\n",
    "\n",
    "## ðŸ“– What are Tensors?\n",
    "\n",
    "**Tensors** are the fundamental data structure in PyTorch - multi-dimensional arrays similar to NumPy arrays but with GPU acceleration and automatic differentiation support.\n",
    "\n",
    "**Tensor Dimensions:**\n",
    "- **0D (Scalar)**: Single number `tensor(5)`\n",
    "- **1D (Vector)**: `[1, 2, 3]`\n",
    "- **2D (Matrix)**: `[[1, 2], [3, 4]]`\n",
    "- **3D+**: Images (CÃ—HÃ—W), Videos (TÃ—CÃ—HÃ—W), etc.\n",
    "\n",
    "**Key Features:**\n",
    "- GPU acceleration for massive speedup\n",
    "- Automatic differentiation (autograd)\n",
    "- Seamless NumPy conversion\n",
    "- Dynamic computation graphs\n",
    "\n",
    "## ðŸŽ¯ Why Use Tensors?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **GPU Acceleration** - 10-100x faster than NumPy on GPU\n",
    "2. **Automatic Differentiation** - Compute gradients automatically\n",
    "3. **Neural Network Ready** - Designed for deep learning\n",
    "4. **Flexible** - Dynamic graphs, easy debugging\n",
    "5. **Research Friendly** - Pythonic, intuitive API\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Memory Overhead** - Tracking gradients uses extra memory\n",
    "2. **GPU Setup** - Requires CUDA installation\n",
    "3. **Learning Curve** - Different from NumPy in some ways\n",
    "\n",
    "## â±ï¸ When to Use Tensors\n",
    "\n",
    "### âœ… **Use When:**\n",
    "\n",
    "**1. Building Neural Networks**\n",
    "- Example: Image classification, NLP models\n",
    "- Why: Need automatic differentiation\n",
    "- Benefit: Gradients computed automatically\n",
    "\n",
    "**2. GPU Acceleration Needed**\n",
    "- Example: Train model on 1 million images\n",
    "- Why: GPU 50x faster than CPU\n",
    "- Performance: Minutes instead of hours\n",
    "\n",
    "**3. Deep Learning Research**\n",
    "- Example: Implement new architecture\n",
    "- Why: Flexible, dynamic graphs\n",
    "- Benefit: Easy experimentation\n",
    "\n",
    "**4. Large Matrix Operations**\n",
    "- Example: Matrix multiplication on huge matrices\n",
    "- Why: Optimized CUDA kernels\n",
    "- Speed: 100x faster than NumPy on GPU\n",
    "\n",
    "**5. Custom Gradient Logic**\n",
    "- Example: Novel loss functions, custom layers\n",
    "- Why: Full control over backpropagation\n",
    "- Use case: Research, specialized applications\n",
    "\n",
    "### âŒ **Don't Use When:**\n",
    "\n",
    "**1. Simple NumPy Operations**\n",
    "- Problem: Overhead for small operations\n",
    "- Better: Use NumPy directly\n",
    "- Why: NumPy faster for CPU-only small data\n",
    "\n",
    "**2. No GPU Available**\n",
    "- Problem: PyTorch on CPU slower than NumPy\n",
    "- Better: NumPy for CPU-only work\n",
    "- Why: Overhead without benefit\n",
    "\n",
    "**3. Production Inference Only**\n",
    "- Problem: Gradient tracking overhead\n",
    "- Better: Use `torch.no_grad()` or ONNX\n",
    "- Why: Don't need autograd for inference\n",
    "\n",
    "**4. Non-ML Tasks**\n",
    "- Problem: Overkill for basic math\n",
    "- Better: NumPy, SciPy\n",
    "- Why: Simpler tools for simpler tasks\n",
    "\n",
    "## ðŸ“Š How It Works\n",
    "\n",
    "**Tensor Creation:**\n",
    "```python\n",
    "torch.tensor([1, 2, 3])        # From list\n",
    "torch.zeros(3, 4)              # All zeros\n",
    "torch.ones(2, 3)               # All ones\n",
    "torch.randn(2, 2)              # Random normal\n",
    "torch.arange(0, 10, 2)         # Range\n",
    "```\n",
    "\n",
    "**Key Attributes:**\n",
    "- `.shape` or `.size()` - Dimensions\n",
    "- `.dtype` - Data type (float32, int64, etc.)\n",
    "- `.device` - CPU or GPU location\n",
    "- `.requires_grad` - Track gradients?\n",
    "\n",
    "## ðŸŒ Real-World Applications\n",
    "\n",
    "1. **Computer Vision** - Image processing, object detection\n",
    "2. **Natural Language Processing** - Text generation, translation\n",
    "3. **Recommender Systems** - Netflix, YouTube recommendations\n",
    "4. **Medical Imaging** - Disease detection, MRI analysis\n",
    "5. **Autonomous Vehicles** - Tesla autopilot, perception\n",
    "6. **Speech Recognition** - Siri, Alexa, Google Assistant\n",
    "7. **Drug Discovery** - Protein folding, molecule generation\n",
    "\n",
    "## ðŸ’¡ Key Insights\n",
    "\n",
    "âœ… Default dtype is `torch.float32` (32-bit float)  \n",
    "âœ… Move to GPU: `tensor.to('cuda')`  \n",
    "âœ… Convert to NumPy: `tensor.numpy()`  \n",
    "âœ… From NumPy: `torch.from_numpy(array)`  \n",
    "âœ… Clone tensor: `tensor.clone()` (not `tensor.copy()`)  \n",
    "âœ… Detach from graph: `tensor.detach()`  \n",
    "âœ… In-place ops end with `_`: `tensor.add_(5)`  \n",
    "âœ… Check CUDA: `torch.cuda.is_available()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORS - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PYTORCH TENSORS - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# NOTE: Install PyTorch first: pip install torch torchvision\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. CREATING TENSORS\n",
    "print(\"\\n1. CREATING TENSORS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# From Python list\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "print(f\"  Type: {type(tensor_from_list)}\")\n",
    "print(f\"  Shape: {tensor_from_list.shape}\")\n",
    "print(f\"  Data type: {tensor_from_list.dtype}\")\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "print(f\"\\n2D Tensor (Matrix):\\n{matrix}\")\n",
    "print(f\"  Shape: {matrix.shape}\")\n",
    "print(f\"  Dimensions: {matrix.ndim}\")\n",
    "\n",
    "# 3D tensor (common in deep learning)\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]],\n",
    "                          [[5, 6], [7, 8]]])\n",
    "print(f\"\\n3D Tensor:\\n{tensor_3d}\")\n",
    "print(f\"  Shape: {tensor_3d.shape}\")\n",
    "\n",
    "# 2. TENSOR INITIALIZATION METHODS\n",
    "print(\"\\n2. TENSOR INITIALIZATION METHODS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# All zeros\n",
    "zeros = torch.zeros(3, 4)\n",
    "print(f\"Zeros (3x4):\\n{zeros}\")\n",
    "\n",
    "# All ones\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"\\nOnes (2x3):\\n{ones}\")\n",
    "\n",
    "# Random normal distribution (mean=0, std=1)\n",
    "randn = torch.randn(2, 3)\n",
    "print(f\"\\nRandom Normal (2x3):\\n{randn}\")\n",
    "\n",
    "# Random uniform [0, 1)\n",
    "rand = torch.rand(2, 3)\n",
    "print(f\"\\nRandom Uniform (2x3):\\n{rand}\")\n",
    "\n",
    "# Random integers\n",
    "randint = torch.randint(0, 10, (3, 3))\n",
    "print(f\"\\nRandom Integers 0-10 (3x3):\\n{randint}\")\n",
    "\n",
    "# Range (like arange in NumPy)\n",
    "range_tensor = torch.arange(0, 10, 2)\n",
    "print(f\"\\nRange 0-10 step 2: {range_tensor}\")\n",
    "\n",
    "# Linearly spaced\n",
    "linspace = torch.linspace(0, 1, 5)\n",
    "print(f\"\\nLinspace 0-1 (5 points): {linspace}\")\n",
    "\n",
    "# Identity matrix\n",
    "eye = torch.eye(3)\n",
    "print(f\"\\nIdentity Matrix (3x3):\\n{eye}\")\n",
    "\n",
    "# Like another tensor (same shape)\n",
    "like_tensor = torch.ones_like(matrix)\n",
    "print(f\"\\nOnes like matrix:\\n{like_tensor}\")\n",
    "\n",
    "# 3. DATA TYPES\n",
    "print(\"\\n3. DATA TYPES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Different data types\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "int_tensor = torch.tensor([1, 2, 3])\n",
    "bool_tensor = torch.tensor([True, False, True])\n",
    "\n",
    "print(f\"Float tensor: {float_tensor}, dtype: {float_tensor.dtype}\")\n",
    "print(f\"Int tensor: {int_tensor}, dtype: {int_tensor.dtype}\")\n",
    "print(f\"Bool tensor: {bool_tensor}, dtype: {bool_tensor.dtype}\")\n",
    "\n",
    "# Specify dtype explicitly\n",
    "float32 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "float64 = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "int64 = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "\n",
    "print(f\"\\nFloat32: {float32}, dtype: {float32.dtype}\")\n",
    "print(f\"Float64: {float64}, dtype: {float64.dtype}\")\n",
    "print(f\"Int64: {int64}, dtype: {int64.dtype}\")\n",
    "\n",
    "# Type conversion\n",
    "converted = int_tensor.float()\n",
    "print(f\"\\nConverted int to float: {converted}, dtype: {converted.dtype}\")\n",
    "\n",
    "# 4. TENSOR ATTRIBUTES\n",
    "print(\"\\n4. TENSOR ATTRIBUTES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "example = torch.randn(3, 4, 5)\n",
    "print(f\"Example tensor shape: {example.shape}\")\n",
    "print(f\"  .shape: {example.shape}\")\n",
    "print(f\"  .size(): {example.size()}\")\n",
    "print(f\"  .ndim: {example.ndim} (number of dimensions)\")\n",
    "print(f\"  .numel(): {example.numel()} (total elements)\")\n",
    "print(f\"  .dtype: {example.dtype}\")\n",
    "print(f\"  .device: {example.device}\")\n",
    "print(f\"  .requires_grad: {example.requires_grad}\")\n",
    "\n",
    "# 5. NUMPY CONVERSION\n",
    "print(\"\\n5. NUMPY CONVERSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# PyTorch to NumPy\n",
    "pt_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "np_array = pt_tensor.numpy()\n",
    "print(f\"PyTorch tensor: {pt_tensor}\")\n",
    "print(f\"NumPy array: {np_array}, type: {type(np_array)}\")\n",
    "\n",
    "# NumPy to PyTorch\n",
    "np_array2 = np.array([10, 20, 30, 40, 50])\n",
    "pt_tensor2 = torch.from_numpy(np_array2)\n",
    "print(f\"\\nNumPy array: {np_array2}\")\n",
    "print(f\"PyTorch tensor: {pt_tensor2}, type: {type(pt_tensor2)}\")\n",
    "\n",
    "# Warning: Share memory!\n",
    "print(f\"\\nMemory sharing demonstration:\")\n",
    "np_arr = np.array([1, 2, 3])\n",
    "pt_ten = torch.from_numpy(np_arr)\n",
    "print(f\"Before: NumPy={np_arr}, PyTorch={pt_ten}\")\n",
    "np_arr[0] = 999\n",
    "print(f\"After modifying NumPy: NumPy={np_arr}, PyTorch={pt_ten} (shared!)\")\n",
    "\n",
    "# 6. GPU SUPPORT\n",
    "print(\"\\n6. GPU SUPPORT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create tensor on GPU\n",
    "    gpu_tensor = torch.tensor([1, 2, 3], device='cuda')\n",
    "    print(f\"\\n  Tensor on GPU: {gpu_tensor}\")\n",
    "    print(f\"  Device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # Move to GPU\n",
    "    cpu_tensor = torch.tensor([4, 5, 6])\n",
    "    gpu_moved = cpu_tensor.to('cuda')\n",
    "    print(f\"\\n  Moved to GPU: {gpu_moved}\")\n",
    "    \n",
    "    # Move back to CPU\n",
    "    back_to_cpu = gpu_moved.to('cpu')\n",
    "    print(f\"  Back to CPU: {back_to_cpu}\")\n",
    "else:\n",
    "    print(\"  No GPU available - using CPU\")\n",
    "    \n",
    "# Device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "tensor_on_device = torch.tensor([1, 2, 3], device=device)\n",
    "print(f\"Tensor on {device}: {tensor_on_device}\")\n",
    "\n",
    "# 7. BASIC TENSOR OPERATIONS\n",
    "print(\"\\n7. BASIC TENSOR OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"\\nElement-wise operations:\")\n",
    "print(f\"  a + b = {a + b}\")\n",
    "print(f\"  a - b = {a - b}\")\n",
    "print(f\"  a * b = {a * b} (element-wise multiplication)\")\n",
    "print(f\"  a / b = {a / b}\")\n",
    "print(f\"  a ** 2 = {a ** 2}\")\n",
    "\n",
    "# Mathematical functions\n",
    "print(f\"\\nMathematical functions:\")\n",
    "print(f\"  torch.sqrt(a) = {torch.sqrt(a.float())}\")\n",
    "print(f\"  torch.exp(a) = {torch.exp(a.float())}\")\n",
    "print(f\"  torch.log(a) = {torch.log(a.float())}\")\n",
    "print(f\"  torch.abs(a) = {torch.abs(a)}\")\n",
    "\n",
    "# Aggregation\n",
    "print(f\"\\nAggregation:\")\n",
    "print(f\"  a.sum() = {a.sum()}\")\n",
    "print(f\"  a.mean() = {a.float().mean()}\")\n",
    "print(f\"  a.max() = {a.max()}\")\n",
    "print(f\"  a.min() = {a.min()}\")\n",
    "print(f\"  a.argmax() = {a.argmax()} (index of max)\")\n",
    "\n",
    "# 8. MATRIX OPERATIONS\n",
    "print(\"\\n8. MATRIX OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "B = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"\\nMatrix B:\\n{B}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\nMatrix multiplication (A @ B):\\n{A @ B}\")\n",
    "print(f\"Alternative: torch.matmul(A, B):\\n{torch.matmul(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nTranspose A:\\n{A.T}\")\n",
    "\n",
    "# Inverse\n",
    "A_float = A.float()\n",
    "print(f\"\\nInverse of A:\\n{torch.inverse(A_float)}\")\n",
    "\n",
    "# Determinant\n",
    "print(f\"\\nDeterminant of A: {torch.det(A_float)}\")\n",
    "\n",
    "# 9. IN-PLACE OPERATIONS\n",
    "print(\"\\n9. IN-PLACE OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"Original x: {x}\")\n",
    "\n",
    "# Regular operation (creates new tensor)\n",
    "y = x.add(10)\n",
    "print(f\"After y = x.add(10): x={x}, y={y}\")\n",
    "\n",
    "# In-place operation (modifies original)\n",
    "x.add_(10)\n",
    "print(f\"After x.add_(10): x={x} (modified in-place)\")\n",
    "\n",
    "# Other in-place operations\n",
    "z = torch.tensor([5.0, 10.0, 15.0])\n",
    "print(f\"\\nOriginal z: {z}\")\n",
    "z.mul_(2)  # Multiply by 2\n",
    "print(f\"After z.mul_(2): {z}\")\n",
    "z.sqrt_()  # Square root\n",
    "print(f\"After z.sqrt_(): {z}\")\n",
    "\n",
    "# 10. PRACTICAL EXAMPLE: IMAGE PROCESSING\n",
    "print(\"\\n10. PRACTICAL EXAMPLE: IMAGE PROCESSING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simulate image tensor (Batch, Channels, Height, Width)\n",
    "batch_size = 4\n",
    "channels = 3  # RGB\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "# Create random image batch\n",
    "images = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Channels: {channels} (RGB)\")\n",
    "print(f\"  Height: {height}\")\n",
    "print(f\"  Width: {width}\")\n",
    "print(f\"  Total elements: {images.numel():,}\")\n",
    "print(f\"  Memory size: {images.element_size() * images.numel() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Normalize images (common preprocessing)\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "normalized_images = (images - mean) / std\n",
    "\n",
    "print(f\"\\nNormalized images:\")\n",
    "print(f\"  Mean: {normalized_images.mean():.4f}\")\n",
    "print(f\"  Std: {normalized_images.std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Tensors are the foundation of PyTorch\")\n",
    "print(\"âœ“ Similar to NumPy arrays but with GPU support\")\n",
    "print(\"âœ“ Support automatic differentiation for deep learning\")\n",
    "print(\"âœ“ Create with torch.tensor(), torch.zeros(), torch.randn(), etc.\")\n",
    "print(\"âœ“ Move to GPU with .to('cuda') or .cuda()\")\n",
    "print(\"âœ“ Convert to NumPy with .numpy()\")\n",
    "print(\"âœ“ In-place operations end with underscore (_)\")\n",
    "print(\"âœ“ Check attributes: .shape, .dtype, .device, .requires_grad\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7be01",
   "metadata": {},
   "source": [
    "# 2. Tensor Operations\n",
    "\n",
    "## ðŸ“– What are Tensor Operations?\n",
    "\n",
    "**Tensor operations** are mathematical computations performed on tensors - the building blocks of all neural network calculations.\n",
    "\n",
    "**Operation Categories:**\n",
    "1. **Element-wise**: Apply function to each element independently\n",
    "2. **Reduction**: Aggregate tensor to smaller dimension (sum, mean)\n",
    "3. **Reshaping**: Change tensor structure without changing data\n",
    "4. **Linear Algebra**: Matrix multiplication, transpose, inverse\n",
    "5. **Broadcasting**: Auto-expand dimensions for operations\n",
    "\n",
    "## ðŸŽ¯ Why Use Tensor Operations?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Vectorization** - 100x faster than Python loops\n",
    "2. **GPU Acceleration** - Parallel processing on thousands of cores\n",
    "3. **Memory Efficient** - Optimized memory access patterns\n",
    "4. **Automatic Differentiation** - Gradients computed automatically\n",
    "5. **Numerical Stability** - Optimized implementations\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Learning Curve** - Different from Python operations\n",
    "2. **Broadcasting Rules** - Can be confusing initially\n",
    "3. **Memory** - Large intermediate tensors\n",
    "\n",
    "## â±ï¸ When to Use Tensor Operations\n",
    "\n",
    "### âœ… **Use When:**\n",
    "\n",
    "**1. Working with Arrays of Data**\n",
    "- Example: Process batch of 1000 images\n",
    "- Why: Single operation instead of loop\n",
    "- Speed: 100x faster than Python loops\n",
    "\n",
    "**2. Neural Network Forward Pass**\n",
    "- Example: y = activation(W @ x + b)\n",
    "- Why: Matrix multiplication, element-wise ops\n",
    "- Benefit: GPU parallelization\n",
    "\n",
    "**3. Need Broadcasting**\n",
    "- Example: Normalize batch by subtracting mean\n",
    "- Why: Auto-expand dimensions\n",
    "- Code: `batch - mean` (mean broadcasts)\n",
    "\n",
    "**4. Statistical Computations**\n",
    "- Example: Calculate mean, std, variance\n",
    "- Why: Optimized reduction operations\n",
    "- Use case: Data preprocessing, monitoring\n",
    "\n",
    "**5. Linear Algebra**\n",
    "- Example: Solve linear systems, eigenvalues\n",
    "- Why: BLAS/LAPACK optimized routines\n",
    "- Performance: 10x faster than pure Python\n",
    "\n",
    "### âŒ **Don't Use When:**\n",
    "\n",
    "**1. Single Values**\n",
    "- Problem: Overhead for scalar operations\n",
    "- Better: Regular Python math\n",
    "- Why: `x + y` simpler than `torch.tensor(x) + torch.tensor(y)`\n",
    "\n",
    "**2. Complex Conditional Logic**\n",
    "- Problem: Hard to vectorize if/else\n",
    "- Better: Use loops when necessary\n",
    "- Why: Readability > performance sometimes\n",
    "\n",
    "**3. String Operations**\n",
    "- Problem: Tensors are numeric\n",
    "- Better: Python strings, pandas\n",
    "- Why: Not designed for text\n",
    "\n",
    "## ðŸ“Š How It Works\n",
    "\n",
    "**Broadcasting Rules:**\n",
    "1. Dimensions aligned from right to left\n",
    "2. Size 1 dimensions stretched to match\n",
    "3. Missing dimensions added on left\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "A: (3, 1)  â†’  (3, 4)  # Stretch column\n",
    "B: (4,)    â†’  (3, 4)  # Add dimension\n",
    "```\n",
    "\n",
    "## ðŸŒ Real-World Applications\n",
    "\n",
    "1. **Image Processing** - Batch normalization, filtering\n",
    "2. **Neural Networks** - All layer computations\n",
    "3. **Data Preprocessing** - Scaling, normalization\n",
    "4. **Scientific Computing** - Simulations, modeling\n",
    "5. **Signal Processing** - FFT, convolutions\n",
    "6. **Computer Graphics** - 3D transformations\n",
    "\n",
    "## ðŸ’¡ Key Insights\n",
    "\n",
    "âœ… Always vectorize - avoid Python loops  \n",
    "âœ… Use broadcasting for dimension mismatches  \n",
    "âœ… `@` for matrix multiplication, `*` for element-wise  \n",
    "âœ… Specify `dim` for reductions: `sum(dim=0)`  \n",
    "âœ… Use `keepdim=True` to preserve dimensions  \n",
    "âœ… Check shapes: `print(tensor.shape)` before operations  \n",
    "âœ… In-place ops (`+=`, `*=`) save memory  \n",
    "âœ… Use `torch.no_grad()` for inference (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR OPERATIONS - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PYTORCH TENSOR OPERATIONS - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 1. ELEMENT-WISE OPERATIONS\n",
    "print(\"\\n1. ELEMENT-WISE OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "b = torch.tensor([5.0, 6.0, 7.0, 8.0])\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "# Arithmetic operations\n",
    "print(f\"\\nArithmetic:\")\n",
    "print(f\"  a + b = {a + b}\")\n",
    "print(f\"  a - b = {a - b}\")\n",
    "print(f\"  a * b = {a * b} (element-wise)\")\n",
    "print(f\"  a / b = {a / b}\")\n",
    "print(f\"  a ** 2 = {a ** 2}\")\n",
    "print(f\"  a % 2 = {a % 2}\")\n",
    "\n",
    "# Comparison operations\n",
    "print(f\"\\nComparison (returns boolean):\")\n",
    "print(f\"  a > 2 = {a > 2}\")\n",
    "print(f\"  a == 3 = {a == 3}\")\n",
    "print(f\"  a != b = {a != b}\")\n",
    "\n",
    "# Mathematical functions\n",
    "print(f\"\\nMathematical functions:\")\n",
    "print(f\"  torch.sqrt(a) = {torch.sqrt(a)}\")\n",
    "print(f\"  torch.exp(a) = {torch.exp(a)}\")\n",
    "print(f\"  torch.log(a) = {torch.log(a)}\")\n",
    "print(f\"  torch.sin(a) = {torch.sin(a)}\")\n",
    "print(f\"  torch.abs(a-b) = {torch.abs(a-b)}\")\n",
    "\n",
    "# Clamping\n",
    "print(f\"\\nClamping (clip values):\")\n",
    "print(f\"  torch.clamp(a, 2, 3) = {torch.clamp(a, 2, 3)} (values between 2 and 3)\")\n",
    "\n",
    "# 2. REDUCTION OPERATIONS\n",
    "print(\"\\n2. REDUCTION OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "\n",
    "# Reduce all\n",
    "print(f\"\\nReduce all elements:\")\n",
    "print(f\"  sum: {matrix.sum()}\")\n",
    "print(f\"  mean: {matrix.mean()}\")\n",
    "print(f\"  max: {matrix.max()}\")\n",
    "print(f\"  min: {matrix.min()}\")\n",
    "print(f\"  std: {matrix.std()}\")\n",
    "print(f\"  var: {matrix.var()}\")\n",
    "\n",
    "# Reduce along dimension\n",
    "print(f\"\\nReduce along dim=0 (columns):\")\n",
    "print(f\"  sum: {matrix.sum(dim=0)}\")\n",
    "print(f\"  mean: {matrix.mean(dim=0)}\")\n",
    "print(f\"  max: {matrix.max(dim=0)}\")\n",
    "\n",
    "print(f\"\\nReduce along dim=1 (rows):\")\n",
    "print(f\"  sum: {matrix.sum(dim=1)}\")\n",
    "print(f\"  mean: {matrix.mean(dim=1)}\")\n",
    "\n",
    "# Keep dimensions\n",
    "print(f\"\\nWith keepdim=True:\")\n",
    "print(f\"  sum(dim=0, keepdim=True):\\n{matrix.sum(dim=0, keepdim=True)}\")\n",
    "print(f\"  Shape: {matrix.sum(dim=0, keepdim=True).shape}\")\n",
    "\n",
    "# 3. MATRIX OPERATIONS\n",
    "print(\"\\n3. MATRIX OPERATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6],\n",
    "                  [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Matrix B:\\n{B}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\nMatrix multiplication:\")\n",
    "print(f\"  A @ B:\\n{A @ B}\")\n",
    "print(f\"  torch.matmul(A, B):\\n{torch.matmul(A, B)}\")\n",
    "print(f\"  torch.mm(A, B):\\n{torch.mm(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nTranspose:\")\n",
    "print(f\"  A.T:\\n{A.T}\")\n",
    "print(f\"  A.transpose(0, 1):\\n{A.transpose(0, 1)}\")\n",
    "\n",
    "# Dot product (1D tensors)\n",
    "v1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "v2 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "print(f\"\\nDot product:\")\n",
    "print(f\"  v1 = {v1}\")\n",
    "print(f\"  v2 = {v2}\")\n",
    "print(f\"  torch.dot(v1, v2) = {torch.dot(v1, v2)}\")\n",
    "\n",
    "# Outer product\n",
    "print(f\"\\nOuter product:\")\n",
    "print(f\"  torch.outer(v1, v2):\\n{torch.outer(v1, v2)}\")\n",
    "\n",
    "# 4. BROADCASTING\n",
    "print(\"\\n4. BROADCASTING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Example 1: Scalar broadcasting\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "scalar = 10\n",
    "result = tensor + scalar\n",
    "print(f\"Scalar broadcasting:\")\n",
    "print(f\"  Tensor:\\n{tensor}\")\n",
    "print(f\"  Scalar: {scalar}\")\n",
    "print(f\"  Result:\\n{result}\")\n",
    "\n",
    "# Example 2: Vector broadcasting\n",
    "batch = torch.tensor([[1, 2, 3],\n",
    "                      [4, 5, 6],\n",
    "                      [7, 8, 9]])\n",
    "vector = torch.tensor([10, 20, 30])\n",
    "result = batch + vector\n",
    "print(f\"\\nVector broadcasting:\")\n",
    "print(f\"  Batch (3x3):\\n{batch}\")\n",
    "print(f\"  Vector (3,): {vector}\")\n",
    "print(f\"  Result:\\n{result}\")\n",
    "\n",
    "# Example 3: Matrix broadcasting\n",
    "mat1 = torch.tensor([[1],\n",
    "                     [2],\n",
    "                     [3]])  # Shape: (3, 1)\n",
    "mat2 = torch.tensor([10, 20, 30])  # Shape: (3,)\n",
    "result = mat1 + mat2\n",
    "print(f\"\\nMatrix broadcasting:\")\n",
    "print(f\"  Matrix1 (3x1):\\n{mat1}\")\n",
    "print(f\"  Matrix2 (3,): {mat2}\")\n",
    "print(f\"  Result (3x3):\\n{result}\")\n",
    "\n",
    "# 5. INDEXING AND MASKING\n",
    "print(\"\\n5. INDEXING AND MASKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "data = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(f\"Data: {data}\")\n",
    "\n",
    "# Boolean mask\n",
    "mask = data > 5\n",
    "print(f\"\\nMask (data > 5): {mask}\")\n",
    "print(f\"Filtered data: {data[mask]}\")\n",
    "\n",
    "# Where (conditional selection)\n",
    "result = torch.where(data > 5, data, torch.tensor(0))\n",
    "print(f\"\\nWhere (if > 5, keep, else 0): {result}\")\n",
    "\n",
    "# Advanced indexing\n",
    "indices = torch.tensor([0, 2, 4, 6])\n",
    "print(f\"\\nIndices: {indices}\")\n",
    "print(f\"Selected: {data[indices]}\")\n",
    "\n",
    "# 6. CONCATENATION AND STACKING\n",
    "print(\"\\n6. CONCATENATION AND STACKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "t1 = torch.tensor([[1, 2],\n",
    "                   [3, 4]])\n",
    "t2 = torch.tensor([[5, 6],\n",
    "                   [7, 8]])\n",
    "\n",
    "print(f\"Tensor 1:\\n{t1}\")\n",
    "print(f\"Tensor 2:\\n{t2}\")\n",
    "\n",
    "# Concatenate\n",
    "print(f\"\\nConcatenate dim=0 (vertical):\\n{torch.cat([t1, t2], dim=0)}\")\n",
    "print(f\"\\nConcatenate dim=1 (horizontal):\\n{torch.cat([t1, t2], dim=1)}\")\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "print(f\"\\nStack dim=0:\\n{torch.stack([t1, t2], dim=0)}\")\n",
    "print(f\"Shape: {torch.stack([t1, t2], dim=0).shape}\")\n",
    "\n",
    "# 7. PERFORMANCE: VECTORIZATION\n",
    "print(\"\\n7. PERFORMANCE: VECTORIZATION VS LOOPS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create large tensor\n",
    "size = 1_000_000\n",
    "data = torch.randn(size)\n",
    "\n",
    "# Method 1: Python loop (SLOW)\n",
    "print(f\"\\nProcessing {size:,} elements...\")\n",
    "start = time.time()\n",
    "result_loop = torch.zeros(size)\n",
    "for i in range(size):\n",
    "    result_loop[i] = data[i] ** 2\n",
    "time_loop = time.time() - start\n",
    "print(f\"Python loop: {time_loop:.4f} seconds\")\n",
    "\n",
    "# Method 2: Vectorized (FAST)\n",
    "start = time.time()\n",
    "result_vec = data ** 2\n",
    "time_vec = time.time() - start\n",
    "print(f\"Vectorized: {time_vec:.4f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_loop/time_vec:.1f}x faster!\")\n",
    "print(f\"Results match: {torch.allclose(result_loop, result_vec)}\")\n",
    "\n",
    "# 8. PRACTICAL EXAMPLE: BATCH NORMALIZATION\n",
    "print(\"\\n8. PRACTICAL EXAMPLE: BATCH NORMALIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simulate batch of features\n",
    "batch_size = 32\n",
    "features = 10\n",
    "batch = torch.randn(batch_size, features)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Original statistics:\")\n",
    "print(f\"  Mean per feature: {batch.mean(dim=0)[:5]}... (first 5)\")\n",
    "print(f\"  Std per feature: {batch.std(dim=0)[:5]}... (first 5)\")\n",
    "\n",
    "# Normalize: (x - mean) / std\n",
    "mean = batch.mean(dim=0, keepdim=True)\n",
    "std = batch.std(dim=0, keepdim=True)\n",
    "normalized = (batch - mean) / (std + 1e-8)  # Add epsilon for stability\n",
    "\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"  Mean per feature: {normalized.mean(dim=0)[:5]}... (â‰ˆ 0)\")\n",
    "print(f\"  Std per feature: {normalized.std(dim=0)[:5]}... (â‰ˆ 1)\")\n",
    "\n",
    "# 9. COMMON OPERATIONS SUMMARY\n",
    "print(\"\\n9. COMMON OPERATIONS SUMMARY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "print(f\"Tensor x:\\n{x}\")\n",
    "print(f\"\\nOperation examples:\")\n",
    "print(f\"  x.shape: {x.shape}\")\n",
    "print(f\"  x.sum(): {x.sum()}\")\n",
    "print(f\"  x.sum(dim=0): {x.sum(dim=0)} (sum columns)\")\n",
    "print(f\"  x.sum(dim=1): {x.sum(dim=1)} (sum rows)\")\n",
    "print(f\"  x.max(): {x.max()} (max value)\")\n",
    "print(f\"  x.argmax(): {x.argmax()} (index of max in flattened)\")\n",
    "print(f\"  x.argmax(dim=1): {x.argmax(dim=1)} (argmax per row)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Tensor operations are foundation of neural networks\")\n",
    "print(\"âœ“ Always vectorize - avoid Python loops (100x faster)\")\n",
    "print(\"âœ“ Use broadcasting for dimension mismatches\")\n",
    "print(\"âœ“ @ for matrix multiplication, * for element-wise\")\n",
    "print(\"âœ“ Reductions: sum(), mean(), max(), min() with dim parameter\")\n",
    "print(\"âœ“ keepdim=True preserves dimensions after reduction\")\n",
    "print(\"âœ“ Boolean masking for filtering: tensor[tensor > 5]\")\n",
    "print(\"âœ“ Concatenate with torch.cat(), stack with torch.stack()\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
