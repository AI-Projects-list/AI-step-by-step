{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8737e0af",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Learning Guide\n",
    "\n",
    "## What is Exploratory Data Analysis?\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the critical first step in any data science project. It's the process of investigating datasets to discover patterns, spot anomalies, test hypotheses, and check assumptions using statistical summaries and graphical representations.\n",
    "\n",
    "## Why is EDA Important?\n",
    "\n",
    "1. **Understand Your Data** - Get familiar with the structure, size, and content\n",
    "2. **Detect Data Quality Issues** - Find missing values, duplicates, and errors\n",
    "3. **Identify Patterns** - Discover relationships and trends in the data\n",
    "4. **Spot Outliers** - Detect anomalies that might affect analysis\n",
    "5. **Inform Feature Engineering** - Guide creation of new features for modeling\n",
    "6. **Select Appropriate Models** - Choose algorithms based on data characteristics\n",
    "7. **Validate Assumptions** - Test statistical assumptions before modeling\n",
    "\n",
    "## What You'll Learn in This Notebook\n",
    "\n",
    "This comprehensive guide covers **11 essential EDA topics**:\n",
    "\n",
    "1. [Data Loading & Initial Inspection](#1-data-loading--initial-inspection) - Understanding dataset structure\n",
    "2. [Data Types & Structure](#2-data-types--structure) - Identifying numerical and categorical features\n",
    "3. [Missing Data Analysis](#3-missing-data-analysis) - Detecting and handling missing values\n",
    "4. [Descriptive Statistics](#4-descriptive-statistics) - Central tendency, dispersion, and shape\n",
    "5. [Univariate Analysis](#5-univariate-analysis-single-variable) - Distribution of individual variables\n",
    "6. [Bivariate Analysis](#6-bivariate-analysis-two-variables) - Relationships between pairs of variables\n",
    "7. [Multivariate Analysis](#7-multivariate-analysis) - Complex multi-variable patterns\n",
    "8. [Outlier Detection](#8-outlier-detection) - Identifying and handling anomalies\n",
    "9. [Distribution Analysis](#9-distribution-analysis) - Understanding data shapes and normality\n",
    "10. [Correlation Analysis](#10-correlation-analysis) - Feature relationships and dependencies\n",
    "11. [Best Practices & Summary](#11-eda-best-practices--summary) - Professional EDA workflow\n",
    "\n",
    "## Tools & Libraries Used\n",
    "\n",
    "- **pandas** - Data manipulation and analysis\n",
    "- **numpy** - Numerical operations\n",
    "- **matplotlib** - Basic plotting\n",
    "- **seaborn** - Statistical visualizations\n",
    "- **scipy** - Statistical tests and functions\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run cells sequentially** - Each section builds on previous knowledge\n",
    "2. **Experiment** - Modify code to explore different aspects\n",
    "3. **Apply to your data** - Replace sample data with your own datasets\n",
    "4. **Document insights** - Keep notes on findings and decisions\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "This notebook uses a **synthetic loan default dataset** with 1,000 customers and the following features:\n",
    "\n",
    "- **Numerical**: age, income, credit_score, loan_amount, employment_years\n",
    "- **Categorical**: education, owns_home\n",
    "- **Target**: default (0=No, 1=Yes)\n",
    "\n",
    "This realistic dataset includes missing values and outliers to demonstrate real-world EDA scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP: Import Essential Libraries for EDA\n",
    "# ============================================\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a35a8f",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Inspection\n",
    "\n",
    "**Why**: Understand the basic structure and size of your dataset before diving deeper.\n",
    "\n",
    "**When to use**: \n",
    "- First step in any EDA\n",
    "- After loading new datasets\n",
    "- When combining multiple data sources\n",
    "\n",
    "**Key Questions**:\n",
    "- How many rows and columns?\n",
    "- What do the first/last few rows look like?\n",
    "- What's the overall structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. DATA LOADING & INITIAL INSPECTION\n",
    "# ============================================\n",
    "\n",
    "# Create sample dataset (in practice, you'd load from CSV, database, etc.)\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    'customer_id': range(1, 1001),\n",
    "    'age': np.random.randint(18, 80, 1000),\n",
    "    'income': np.random.normal(50000, 20000, 1000),\n",
    "    'credit_score': np.random.randint(300, 850, 1000),\n",
    "    'loan_amount': np.random.normal(15000, 7000, 1000),\n",
    "    'employment_years': np.random.randint(0, 40, 1000),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 1000),\n",
    "    'owns_home': np.random.choice(['Yes', 'No'], 1000),\n",
    "    'default': np.random.choice([0, 1], 1000, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values (realistic scenario)\n",
    "df.loc[np.random.choice(df.index, 50, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30, replace=False), 'credit_score'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 20, replace=False), 'employment_years'] = np.nan\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIAL DATA INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Shape of dataset\n",
    "print(f\"\\n1. Dataset Shape: {df.shape}\")\n",
    "print(f\"   - Rows (observations): {df.shape[0]:,}\")\n",
    "print(f\"   - Columns (features): {df.shape[1]}\")\n",
    "\n",
    "# 2. First few rows\n",
    "print(\"\\n2. First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Last few rows\n",
    "print(\"\\n3. Last 5 rows:\")\n",
    "print(df.tail())\n",
    "\n",
    "# 4. Random sample\n",
    "print(\"\\n4. Random sample (3 rows):\")\n",
    "print(df.sample(3))\n",
    "\n",
    "# 5. Column names and index\n",
    "print(\"\\n5. Column Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 6. Basic info\n",
    "print(\"\\n6. Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# 7. Memory usage\n",
    "print(f\"\\n7. Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a9172",
   "metadata": {},
   "source": [
    "## 2. Data Types & Structure\n",
    "\n",
    "**Why**: Understanding data types helps choose appropriate analysis methods and visualizations.\n",
    "\n",
    "**When to use**:\n",
    "- After loading data\n",
    "- Before data cleaning\n",
    "- When planning transformations\n",
    "\n",
    "**Key Questions**:\n",
    "- Are numeric columns actually numeric?\n",
    "- Are categorical variables properly encoded?\n",
    "- Do data types match expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ba9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. DATA TYPES & STRUCTURE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA TYPES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Data types overview\n",
    "print(\"\\n1. Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 2. Count of each data type\n",
    "print(\"\\n2. Data Type Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# 3. Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\n3. Numerical Columns ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "\n",
    "print(f\"\\n4. Categorical Columns ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# 5. Unique values in each column\n",
    "print(\"\\n5. Unique Values Count:\")\n",
    "for col in df.columns:\n",
    "    print(f\"   {col:20s}: {df[col].nunique():5d} unique values\")\n",
    "\n",
    "# 6. Cardinality check (for categorical variables)\n",
    "print(\"\\n6. Categorical Variables Detail:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"   - Unique values: {df[col].nunique()}\")\n",
    "    print(f\"   - Value counts:\")\n",
    "    print(df[col].value_counts())\n",
    "\n",
    "# 7. Check for constant columns (no variance)\n",
    "print(\"\\n7. Checking for Constant Columns:\")\n",
    "constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "if constant_cols:\n",
    "    print(f\"   Constant columns found: {constant_cols}\")\n",
    "else:\n",
    "    print(\"   âœ“ No constant columns found\")\n",
    "\n",
    "# 8. Data type conversion example\n",
    "print(\"\\n8. Example: Converting Data Types\")\n",
    "print(f\"   Before: default dtype = {df['default'].dtype}\")\n",
    "df['default'] = df['default'].astype('category')\n",
    "print(f\"   After: default dtype = {df['default'].dtype}\")\n",
    "print(f\"   Memory saved: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb864b06",
   "metadata": {},
   "source": [
    "## 3. Missing Data Analysis\n",
    "\n",
    "**Why**: Missing data can bias results, reduce statistical power, and cause model errors.\n",
    "\n",
    "**When to use**:\n",
    "- Before any analysis\n",
    "- When deciding on imputation strategies\n",
    "- When assessing data quality\n",
    "\n",
    "**Strategies**:\n",
    "- **Drop**: If < 5% missing and random\n",
    "- **Impute**: Mean/median (numerical), mode (categorical)\n",
    "- **Flag**: Create indicator variable for missingness\n",
    "- **Model**: Predict missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. MISSING DATA ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING DATA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Count missing values\n",
    "print(\"\\n1. Missing Values Count:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# 2. Percentage of missing values\n",
    "print(\"\\n2. Missing Values Percentage:\")\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "\n",
    "# 3. Create comprehensive missing data report\n",
    "def missing_data_report(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data[missing_data['Total'] > 0]\n",
    "\n",
    "print(\"\\n3. Missing Data Report:\")\n",
    "print(missing_data_report(df))\n",
    "\n",
    "# 4. Visualize missing data\n",
    "print(\"\\n4. Missing Data Visualization:\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "missing_data_report(df).plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Missing Values by Column')\n",
    "axes[0].set_ylabel('Count / Percentage')\n",
    "axes[0].legend(['Total Missing', 'Percentage Missing'])\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis', ax=axes[1])\n",
    "axes[1].set_title('Missing Data Heatmap (Yellow = Missing)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Missing data patterns\n",
    "print(\"\\n5. Missing Data Patterns:\")\n",
    "print(f\"   - Rows with any missing value: {df.isnull().any(axis=1).sum()} ({df.isnull().any(axis=1).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   - Rows with all values present: {(~df.isnull().any(axis=1)).sum()} ({(~df.isnull().any(axis=1)).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 6. Correlation of missingness\n",
    "print(\"\\n6. Missingness Correlation:\")\n",
    "missing_corr = df.isnull().corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(missing_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation of Missing Values')\n",
    "plt.show()\n",
    "\n",
    "# 7. Example: Handling missing data\n",
    "print(\"\\n7. Example: Handling Missing Data\")\n",
    "\n",
    "# Strategy 1: Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(f\"   After dropping rows: {df_dropped.shape[0]} rows remain\")\n",
    "\n",
    "# Strategy 2: Fill with median (numerical)\n",
    "df_filled = df.copy()\n",
    "for col in ['income', 'credit_score', 'employment_years']:\n",
    "    if col in df_filled.columns:\n",
    "        df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
    "print(f\"   After median imputation: {df_filled.isnull().sum().sum()} missing values remain\")\n",
    "\n",
    "# Strategy 3: Forward fill (time series)\n",
    "# df_ffill = df.fillna(method='ffill')\n",
    "\n",
    "# Strategy 4: Create missing indicator\n",
    "df['income_missing'] = df['income'].isnull().astype(int)\n",
    "print(f\"   Created missing indicator: income_missing\")\n",
    "print(f\"   - Missing: {df['income_missing'].sum()}\")\n",
    "print(f\"   - Present: {(~df['income_missing'].astype(bool)).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676d1ad",
   "metadata": {},
   "source": [
    "## 4. Descriptive Statistics\n",
    "\n",
    "**Why**: Get a statistical summary of your data's central tendency, dispersion, and shape.\n",
    "\n",
    "**When to use**:\n",
    "- Understanding data distributions\n",
    "- Identifying potential outliers\n",
    "- Comparing variables\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Central Tendency**: Mean, median, mode\n",
    "- **Dispersion**: Std dev, variance, range, IQR\n",
    "- **Shape**: Skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. DESCRIPTIVE STATISTICS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Basic statistics for numerical columns\n",
    "print(\"\\n1. Basic Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 2. Extended statistics\n",
    "print(\"\\n2. Extended Statistical Summary (including percentiles):\")\n",
    "print(df.describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))\n",
    "\n",
    "# 3. Statistics for categorical columns\n",
    "print(\"\\n3. Categorical Variables Summary:\")\n",
    "print(df.describe(include=['object', 'category']))\n",
    "\n",
    "# 4. Individual statistics\n",
    "print(\"\\n4. Detailed Statistics for Numerical Columns:\")\n",
    "for col in numerical_cols:\n",
    "    if col != 'customer_id':  # Skip ID column\n",
    "        print(f\"\\n   {col}:\")\n",
    "        print(f\"      Mean:     {df[col].mean():.2f}\")\n",
    "        print(f\"      Median:   {df[col].median():.2f}\")\n",
    "        print(f\"      Mode:     {df[col].mode().values[0]:.2f}\")\n",
    "        print(f\"      Std Dev:  {df[col].std():.2f}\")\n",
    "        print(f\"      Variance: {df[col].var():.2f}\")\n",
    "        print(f\"      Min:      {df[col].min():.2f}\")\n",
    "        print(f\"      Max:      {df[col].max():.2f}\")\n",
    "        print(f\"      Range:    {df[col].max() - df[col].min():.2f}\")\n",
    "        print(f\"      IQR:      {df[col].quantile(0.75) - df[col].quantile(0.25):.2f}\")\n",
    "        print(f\"      Skewness: {df[col].skew():.2f}\")\n",
    "        print(f\"      Kurtosis: {df[col].kurtosis():.2f}\")\n",
    "\n",
    "# 5. Custom aggregations\n",
    "print(\"\\n5. Custom Aggregations:\")\n",
    "custom_stats = df[['age', 'income', 'credit_score']].agg({\n",
    "    'age': ['min', 'max', 'mean', 'median'],\n",
    "    'income': ['min', 'max', 'mean', 'std'],\n",
    "    'credit_score': ['min', 'max', 'mean', 'median']\n",
    "})\n",
    "print(custom_stats)\n",
    "\n",
    "# 6. Group statistics\n",
    "print(\"\\n6. Statistics by Group (Education Level):\")\n",
    "print(df.groupby('education')['income'].describe())\n",
    "\n",
    "# 7. Correlation of statistics with target\n",
    "print(\"\\n7. Statistics Summary with Target (Default):\")\n",
    "print(df.groupby('default').agg({\n",
    "    'age': ['mean', 'median'],\n",
    "    'income': ['mean', 'median'],\n",
    "    'credit_score': ['mean', 'median']\n",
    "}))\n",
    "\n",
    "# 8. Coefficient of Variation (CV)\n",
    "print(\"\\n8. Coefficient of Variation (Relative Variability):\")\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount']:\n",
    "    cv = (df[col].std() / df[col].mean()) * 100\n",
    "    print(f\"   {col:20s}: {cv:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4c3be",
   "metadata": {},
   "source": [
    "## 5. Univariate Analysis (Single Variable)\n",
    "\n",
    "**Why**: Understand the distribution and characteristics of individual variables.\n",
    "\n",
    "**When to use**:\n",
    "- Analyzing each feature independently\n",
    "- Detecting outliers and anomalies\n",
    "- Understanding value distributions\n",
    "\n",
    "**Techniques**:\n",
    "- **Numerical**: Histograms, box plots, density plots\n",
    "- **Categorical**: Bar charts, pie charts, frequency tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. UNIVARIATE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UNIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Histogram for numerical variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "numerical_features = ['age', 'income', 'credit_score', 'loan_amount', 'employment_years']\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    df[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].axvline(df[col].mean(), color='red', linestyle='--', label=f'Mean: {df[col].mean():.1f}')\n",
    "    axes[idx].axvline(df[col].median(), color='green', linestyle='--', label=f'Median: {df[col].median():.1f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Density plots (KDE)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    df[col].dropna().plot(kind='density', ax=axes[idx], color='blue', alpha=0.7)\n",
    "    axes[idx].set_title(f'Density Plot of {col}')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Box plots for outlier detection\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    df.boxplot(column=col, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Box Plot of {col}')\n",
    "    axes[idx].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Categorical variables - bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Education distribution\n",
    "df['education'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Education Levels')\n",
    "axes[0].set_xlabel('Education')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Home ownership distribution\n",
    "df['owns_home'].value_counts().plot(kind='bar', ax=axes[1], color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Home Ownership')\n",
    "axes[1].set_xlabel('Owns Home')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Pie charts for categorical variables\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Education pie chart\n",
    "df['education'].value_counts().plot(kind='pie', ax=axes[0], autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Education Distribution')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Default rate pie chart\n",
    "df['default'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90, \n",
    "                                   labels=['No Default', 'Default'], colors=['green', 'red'])\n",
    "axes[1].set_title('Default Rate')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Value counts for categorical variables\n",
    "print(\"\\n6. Frequency Tables:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n   {col}:\")\n",
    "    counts = df[col].value_counts()\n",
    "    percentages = df[col].value_counts(normalize=True) * 100\n",
    "    freq_table = pd.DataFrame({'Count': counts, 'Percentage': percentages})\n",
    "    print(freq_table)\n",
    "\n",
    "# 7. QQ Plot for normality check\n",
    "from scipy.stats import probplot\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    probplot(df[col].dropna(), dist=\"norm\", plot=axes[idx])\n",
    "    axes[idx].set_title(f'Q-Q Plot: {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n7. Normality Interpretation:\")\n",
    "print(\"   - Points on the line = Normal distribution\")\n",
    "print(\"   - Points deviate = Non-normal distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f8030",
   "metadata": {},
   "source": [
    "## 6. Bivariate Analysis (Two Variables)\n",
    "\n",
    "**Why**: Understand relationships between pairs of variables.\n",
    "\n",
    "**When to use**:\n",
    "- Exploring feature relationships\n",
    "- Identifying correlations\n",
    "- Understanding target variable relationships\n",
    "\n",
    "**Techniques**:\n",
    "- **Numerical vs Numerical**: Scatter plots, correlation\n",
    "- **Numerical vs Categorical**: Box plots, violin plots\n",
    "- **Categorical vs Categorical**: Crosstabs, stacked bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576376a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. BIVARIATE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Scatter plots (Numerical vs Numerical)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "pairs = [\n",
    "    ('age', 'income'),\n",
    "    ('credit_score', 'loan_amount'),\n",
    "    ('employment_years', 'income'),\n",
    "    ('age', 'credit_score')\n",
    "]\n",
    "\n",
    "for idx, (x, y) in enumerate(pairs):\n",
    "    axes[idx].scatter(df[x], df[y], alpha=0.5)\n",
    "    axes[idx].set_xlabel(x)\n",
    "    axes[idx].set_ylabel(y)\n",
    "    axes[idx].set_title(f'{y} vs {x}')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[x].dropna(), df[y].dropna(), 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(df[x].dropna(), p(df[x].dropna()), \"r--\", alpha=0.8, label='Trend')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Correlation between variables\n",
    "print(\"\\n1. Correlation Analysis:\")\n",
    "correlations = df[['age', 'income', 'credit_score', 'loan_amount', 'employment_years']].corr()\n",
    "print(correlations)\n",
    "\n",
    "# 3. Scatter plot with color (target variable)\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df['credit_score'], df['income'], \n",
    "                     c=df['default'], cmap='RdYlGn_r', alpha=0.6)\n",
    "plt.xlabel('Credit Score')\n",
    "plt.ylabel('Income')\n",
    "plt.title('Income vs Credit Score (colored by Default)')\n",
    "plt.colorbar(scatter, label='Default (0=No, 1=Yes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 4. Box plots (Numerical vs Categorical)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Income by education\n",
    "df.boxplot(column='income', by='education', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Income by Education Level')\n",
    "axes[0, 0].set_xlabel('Education')\n",
    "axes[0, 0].set_ylabel('Income')\n",
    "\n",
    "# Credit score by home ownership\n",
    "df.boxplot(column='credit_score', by='owns_home', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Credit Score by Home Ownership')\n",
    "axes[0, 1].set_xlabel('Owns Home')\n",
    "axes[0, 1].set_ylabel('Credit Score')\n",
    "\n",
    "# Loan amount by default\n",
    "df.boxplot(column='loan_amount', by='default', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Loan Amount by Default Status')\n",
    "axes[1, 0].set_xlabel('Default (0=No, 1=Yes)')\n",
    "axes[1, 0].set_ylabel('Loan Amount')\n",
    "\n",
    "# Age by education\n",
    "df.boxplot(column='age', by='education', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Age by Education Level')\n",
    "axes[1, 1].set_xlabel('Education')\n",
    "axes[1, 1].set_ylabel('Age')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Violin plots (better than box plots for distribution)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.violinplot(data=df, x='education', y='income', ax=axes[0])\n",
    "axes[0].set_title('Income Distribution by Education')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.violinplot(data=df, x='default', y='credit_score', ax=axes[1])\n",
    "axes[1].set_title('Credit Score Distribution by Default')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Crosstab (Categorical vs Categorical)\n",
    "print(\"\\n2. Crosstab Analysis:\")\n",
    "crosstab = pd.crosstab(df['education'], df['default'], normalize='index') * 100\n",
    "print(\"\\nDefault Rate by Education (%):\")\n",
    "print(crosstab)\n",
    "\n",
    "crosstab2 = pd.crosstab(df['owns_home'], df['default'], normalize='index') * 100\n",
    "print(\"\\nDefault Rate by Home Ownership (%):\")\n",
    "print(crosstab2)\n",
    "\n",
    "# 7. Stacked bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "pd.crosstab(df['education'], df['default']).plot(kind='bar', stacked=True, ax=axes[0], \n",
    "                                                  color=['green', 'red'], alpha=0.7)\n",
    "axes[0].set_title('Default by Education (Stacked)')\n",
    "axes[0].set_xlabel('Education')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(['No Default', 'Default'])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "pd.crosstab(df['owns_home'], df['default']).plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                                                  color=['green', 'red'], alpha=0.7)\n",
    "axes[1].set_title('Default by Home Ownership (Stacked)')\n",
    "axes[1].set_xlabel('Owns Home')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['No Default', 'Default'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Pairwise scatter plot matrix\n",
    "print(\"\\n3. Pairwise Relationships (Scatter Matrix):\")\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df[['age', 'income', 'credit_score', 'loan_amount']], \n",
    "               figsize=(12, 12), alpha=0.5, diagonal='hist')\n",
    "plt.suptitle('Pairwise Scatter Matrix', y=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98097b1e",
   "metadata": {},
   "source": [
    "## 7. Multivariate Analysis\n",
    "\n",
    "**Why**: Understand complex relationships among multiple variables simultaneously.\n",
    "\n",
    "**When to use**:\n",
    "- Exploring high-dimensional data\n",
    "- Understanding feature interactions\n",
    "- Dimensionality reduction\n",
    "\n",
    "**Techniques**:\n",
    "- Correlation heatmaps\n",
    "- Pair plots with hue\n",
    "- Parallel coordinates\n",
    "- PCA visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. MULTIVARIATE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df[['age', 'income', 'credit_score', 'loan_amount', 'employment_years']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, \n",
    "            linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n1. High Correlations (|r| > 0.5):\")\n",
    "# Find highly correlated pairs\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.5:\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "for col1, col2, corr_val in high_corr:\n",
    "    print(f\"   {col1} <-> {col2}: {corr_val:.3f}\")\n",
    "\n",
    "# 2. Pair plot with target variable\n",
    "print(\"\\n2. Pair Plot with Default Status:\")\n",
    "# Sample for faster plotting\n",
    "df_sample = df.sample(min(500, len(df)), random_state=42)\n",
    "sns.pairplot(df_sample[['age', 'income', 'credit_score', 'loan_amount', 'default']], \n",
    "             hue='default', diag_kind='kde', palette='Set1')\n",
    "plt.suptitle('Pairplot by Default Status', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 3. Grouped box plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Income by education and default\n",
    "df.boxplot(column='income', by=['education', 'default'], ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Income by Education and Default')\n",
    "axes[0, 0].set_xlabel('Education - Default')\n",
    "\n",
    "# Credit score by home ownership and default\n",
    "df.boxplot(column='credit_score', by=['owns_home', 'default'], ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Credit Score by Home Ownership and Default')\n",
    "axes[0, 1].set_xlabel('Owns Home - Default')\n",
    "\n",
    "# Loan amount by education and home ownership\n",
    "df.boxplot(column='loan_amount', by=['education', 'owns_home'], ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Loan Amount by Education and Home Ownership')\n",
    "axes[1, 0].set_xlabel('Education - Owns Home')\n",
    "\n",
    "# Age by education and default\n",
    "df.boxplot(column='age', by=['education', 'default'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Age by Education and Default')\n",
    "axes[1, 1].set_xlabel('Education - Default')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Facet grid (multiple subplots)\n",
    "print(\"\\n3. Facet Grid Analysis:\")\n",
    "g = sns.FacetGrid(df, col='owns_home', row='default', hue='education', \n",
    "                  height=4, aspect=1.5, palette='Set2')\n",
    "g.map(plt.scatter, 'credit_score', 'income', alpha=0.5)\n",
    "g.add_legend()\n",
    "plt.show()\n",
    "\n",
    "# 5. 3D scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(df['age'], df['income'], df['credit_score'], \n",
    "                     c=df['default'], cmap='RdYlGn_r', alpha=0.6)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_zlabel('Credit Score')\n",
    "ax.set_title('3D Scatter: Age, Income, Credit Score (colored by Default)')\n",
    "plt.colorbar(scatter, label='Default')\n",
    "plt.show()\n",
    "\n",
    "# 6. Parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Normalize data for better visualization\n",
    "df_normalized = df[['age', 'income', 'credit_score', 'loan_amount', 'default']].copy()\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount']:\n",
    "    df_normalized[col] = (df_normalized[col] - df_normalized[col].min()) / (df_normalized[col].max() - df_normalized[col].min())\n",
    "\n",
    "parallel_coordinates(df_normalized.sample(200), 'default', color=['green', 'red'], alpha=0.5)\n",
    "plt.title('Parallel Coordinates Plot')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 7. Grouped statistics\n",
    "print(\"\\n4. Grouped Statistics (Education Ã— Home Ownership Ã— Default):\")\n",
    "grouped_stats = df.groupby(['education', 'owns_home', 'default']).agg({\n",
    "    'income': 'mean',\n",
    "    'credit_score': 'mean',\n",
    "    'loan_amount': 'mean'\n",
    "}).round(2)\n",
    "print(grouped_stats)\n",
    "\n",
    "# 8. Pivot table\n",
    "print(\"\\n5. Pivot Table (Average Income by Education and Home Ownership):\")\n",
    "pivot = df.pivot_table(values='income', index='education', \n",
    "                       columns='owns_home', aggfunc='mean')\n",
    "print(pivot)\n",
    "\n",
    "# Heatmap of pivot table\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt='.0f', cmap='YlGnBu', linewidths=1)\n",
    "plt.title('Average Income by Education and Home Ownership')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3ad9e",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection\n",
    "\n",
    "**Why**: Outliers can skew analysis and impact model performance.\n",
    "\n",
    "**When to use**:\n",
    "- Before modeling\n",
    "- When cleaning data\n",
    "- When investigating anomalies\n",
    "\n",
    "**Methods**:\n",
    "- **IQR Method**: Values beyond Q1-1.5Ã—IQR or Q3+1.5Ã—IQR\n",
    "- **Z-Score**: |z| > 3 (assuming normal distribution)\n",
    "- **Visual**: Box plots, scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4456ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. OUTLIER DETECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. IQR Method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"\\n1. IQR Method for Outlier Detection:\")\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount']:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"      Lower bound: {lower:.2f}\")\n",
    "    print(f\"      Upper bound: {upper:.2f}\")\n",
    "    print(f\"      Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 2. Z-Score Method\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "    outliers_idx = np.where(z_scores > threshold)[0]\n",
    "    return df.iloc[outliers_idx]\n",
    "\n",
    "print(\"\\n2. Z-Score Method (|z| > 3):\")\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount']:\n",
    "    outliers = detect_outliers_zscore(df, col)\n",
    "    print(f\"   {col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 3. Box plots for visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features_to_check = ['age', 'income', 'credit_score', 'loan_amount']\n",
    "\n",
    "for idx, col in enumerate(features_to_check):\n",
    "    df.boxplot(column=col, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Box Plot: {col}')\n",
    "    axes[idx].set_ylabel(col)\n",
    "    \n",
    "    # Mark outlier boundaries\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    axes[idx].axhline(y=lower, color='r', linestyle='--', label=f'Lower: {lower:.0f}')\n",
    "    axes[idx].axhline(y=upper, color='r', linestyle='--', label=f'Upper: {upper:.0f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Scatter plot highlighting outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Income outliers\n",
    "outliers_income, lower_i, upper_i = detect_outliers_iqr(df, 'income')\n",
    "axes[0].scatter(df.index, df['income'], alpha=0.5, label='Normal')\n",
    "axes[0].scatter(outliers_income.index, outliers_income['income'], \n",
    "                color='red', alpha=0.8, label='Outliers')\n",
    "axes[0].axhline(y=lower_i, color='r', linestyle='--')\n",
    "axes[0].axhline(y=upper_i, color='r', linestyle='--')\n",
    "axes[0].set_title('Income Outliers')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Income')\n",
    "axes[0].legend()\n",
    "\n",
    "# Credit score outliers\n",
    "outliers_credit, lower_c, upper_c = detect_outliers_iqr(df, 'credit_score')\n",
    "axes[1].scatter(df.index, df['credit_score'], alpha=0.5, label='Normal')\n",
    "axes[1].scatter(outliers_credit.index, outliers_credit['credit_score'], \n",
    "                color='red', alpha=0.8, label='Outliers')\n",
    "axes[1].axhline(y=lower_c, color='r', linestyle='--')\n",
    "axes[1].axhline(y=upper_c, color='r', linestyle='--')\n",
    "axes[1].set_title('Credit Score Outliers')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('Credit Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Outlier summary\n",
    "print(\"\\n3. Outlier Summary:\")\n",
    "outlier_summary = []\n",
    "\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount', 'employment_years']:\n",
    "    outliers_iqr, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outliers_z = detect_outliers_zscore(df, col)\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'IQR_Outliers': len(outliers_iqr),\n",
    "        'IQR_Pct': f\"{len(outliers_iqr)/len(df)*100:.1f}%\",\n",
    "        'Z_Outliers': len(outliers_z),\n",
    "        'Z_Pct': f\"{len(outliers_z)/len(df)*100:.1f}%\",\n",
    "        'Lower_Bound': f\"{lower:.1f}\",\n",
    "        'Upper_Bound': f\"{upper:.1f}\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))\n",
    "\n",
    "# 6. Handling outliers (examples)\n",
    "print(\"\\n4. Outlier Handling Strategies:\")\n",
    "\n",
    "# Strategy 1: Remove outliers\n",
    "df_no_outliers = df.copy()\n",
    "for col in ['income', 'loan_amount']:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_no_outliers, col)\n",
    "    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower) & (df_no_outliers[col] <= upper)]\n",
    "\n",
    "print(f\"   - Remove: {len(df)} â†’ {len(df_no_outliers)} rows\")\n",
    "\n",
    "# Strategy 2: Cap outliers (winsorization)\n",
    "df_capped = df.copy()\n",
    "for col in ['income', 'loan_amount']:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_capped, col)\n",
    "    df_capped[col] = df_capped[col].clip(lower, upper)\n",
    "\n",
    "print(f\"   - Cap: Min income = {df_capped['income'].min():.0f}, Max = {df_capped['income'].max():.0f}\")\n",
    "\n",
    "# Strategy 3: Transform (log transformation)\n",
    "df_transformed = df.copy()\n",
    "df_transformed['income_log'] = np.log1p(df_transformed['income'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "df['income'].hist(bins=30, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('Original Income Distribution')\n",
    "axes[0].set_xlabel('Income')\n",
    "\n",
    "df_transformed['income_log'].hist(bins=30, ax=axes[1], edgecolor='black', color='green')\n",
    "axes[1].set_title('Log-Transformed Income Distribution')\n",
    "axes[1].set_xlabel('Log(Income)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"   - Transform: Skewness before = {df['income'].skew():.2f}, after = {df_transformed['income_log'].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49438995",
   "metadata": {},
   "source": [
    "## 9. Distribution Analysis\n",
    "\n",
    "**Why**: Understanding distributions helps choose appropriate statistical tests and transformations.\n",
    "\n",
    "**When to use**:\n",
    "- Before modeling\n",
    "- When selecting transformations\n",
    "- When testing assumptions\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Normal**: Mean = median, symmetric\n",
    "- **Skewed**: Right (positive) or left (negative)\n",
    "- **Kurtosis**: Heavy or light tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ddb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. DISTRIBUTION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Distribution statistics\n",
    "print(\"\\n1. Distribution Metrics:\")\n",
    "distribution_stats = []\n",
    "\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount', 'employment_years']:\n",
    "    distribution_stats.append({\n",
    "        'Feature': col,\n",
    "        'Mean': df[col].mean(),\n",
    "        'Median': df[col].median(),\n",
    "        'Mode': df[col].mode().values[0] if len(df[col].mode()) > 0 else np.nan,\n",
    "        'Skewness': df[col].skew(),\n",
    "        'Kurtosis': df[col].kurtosis()\n",
    "    })\n",
    "\n",
    "dist_df = pd.DataFrame(distribution_stats)\n",
    "print(dist_df.round(2).to_string(index=False))\n",
    "\n",
    "print(\"\\n   Interpretation:\")\n",
    "print(\"   - Skewness ~ 0: Symmetric\")\n",
    "print(\"   - Skewness > 0: Right-skewed (tail extends right)\")\n",
    "print(\"   - Skewness < 0: Left-skewed (tail extends left)\")\n",
    "print(\"   - Kurtosis ~ 0: Normal distribution\")\n",
    "print(\"   - Kurtosis > 0: Heavy tails (more outliers)\")\n",
    "print(\"   - Kurtosis < 0: Light tails (fewer outliers)\")\n",
    "\n",
    "# 2. Histogram + KDE + Normal curve\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features = ['age', 'income', 'credit_score', 'loan_amount', 'employment_years']\n",
    "\n",
    "for idx, col in enumerate(features):\n",
    "    # Histogram\n",
    "    data = df[col].dropna()\n",
    "    axes[idx].hist(data, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # KDE\n",
    "    data.plot(kind='density', ax=axes[idx], color='red', linewidth=2, label='KDE')\n",
    "    \n",
    "    # Normal curve for comparison\n",
    "    mu, sigma = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[idx].plot(x, stats.norm.pdf(x, mu, sigma), 'g--', linewidth=2, label='Normal')\n",
    "    \n",
    "    axes[idx].set_title(f'Distribution: {col}')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].axvline(data.mean(), color='red', linestyle='--', alpha=0.5, label=f'Mean: {data.mean():.0f}')\n",
    "    axes[idx].axvline(data.median(), color='green', linestyle='--', alpha=0.5, label=f'Median: {data.median():.0f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Normality tests\n",
    "from scipy.stats import shapiro, normaltest, kstest\n",
    "\n",
    "print(\"\\n2. Normality Tests (p-value > 0.05 suggests normal):\")\n",
    "normality_results = []\n",
    "\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount']:\n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    shapiro_stat, shapiro_p = shapiro(data.sample(min(5000, len(data))))  # Sample for speed\n",
    "    \n",
    "    # D'Agostino's KÂ² test\n",
    "    k2_stat, k2_p = normaltest(data)\n",
    "    \n",
    "    normality_results.append({\n",
    "        'Feature': col,\n",
    "        'Shapiro_p': f\"{shapiro_p:.4f}\",\n",
    "        'K2_p': f\"{k2_p:.4f}\",\n",
    "        'Normal?': 'Yes' if shapiro_p > 0.05 and k2_p > 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "norm_df = pd.DataFrame(normality_results)\n",
    "print(norm_df.to_string(index=False))\n",
    "\n",
    "# 4. Q-Q plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(features):\n",
    "    data = df[col].dropna()\n",
    "    stats.probplot(data, dist=\"norm\", plot=axes[idx])\n",
    "    axes[idx].set_title(f'Q-Q Plot: {col}')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Transformations for normality\n",
    "print(\"\\n3. Testing Transformations:\")\n",
    "\n",
    "# Log transformation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Income - original vs log\n",
    "axes[0, 0].hist(df['income'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title(f'Income (Original) - Skew: {df[\"income\"].skew():.2f}')\n",
    "axes[0, 0].set_xlabel('Income')\n",
    "\n",
    "income_log = np.log1p(df['income'].dropna())\n",
    "axes[0, 1].hist(income_log, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].set_title(f'Income (Log) - Skew: {income_log.skew():.2f}')\n",
    "axes[0, 1].set_xlabel('Log(Income)')\n",
    "\n",
    "# Loan amount - original vs sqrt\n",
    "axes[1, 0].hist(df['loan_amount'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title(f'Loan Amount (Original) - Skew: {df[\"loan_amount\"].skew():.2f}')\n",
    "axes[1, 0].set_xlabel('Loan Amount')\n",
    "\n",
    "loan_sqrt = np.sqrt(df['loan_amount'].dropna())\n",
    "axes[1, 1].hist(loan_sqrt, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].set_title(f'Loan Amount (Sqrt) - Skew: {loan_sqrt.skew():.2f}')\n",
    "axes[1, 1].set_xlabel('Sqrt(Loan Amount)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Distribution by category\n",
    "print(\"\\n4. Distribution by Category:\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Income by default\n",
    "for default_val in [0, 1]:\n",
    "    df[df['default'] == default_val]['income'].plot(kind='density', ax=axes[0, 0], \n",
    "                                                      label=f'Default={default_val}', alpha=0.7)\n",
    "axes[0, 0].set_title('Income Distribution by Default')\n",
    "axes[0, 0].set_xlabel('Income')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Credit score by education\n",
    "for edu in df['education'].unique():\n",
    "    df[df['education'] == edu]['credit_score'].plot(kind='density', ax=axes[0, 1], \n",
    "                                                      label=edu, alpha=0.7)\n",
    "axes[0, 1].set_title('Credit Score Distribution by Education')\n",
    "axes[0, 1].set_xlabel('Credit Score')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Age by home ownership\n",
    "for home in df['owns_home'].unique():\n",
    "    df[df['owns_home'] == home]['age'].plot(kind='density', ax=axes[1, 0], \n",
    "                                             label=f'Owns Home={home}', alpha=0.7)\n",
    "axes[1, 0].set_title('Age Distribution by Home Ownership')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Loan amount by default\n",
    "for default_val in [0, 1]:\n",
    "    df[df['default'] == default_val]['loan_amount'].plot(kind='density', ax=axes[1, 1], \n",
    "                                                           label=f'Default={default_val}', alpha=0.7)\n",
    "axes[1, 1].set_title('Loan Amount Distribution by Default')\n",
    "axes[1, 1].set_xlabel('Loan Amount')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817d35a",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis\n",
    "\n",
    "**Why**: Identify relationships between variables to inform feature selection and engineering.\n",
    "\n",
    "**When to use**:\n",
    "- Feature selection\n",
    "- Multicollinearity detection\n",
    "- Understanding feature relationships\n",
    "\n",
    "**Methods**:\n",
    "- **Pearson**: Linear relationships\n",
    "- **Spearman**: Monotonic relationships\n",
    "- **Kendall**: Ordinal relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fde789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. CORRELATION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Pearson correlation (linear relationships)\n",
    "print(\"\\n1. Pearson Correlation Matrix:\")\n",
    "pearson_corr = df[['age', 'income', 'credit_score', 'loan_amount', 'employment_years']].corr(method='pearson')\n",
    "print(pearson_corr.round(3))\n",
    "\n",
    "# 2. Spearman correlation (monotonic relationships)\n",
    "print(\"\\n2. Spearman Correlation Matrix:\")\n",
    "spearman_corr = df[['age', 'income', 'credit_score', 'loan_amount', 'employment_years']].corr(method='spearman')\n",
    "print(spearman_corr.round(3))\n",
    "\n",
    "# 3. Correlation heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pearson\n",
    "sns.heatmap(pearson_corr, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
    "axes[0].set_title('Pearson Correlation Heatmap')\n",
    "\n",
    "# Spearman\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[1])\n",
    "axes[1].set_title('Spearman Correlation Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Correlation with target variable\n",
    "print(\"\\n3. Correlation with Target (Default):\")\n",
    "target_corr = df[['age', 'income', 'credit_score', 'loan_amount', 'employment_years', 'default']].corr()['default'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "target_corr.drop('default').plot(kind='barh', color='steelblue', edgecolor='black')\n",
    "plt.title('Feature Correlation with Default')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 5. Highly correlated features (multicollinearity check)\n",
    "print(\"\\n4. Highly Correlated Pairs (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(pearson_corr.columns)):\n",
    "    for j in range(i+1, len(pearson_corr.columns)):\n",
    "        if abs(pearson_corr.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': pearson_corr.columns[i],\n",
    "                'Feature 2': pearson_corr.columns[j],\n",
    "                'Correlation': pearson_corr.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    corr_pairs_df = pd.DataFrame(high_corr_pairs)\n",
    "    print(corr_pairs_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"   âœ“ No highly correlated pairs found (good for modeling!)\")\n",
    "\n",
    "# 6. Correlation triangle (upper triangle only)\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
    "sns.heatmap(pearson_corr, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix (Upper Triangle)')\n",
    "plt.show()\n",
    "\n",
    "# 7. Partial correlation (controlling for other variables)\n",
    "print(\"\\n5. Correlation Interpretation:\")\n",
    "print(\"   |r| < 0.3  : Weak\")\n",
    "print(\"   0.3 â‰¤ |r| < 0.7 : Moderate\")\n",
    "print(\"   |r| â‰¥ 0.7  : Strong\")\n",
    "print(\"\\n   r > 0 : Positive (both increase together)\")\n",
    "print(\"   r < 0 : Negative (one increases, other decreases)\")\n",
    "\n",
    "# 8. Point-biserial correlation (continuous vs binary)\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "print(\"\\n6. Point-Biserial Correlation (Continuous vs Binary Target):\")\n",
    "for col in ['age', 'income', 'credit_score', 'loan_amount', 'employment_years']:\n",
    "    # Remove NaN values\n",
    "    mask = df[col].notna() & df['default'].notna()\n",
    "    corr, p_value = pointbiserialr(df.loc[mask, 'default'], df.loc[mask, col])\n",
    "    print(f\"   {col:20s}: r={corr:6.3f}, p-value={p_value:.4f}\")\n",
    "\n",
    "# 9. Categorical correlation (CramÃ©r's V)\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Calculate CramÃ©r's V statistic for categorical variables\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k-1, r-1))\n",
    "\n",
    "print(\"\\n7. CramÃ©r's V (Categorical Associations):\")\n",
    "cat_vars = ['education', 'owns_home', 'default']\n",
    "\n",
    "for i in range(len(cat_vars)):\n",
    "    for j in range(i+1, len(cat_vars)):\n",
    "        v = cramers_v(df[cat_vars[i]], df[cat_vars[j]])\n",
    "        print(f\"   {cat_vars[i]:15s} Ã— {cat_vars[j]:15s}: V={v:.3f}\")\n",
    "\n",
    "# 10. Correlation scatter plots with trend lines\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "corr_pairs = [\n",
    "    ('age', 'income'),\n",
    "    ('credit_score', 'income'),\n",
    "    ('employment_years', 'income'),\n",
    "    ('credit_score', 'loan_amount')\n",
    "]\n",
    "\n",
    "for idx, (x, y) in enumerate(corr_pairs):\n",
    "    # Scatter plot\n",
    "    axes[idx].scatter(df[x], df[y], alpha=0.5, s=20)\n",
    "    \n",
    "    # Trend line\n",
    "    mask = df[x].notna() & df[y].notna()\n",
    "    z = np.polyfit(df.loc[mask, x], df.loc[mask, y], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(df.loc[mask, x].sort_values(), \n",
    "                   p(df.loc[mask, x].sort_values()), \n",
    "                   \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = df[[x, y]].corr().iloc[0, 1]\n",
    "    axes[idx].set_title(f'{y} vs {x} (r={corr:.3f})')\n",
    "    axes[idx].set_xlabel(x)\n",
    "    axes[idx].set_ylabel(y)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5402fa",
   "metadata": {},
   "source": [
    "## 11. EDA Best Practices & Summary\n",
    "\n",
    "**Key Takeaways**:\n",
    "\n",
    "### **EDA Workflow**\n",
    "1. **Load & Inspect** â†’ Understand structure\n",
    "2. **Clean** â†’ Handle missing data, duplicates\n",
    "3. **Explore** â†’ Univariate, bivariate, multivariate\n",
    "4. **Transform** â†’ Handle outliers, normalize, encode\n",
    "5. **Visualize** â†’ Tell data stories\n",
    "6. **Document** â†’ Record findings and decisions\n",
    "\n",
    "### **Common Pitfalls to Avoid**\n",
    "- Skipping EDA and jumping to modeling\n",
    "- Ignoring missing data patterns\n",
    "- Not checking for outliers\n",
    "- Assuming normality without testing\n",
    "- Overlooking data quality issues\n",
    "- Not documenting assumptions\n",
    "\n",
    "### **Tools & Libraries**\n",
    "- **pandas**: Data manipulation\n",
    "- **numpy**: Numerical operations\n",
    "- **matplotlib/seaborn**: Visualization\n",
    "- **scipy**: Statistical tests\n",
    "- **plotly**: Interactive plots\n",
    "- **pandas-profiling**: Automated EDA reports\n",
    "\n",
    "### **When is EDA Complete?**\n",
    "You've answered:\n",
    "- What is the data structure?\n",
    "- Are there quality issues?\n",
    "- What are the distributions?\n",
    "- How do variables relate?\n",
    "- What transformations are needed?\n",
    "- Which features matter most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPREHENSIVE EDA REPORT GENERATOR\n",
    "# ============================================\n",
    "\n",
    "def generate_eda_report(df, target_column=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive EDA report for a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset to analyze\n",
    "    target_column : str, optional\n",
    "        Name of the target variable for supervised learning\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive report with all EDA insights\n",
    "    \"\"\"\n",
    "    \n",
    "    report = {}\n",
    "    \n",
    "    # 1. Dataset Overview\n",
    "    report['overview'] = {\n",
    "        'shape': df.shape,\n",
    "        'rows': df.shape[0],\n",
    "        'columns': df.shape[1],\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'duplicates': df.duplicated().sum()\n",
    "    }\n",
    "    \n",
    "    # 2. Column Types\n",
    "    report['column_types'] = {\n",
    "        'numerical': df.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "        'categorical': df.select_dtypes(include=['object', 'category']).columns.tolist(),\n",
    "        'datetime': df.select_dtypes(include=['datetime']).columns.tolist()\n",
    "    }\n",
    "    \n",
    "    # 3. Missing Data\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100)\n",
    "    report['missing_data'] = {\n",
    "        col: {'count': int(missing[col]), 'percentage': float(missing_pct[col])}\n",
    "        for col in df.columns if missing[col] > 0\n",
    "    }\n",
    "    \n",
    "    # 4. Numerical Statistics\n",
    "    report['numerical_stats'] = {}\n",
    "    for col in report['column_types']['numerical']:\n",
    "        if df[col].nunique() > 1:  # Skip constant columns\n",
    "            report['numerical_stats'][col] = {\n",
    "                'mean': float(df[col].mean()),\n",
    "                'median': float(df[col].median()),\n",
    "                'std': float(df[col].std()),\n",
    "                'min': float(df[col].min()),\n",
    "                'max': float(df[col].max()),\n",
    "                'skewness': float(df[col].skew()),\n",
    "                'kurtosis': float(df[col].kurtosis()),\n",
    "                'unique': int(df[col].nunique())\n",
    "            }\n",
    "    \n",
    "    # 5. Categorical Statistics\n",
    "    report['categorical_stats'] = {}\n",
    "    for col in report['column_types']['categorical']:\n",
    "        report['categorical_stats'][col] = {\n",
    "            'unique': int(df[col].nunique()),\n",
    "            'top_value': str(df[col].mode().values[0]) if len(df[col].mode()) > 0 else None,\n",
    "            'top_freq': int(df[col].value_counts().iloc[0]) if len(df[col]) > 0 else 0,\n",
    "            'distribution': df[col].value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    # 6. Outliers (IQR method)\n",
    "    report['outliers'] = {}\n",
    "    for col in report['column_types']['numerical']:\n",
    "        if df[col].nunique() > 1:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "            \n",
    "            report['outliers'][col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': float(len(outliers) / len(df) * 100),\n",
    "                'lower_bound': float(lower),\n",
    "                'upper_bound': float(upper)\n",
    "            }\n",
    "    \n",
    "    # 7. Correlations\n",
    "    if len(report['column_types']['numerical']) > 1:\n",
    "        corr_matrix = df[report['column_types']['numerical']].corr()\n",
    "        \n",
    "        # Find high correlations\n",
    "        high_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                    high_corr.append({\n",
    "                        'var1': corr_matrix.columns[i],\n",
    "                        'var2': corr_matrix.columns[j],\n",
    "                        'correlation': float(corr_matrix.iloc[i, j])\n",
    "                    })\n",
    "        \n",
    "        report['high_correlations'] = high_corr\n",
    "    \n",
    "    # 8. Target Analysis (if provided)\n",
    "    if target_column and target_column in df.columns:\n",
    "        report['target_analysis'] = {\n",
    "            'name': target_column,\n",
    "            'type': str(df[target_column].dtype),\n",
    "            'distribution': df[target_column].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        # Correlation with target (if numerical)\n",
    "        if df[target_column].dtype in [np.float64, np.int64]:\n",
    "            target_corr = df.select_dtypes(include=[np.number]).corr()[target_column].sort_values(ascending=False)\n",
    "            report['target_analysis']['correlations'] = target_corr.to_dict()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report for our dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING COMPREHENSIVE EDA REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "report = generate_eda_report(df, target_column='default')\n",
    "\n",
    "# Display report sections\n",
    "print(\"\\nðŸ“Š DATASET OVERVIEW:\")\n",
    "print(f\"   Rows: {report['overview']['rows']:,}\")\n",
    "print(f\"   Columns: {report['overview']['columns']}\")\n",
    "print(f\"   Memory: {report['overview']['memory_mb']:.2f} MB\")\n",
    "print(f\"   Duplicates: {report['overview']['duplicates']}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ COLUMN TYPES:\")\n",
    "print(f\"   Numerical: {len(report['column_types']['numerical'])}\")\n",
    "print(f\"   Categorical: {len(report['column_types']['categorical'])}\")\n",
    "\n",
    "print(\"\\nâŒ MISSING DATA:\")\n",
    "if report['missing_data']:\n",
    "    for col, info in report['missing_data'].items():\n",
    "        print(f\"   {col}: {info['count']} ({info['percentage']:.1f}%)\")\n",
    "else:\n",
    "    print(\"   âœ“ No missing data!\")\n",
    "\n",
    "print(\"\\nðŸ“Š NUMERICAL FEATURES SUMMARY:\")\n",
    "for col, stats in list(report['numerical_stats'].items())[:3]:  # Show first 3\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"      Mean: {stats['mean']:.2f}\")\n",
    "    print(f\"      Median: {stats['median']:.2f}\")\n",
    "    print(f\"      Std Dev: {stats['std']:.2f}\")\n",
    "    print(f\"      Skewness: {stats['skewness']:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ·ï¸  CATEGORICAL FEATURES SUMMARY:\")\n",
    "for col, stats in report['categorical_stats'].items():\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"      Unique values: {stats['unique']}\")\n",
    "    print(f\"      Most common: {stats['top_value']} ({stats['top_freq']} times)\")\n",
    "\n",
    "print(\"\\nâš ï¸  OUTLIERS DETECTED:\")\n",
    "for col, info in list(report['outliers'].items())[:5]:  # Show first 5\n",
    "    if info['count'] > 0:\n",
    "        print(f\"   {col}: {info['count']} ({info['percentage']:.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ”— HIGH CORRELATIONS (|r| > 0.7):\")\n",
    "if report.get('high_correlations'):\n",
    "    for corr in report['high_correlations']:\n",
    "        print(f\"   {corr['var1']} Ã— {corr['var2']}: {corr['correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"   âœ“ No highly correlated features!\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ TARGET VARIABLE ANALYSIS:\")\n",
    "if 'target_analysis' in report:\n",
    "    print(f\"   Name: {report['target_analysis']['name']}\")\n",
    "    print(f\"   Type: {report['target_analysis']['type']}\")\n",
    "    print(f\"   Distribution: {report['target_analysis']['distribution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… EDA REPORT COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc14d57",
   "metadata": {},
   "source": [
    "## Final Summary & Recommendations\n",
    "\n",
    "### **EDA Process Completed! ðŸŽ‰**\n",
    "\n",
    "You've now learned comprehensive EDA techniques covering:\n",
    "\n",
    "1. âœ… **Data Loading & Inspection** - Understanding dataset structure\n",
    "2. âœ… **Data Types & Structure** - Identifying numerical and categorical features\n",
    "3. âœ… **Missing Data Analysis** - Detecting and handling gaps\n",
    "4. âœ… **Descriptive Statistics** - Central tendency, dispersion, shape\n",
    "5. âœ… **Univariate Analysis** - Individual variable distributions\n",
    "6. âœ… **Bivariate Analysis** - Relationships between pairs\n",
    "7. âœ… **Multivariate Analysis** - Complex multi-variable patterns\n",
    "8. âœ… **Outlier Detection** - Identifying anomalies\n",
    "9. âœ… **Distribution Analysis** - Understanding data shapes\n",
    "10. âœ… **Correlation Analysis** - Feature relationships\n",
    "11. âœ… **Best Practices** - Professional EDA workflow\n",
    "\n",
    "### **Next Steps for Your Data Science Journey**\n",
    "\n",
    "#### **Immediate Actions**\n",
    "- Practice EDA on real datasets (Kaggle, UCI ML Repository)\n",
    "- Create EDA templates for different data types\n",
    "- Build a portfolio of EDA projects\n",
    "\n",
    "#### **Advanced EDA Topics to Explore**\n",
    "- **Time Series EDA**: Trend, seasonality, autocorrelation\n",
    "- **Text Data EDA**: Word frequency, n-grams, sentiment\n",
    "- **Image Data EDA**: Pixel distributions, image statistics\n",
    "- **Geospatial EDA**: Maps, spatial patterns\n",
    "- **Interactive EDA**: Plotly, Bokeh for dashboards\n",
    "\n",
    "#### **Tools to Master**\n",
    "- **pandas-profiling**: Automated EDA reports\n",
    "- **sweetviz**: Beautiful comparison reports\n",
    "- **dtale**: Interactive data exploration\n",
    "- **ydata-profiling**: Enhanced profiling\n",
    "- **Tableau/Power BI**: Business intelligence dashboards\n",
    "\n",
    "#### **Machine Learning Pipeline**\n",
    "After EDA, proceed to:\n",
    "1. **Feature Engineering** - Create new features based on insights\n",
    "2. **Data Preprocessing** - Scale, encode, transform\n",
    "3. **Model Selection** - Choose algorithms based on data characteristics\n",
    "4. **Model Training** - Fit models to data\n",
    "5. **Model Evaluation** - Assess performance\n",
    "6. **Deployment** - Put models into production\n",
    "\n",
    "### **Resources for Further Learning**\n",
    "\n",
    "#### **Books**\n",
    "- \"Python for Data Analysis\" by Wes McKinney\n",
    "- \"Storytelling with Data\" by Cole Nussbaumer Knaflic\n",
    "- \"The Art of Statistics\" by David Spiegelhalter\n",
    "\n",
    "#### **Online Courses**\n",
    "- Kaggle Learn: Data Visualization\n",
    "- DataCamp: Exploratory Data Analysis in Python\n",
    "- Coursera: Applied Data Science with Python\n",
    "\n",
    "#### **Practice Datasets**\n",
    "- **Kaggle Datasets**: Diverse, real-world data\n",
    "- **UCI ML Repository**: Classic ML datasets\n",
    "- **Data.gov**: Government open data\n",
    "- **Google Dataset Search**: Find any dataset\n",
    "\n",
    "### **Key Principles to Remember**\n",
    "\n",
    "1. **Always start with EDA** - Never skip this step\n",
    "2. **Visualize, visualize, visualize** - A picture is worth a thousand numbers\n",
    "3. **Question your data** - Don't assume, verify\n",
    "4. **Document everything** - Record assumptions and decisions\n",
    "5. **Iterate** - EDA is not linear, revisit as needed\n",
    "6. **Communicate insights** - Share findings clearly\n",
    "7. **Think critically** - Correlation â‰  Causation\n",
    "\n",
    "### **Final Thought**\n",
    "\n",
    "> \"Exploratory Data Analysis can never be the whole story, but nothing else can serve as the foundation stone.\"\n",
    "> â€” John Tukey (Pioneer of EDA)\n",
    "\n",
    "**You now have the skills to:**\n",
    "- ðŸ” Investigate any dataset systematically\n",
    "- ðŸ“Š Create insightful visualizations\n",
    "- ðŸ§® Perform statistical analysis\n",
    "- ðŸŽ¯ Identify patterns and anomalies\n",
    "- ðŸ’¡ Extract actionable insights\n",
    "- ðŸš€ Prepare data for machine learning\n",
    "\n",
    "**Happy Exploring! ðŸš€ðŸ“Š**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
