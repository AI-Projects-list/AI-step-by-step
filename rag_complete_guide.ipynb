{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb1bd2b",
   "metadata": {},
   "source": [
    "# 1. What is RAG?\n",
    "\n",
    "## ğŸ“– What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** is a technique that enhances Large Language Models (LLMs) by retrieving relevant information from external knowledge sources before generating responses. It combines retrieval systems with generative AI to provide accurate, up-to-date, and grounded answers.\n",
    "\n",
    "**Core Concept:**\n",
    "```\n",
    "Traditional LLM:\n",
    "User Question â†’ LLM â†’ Answer (based only on training data)\n",
    "Problem: Outdated info, hallucinations, no specific knowledge\n",
    "\n",
    "RAG System:\n",
    "User Question â†’ Retrieve Relevant Docs â†’ LLM + Context â†’ Grounded Answer\n",
    "Benefit: Current info, factual, source-backed answers\n",
    "```\n",
    "\n",
    "**RAG Architecture:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    1. USER QUERY                        â”‚\n",
    "â”‚        \"What are the company's vacation policies?\"      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              2. QUERY PROCESSING                        â”‚\n",
    "â”‚  - Query understanding                                  â”‚\n",
    "â”‚  - Query embedding (convert to vector)                  â”‚\n",
    "â”‚  - Query expansion/rewriting (optional)                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              3. RETRIEVAL                               â”‚\n",
    "â”‚  - Search knowledge base (vector database)              â”‚\n",
    "â”‚  - Find top-k most relevant documents                   â”‚\n",
    "â”‚  - Re-rank results (optional)                           â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  Knowledge Base:                                        â”‚\n",
    "â”‚  [Employee Handbook] [Policies] [FAQs] [Wiki]          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           4. CONTEXT PREPARATION                        â”‚\n",
    "â”‚  Retrieved Documents:                                   â”‚\n",
    "â”‚  - \"Section 5.2: Employees receive 15 days PTO...\"      â”‚\n",
    "â”‚  - \"Vacation Policy Update 2024: New hires get...\"      â”‚\n",
    "â”‚  - \"Holiday Schedule: Company observes 10 holidays...\"  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           5. AUGMENTED PROMPT                           â”‚\n",
    "â”‚  Context: [Retrieved documents]                         â”‚\n",
    "â”‚  Question: What are the company's vacation policies?    â”‚\n",
    "â”‚  Instructions: Answer based only on the context above.  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              6. LLM GENERATION                          â”‚\n",
    "â”‚  GPT-4, Claude, Llama, etc.                             â”‚\n",
    "â”‚  Generates answer using retrieved context               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              7. FINAL ANSWER                            â”‚\n",
    "â”‚  \"Based on our employee handbook, you receive 15 days   â”‚\n",
    "â”‚   of PTO per year. New hires accrue PTO monthly...      â”‚\n",
    "â”‚   [Source: Employee Handbook Section 5.2]\"              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Knowledge Base**\n",
    "   - Your documents (PDFs, docs, web pages, databases)\n",
    "   - Chunked into smaller pieces\n",
    "   - Stored as embeddings in vector database\n",
    "\n",
    "2. **Retriever**\n",
    "   - Finds relevant documents for user query\n",
    "   - Uses semantic search (embeddings)\n",
    "   - Returns top-k most relevant chunks\n",
    "\n",
    "3. **Generator**\n",
    "   - LLM (GPT-4, Claude, Llama, etc.)\n",
    "   - Receives query + retrieved context\n",
    "   - Generates answer grounded in context\n",
    "\n",
    "## ğŸ¯ Why Use RAG?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Up-to-date Information** - Access latest data without retraining\n",
    "2. **Reduce Hallucinations** - Answers grounded in real documents\n",
    "3. **Cost-Effective** - No expensive model fine-tuning\n",
    "4. **Domain-Specific Knowledge** - Add proprietary/specialized content\n",
    "5. **Source Attribution** - Cite sources for answers\n",
    "6. **Easy Updates** - Add/remove documents anytime\n",
    "7. **Privacy & Security** - Keep sensitive data in your infrastructure\n",
    "8. **Scalable** - Handle large knowledge bases efficiently\n",
    "\n",
    "### **Problems RAG Solves:**\n",
    "1. **Knowledge Cutoff** - LLMs trained on old data (e.g., GPT-4 cutoff 2023)\n",
    "2. **Hallucinations** - LLMs make up facts when unsure\n",
    "3. **Limited Context** - Can't fit all documents in prompt\n",
    "4. **Private Data** - Can't train models on confidential info\n",
    "5. **Dynamic Content** - Pricing, inventory, news changes daily\n",
    "6. **Specialized Domains** - Medical, legal, technical knowledge\n",
    "\n",
    "## â±ï¸ When to Use RAG\n",
    "\n",
    "### âœ… **Use RAG When:**\n",
    "\n",
    "**1. Question Answering Over Documents**\n",
    "- Example: Customer support chatbot for product documentation\n",
    "- Why: Retrieve exact product specs before answering\n",
    "- Benefit: Accurate, source-backed answers\n",
    "\n",
    "**2. Enterprise Knowledge Management**\n",
    "- Example: Employee assistant for company policies\n",
    "- Why: Access internal wikis, handbooks, policies\n",
    "- Benefit: Centralized knowledge, consistent answers\n",
    "\n",
    "**3. Research & Analysis**\n",
    "- Example: Legal research assistant\n",
    "- Why: Search through case law, statutes, contracts\n",
    "- Benefit: Fast, comprehensive research\n",
    "\n",
    "**4. Customer Support**\n",
    "- Example: Technical support chatbot\n",
    "- Why: Retrieve troubleshooting guides, FAQs\n",
    "- Benefit: 24/7 support, reduced ticket volume\n",
    "\n",
    "**5. Content Generation with Facts**\n",
    "- Example: Blog post writer with research\n",
    "- Why: Include real statistics, quotes, data\n",
    "- Benefit: Factual, well-researched content\n",
    "\n",
    "**6. Medical/Healthcare Applications**\n",
    "- Example: Medical information assistant\n",
    "- Why: Access medical literature, guidelines\n",
    "- Benefit: Evidence-based answers\n",
    "\n",
    "**7. E-commerce Product Search**\n",
    "- Example: \"Find waterproof hiking boots under $100\"\n",
    "- Why: Semantic search through product catalog\n",
    "- Benefit: Better search than keyword matching\n",
    "\n",
    "**8. Code Documentation Assistant**\n",
    "- Example: Help developers understand codebase\n",
    "- Why: Search code, docs, commit history\n",
    "- Benefit: Faster onboarding, fewer questions\n",
    "\n",
    "**9. News & Current Events**\n",
    "- Example: News summarization with latest articles\n",
    "- Why: LLMs don't know today's news\n",
    "- Benefit: Up-to-date information\n",
    "\n",
    "**10. Personal Knowledge Base**\n",
    "- Example: \"Second brain\" for notes, articles, ideas\n",
    "- Why: Search your personal knowledge\n",
    "- Benefit: Never forget what you learned\n",
    "\n",
    "### âŒ **Don't Use RAG When:**\n",
    "\n",
    "**1. General Knowledge Questions**\n",
    "- Problem: \"What is the capital of France?\"\n",
    "- Better: Direct LLM call\n",
    "- Why: LLM already knows this, no retrieval needed\n",
    "\n",
    "**2. Creative Writing Without Facts**\n",
    "- Problem: \"Write a fantasy story\"\n",
    "- Better: Direct LLM generation\n",
    "- Why: No external knowledge needed\n",
    "\n",
    "**3. Small Document Set (<10 pages)**\n",
    "- Problem: 5-page document that fits in context\n",
    "- Better: Include entire document in prompt\n",
    "- Why: Overhead of RAG not worth it\n",
    "\n",
    "**4. Real-Time API Data**\n",
    "- Problem: \"What's the current stock price?\"\n",
    "- Better: Direct API call + LLM\n",
    "- Why: Need live data, not indexed documents\n",
    "\n",
    "**5. Structured Data Queries**\n",
    "- Problem: \"Show sales by region from database\"\n",
    "- Better: SQL query + LLM for formatting\n",
    "- Why: Databases are better for structured queries\n",
    "\n",
    "**6. Low Latency Requirements (<100ms)**\n",
    "- Problem: Need instant responses\n",
    "- Better: Cached responses or smaller models\n",
    "- Why: RAG adds retrieval + generation latency\n",
    "\n",
    "**7. Highly Sensitive Data (No Cloud)**\n",
    "- Problem: Data can't leave secure environment\n",
    "- Better: On-premise LLM + local vector DB\n",
    "- Why: Can still use RAG, but need local setup\n",
    "\n",
    "## ğŸ“Š How RAG Works (Step-by-Step)\n",
    "\n",
    "**Indexing Phase (One-Time Setup):**\n",
    "```\n",
    "1. Load Documents\n",
    "   - PDFs, Word docs, web pages, databases\n",
    "   \n",
    "2. Split into Chunks\n",
    "   - Break documents into 500-1000 token chunks\n",
    "   - With overlap (e.g., 50-200 tokens)\n",
    "   \n",
    "3. Generate Embeddings\n",
    "   - Convert chunks to vectors (e.g., 1536 dimensions)\n",
    "   - Using OpenAI, Sentence Transformers, etc.\n",
    "   \n",
    "4. Store in Vector Database\n",
    "   - Pinecone, Weaviate, Chroma, FAISS, etc.\n",
    "   - Enable fast similarity search\n",
    "```\n",
    "\n",
    "**Query Phase (Every Request):**\n",
    "```\n",
    "1. User asks question\n",
    "   Input: \"What are the return policies?\"\n",
    "   \n",
    "2. Embed query\n",
    "   Convert question to vector (same model as chunks)\n",
    "   \n",
    "3. Retrieve top-k chunks\n",
    "   Search vector DB for most similar chunks\n",
    "   Typically k=3-10\n",
    "   \n",
    "4. Re-rank (optional)\n",
    "   Improve relevance with cross-encoder\n",
    "   \n",
    "5. Build augmented prompt\n",
    "   Prompt = Context + Question + Instructions\n",
    "   \n",
    "6. Send to LLM\n",
    "   GPT-4, Claude, Llama, etc.\n",
    "   \n",
    "7. Return answer\n",
    "   With optional source citations\n",
    "```\n",
    "\n",
    "## ğŸŒ Real-World Applications\n",
    "\n",
    "1. **ChatGPT with Browsing** - Retrieves web pages for current info\n",
    "2. **Perplexity.ai** - Search engine with RAG for cited answers\n",
    "3. **GitHub Copilot Chat** - Code search + generation\n",
    "4. **Notion AI** - Search workspace before answering\n",
    "5. **Intercom/Zendesk AI** - Customer support with knowledge base\n",
    "6. **Legal research tools** - Casetext, Ross Intelligence\n",
    "7. **Healthcare AI** - UpToDate, IBM Watson Health\n",
    "8. **Microsoft Copilot** - Office 365 documents search\n",
    "9. **Glean** - Enterprise search with AI\n",
    "10. **You.com** - AI search with sources\n",
    "\n",
    "## ğŸ’¡ Key Insights\n",
    "\n",
    "âœ… **RAG = Retrieval + Generation** - Best of both worlds  \n",
    "âœ… **Reduces hallucinations** - Answers grounded in documents  \n",
    "âœ… **No fine-tuning needed** - Add knowledge without retraining  \n",
    "âœ… **Dynamic knowledge** - Update documents anytime  \n",
    "âœ… **Source attribution** - Know where answers come from  \n",
    "âœ… **Cost-effective** - Cheaper than fine-tuning large models  \n",
    "âœ… **Chunk size matters** - 500-1000 tokens is typical  \n",
    "âœ… **Retrieval quality is critical** - Bad retrieval = bad answers  \n",
    "âœ… **Hybrid retrieval often best** - Dense + sparse (BM25)  \n",
    "âœ… **Evaluation is key** - Measure retrieval and generation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad522955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG (RETRIEVAL-AUGMENTED GENERATION) - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RAG (RETRIEVAL-AUGMENTED GENERATION) - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# 1. DOCUMENT COLLECTION (Knowledge Base)\n",
    "print(\"\\n1. DOCUMENT COLLECTION (Knowledge Base)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simulate a company knowledge base\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Vacation Policy\",\n",
    "        \"content\": \"All full-time employees receive 15 days of paid time off (PTO) per year. PTO accrues monthly at a rate of 1.25 days per month. New hires begin accruing PTO from their start date. Unused PTO can be carried over up to 5 days into the next year.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Health Insurance\",\n",
    "        \"content\": \"The company offers comprehensive health insurance coverage including medical, dental, and vision. Employees contribute 20% of the premium cost. Coverage begins on the first day of the month following 30 days of employment. Dependents can be added to the plan.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Remote Work Policy\",\n",
    "        \"content\": \"Employees may work remotely up to 3 days per week with manager approval. Remote work equipment including laptop and monitor will be provided. Employees must maintain a dedicated workspace and be available during core hours 10am-3pm EST.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"title\": \"Holiday Schedule\",\n",
    "        \"content\": \"The company observes 10 paid holidays per year including New Year's Day, Memorial Day, Independence Day, Labor Day, Thanksgiving (2 days), and Christmas (December 24-26). Holidays falling on weekends are observed on the nearest weekday.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"title\": \"401(k) Plan\",\n",
    "        \"content\": \"Employees are eligible to participate in the 401(k) plan after 90 days of employment. The company matches 50% of employee contributions up to 6% of salary. Vesting is immediate for employee contributions and follows a 4-year schedule for employer matches.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Knowledge Base: {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc['title']}\")\n",
    "\n",
    "# 2. TEXT CHUNKING (Split documents into smaller pieces)\n",
    "print(\"\\n2. TEXT CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a text chunk with metadata\"\"\"\n",
    "    id: str\n",
    "    doc_id: str\n",
    "    doc_title: str\n",
    "    text: str\n",
    "    embedding: np.ndarray = None\n",
    "\n",
    "def chunk_documents(documents: List[Dict], chunk_size: int = 200) -> List[Chunk]:\n",
    "    \"\"\"Split documents into chunks (simplified - by sentences)\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Split by sentences (simple version)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', doc['content'])\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        chunk_idx = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence.split())\n",
    "            \n",
    "            if current_length + sentence_length > chunk_size and current_chunk:\n",
    "                # Create chunk\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunks.append(Chunk(\n",
    "                    id=f\"{doc['id']}_chunk{chunk_idx}\",\n",
    "                    doc_id=doc['id'],\n",
    "                    doc_title=doc['title'],\n",
    "                    text=chunk_text\n",
    "                ))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "                chunk_idx += 1\n",
    "            \n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        \n",
    "        # Add remaining chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append(Chunk(\n",
    "                id=f\"{doc['id']}_chunk{chunk_idx}\",\n",
    "                doc_id=doc['id'],\n",
    "                doc_title=doc['title'],\n",
    "                text=chunk_text\n",
    "            ))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_documents(documents)\n",
    "print(f\"\\nCreated {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(\"\\nExample chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n  Chunk {i+1} ({chunk.doc_title}):\")\n",
    "    print(f\"    {chunk.text[:100]}...\")\n",
    "\n",
    "# 3. EMBEDDING GENERATION (Convert text to vectors)\n",
    "print(\"\\n3. EMBEDDING GENERATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def simple_embedding(text: str, dim: int = 50) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simplified embedding function for demonstration.\n",
    "    In production, use OpenAI embeddings, Sentence Transformers, etc.\n",
    "    \"\"\"\n",
    "    # Simple word-based embedding (for demo only)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Create a deterministic but simple embedding\n",
    "    np.random.seed(hash(text) % (2**32))\n",
    "    embedding = np.random.randn(dim)\n",
    "    \n",
    "    # Add some semantic meaning based on keywords\n",
    "    keywords = {\n",
    "        'vacation': 0, 'pto': 0, 'time off': 0,\n",
    "        'health': 10, 'insurance': 10, 'medical': 10,\n",
    "        'remote': 20, 'work from home': 20,\n",
    "        'holiday': 30,\n",
    "        '401k': 40, 'retirement': 40\n",
    "    }\n",
    "    \n",
    "    for keyword, idx in keywords.items():\n",
    "        if keyword in text.lower() and idx < dim:\n",
    "            embedding[idx] += 2.0\n",
    "    \n",
    "    # Normalize\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"\\nGenerating embeddings for all chunks...\")\n",
    "for chunk in chunks:\n",
    "    chunk.embedding = simple_embedding(chunk.text)\n",
    "\n",
    "print(f\"Generated {len(chunks)} embeddings (dimension: {chunks[0].embedding.shape[0]})\")\n",
    "print(f\"\\nExample embedding (first 10 values):\")\n",
    "print(f\"  {chunks[0].embedding[:10]}\")\n",
    "\n",
    "# 4. VECTOR DATABASE (Simple in-memory implementation)\n",
    "print(\"\\n4. VECTOR DATABASE (In-Memory)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "class SimpleVectorDB:\n",
    "    \"\"\"Simple in-memory vector database for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunks: List[Chunk] = []\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Chunk]):\n",
    "        \"\"\"Add chunks to the database\"\"\"\n",
    "        self.chunks.extend(chunks)\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"Search for most similar chunks using cosine similarity\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for chunk in self.chunks:\n",
    "            similarity = cosine_similarity(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                chunk.embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarities.append((chunk, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "\n",
    "# Create vector database and index chunks\n",
    "vector_db = SimpleVectorDB()\n",
    "vector_db.add_chunks(chunks)\n",
    "\n",
    "print(f\"Vector DB initialized with {len(vector_db.chunks)} chunks\")\n",
    "\n",
    "# 5. RETRIEVAL (Search for relevant chunks)\n",
    "print(\"\\n5. RETRIEVAL - SEMANTIC SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def retrieve(query: str, vector_db: SimpleVectorDB, top_k: int = 3) -> List[Tuple[Chunk, float]]:\n",
    "    \"\"\"Retrieve most relevant chunks for a query\"\"\"\n",
    "    # Embed query\n",
    "    query_embedding = simple_embedding(query)\n",
    "    \n",
    "    # Search vector database\n",
    "    results = vector_db.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How many vacation days do I get?\",\n",
    "    \"Tell me about health insurance\",\n",
    "    \"Can I work from home?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = retrieve(query, vector_db, top_k=3)\n",
    "    \n",
    "    print(\"\\nTop 3 Retrieved Chunks:\")\n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        print(f\"\\n  {i}. [{score:.3f}] {chunk.doc_title}\")\n",
    "        print(f\"     {chunk.text[:100]}...\")\n",
    "\n",
    "# 6. GENERATION (Augmented Prompt)\n",
    "print(\"\\n6. GENERATION - AUGMENTED PROMPT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def generate_answer(query: str, retrieved_chunks: List[Tuple[Chunk, float]]) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using retrieved context.\n",
    "    In production, this would call GPT-4, Claude, etc.\n",
    "    \"\"\"\n",
    "    # Build context from retrieved chunks\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {chunk.doc_title}\\n{chunk.text}\"\n",
    "        for chunk, score in retrieved_chunks\n",
    "    ])\n",
    "    \n",
    "    # Build augmented prompt\n",
    "    prompt = f\"\"\"You are a helpful assistant answering questions about company policies.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the context provided above\n",
    "- If the answer is not in the context, say \"I don't have that information\"\n",
    "- Cite the source document in your answer\n",
    "- Be concise and accurate\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Simulate LLM response (in production, call actual LLM)\n",
    "    # For demo, we'll create a simple rule-based response\n",
    "    if \"vacation\" in query.lower() or \"pto\" in query.lower():\n",
    "        answer = \"Based on the Vacation Policy, all full-time employees receive 15 days of paid time off (PTO) per year. PTO accrues monthly at 1.25 days per month, and you begin accruing from your start date. You can carry over up to 5 unused days into the next year.\"\n",
    "    elif \"health\" in query.lower() or \"insurance\" in query.lower():\n",
    "        answer = \"According to our Health Insurance policy, the company offers comprehensive coverage including medical, dental, and vision. You contribute 20% of the premium cost, and coverage begins on the first day of the month following 30 days of employment. Dependents can also be added.\"\n",
    "    elif \"remote\" in query.lower() or \"work from home\" in query.lower():\n",
    "        answer = \"Per the Remote Work Policy, you may work remotely up to 3 days per week with manager approval. The company provides equipment (laptop and monitor), and you must maintain a dedicated workspace and be available during core hours 10am-3pm EST.\"\n",
    "    else:\n",
    "        answer = \"Based on the retrieved documents, I can provide information about that topic. Please refer to the context above for details.\"\n",
    "    \n",
    "    return prompt, answer\n",
    "\n",
    "# Test RAG pipeline\n",
    "print(\"\\nFull RAG Pipeline Example:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query = \"How many vacation days do I get per year?\"\n",
    "print(f\"\\nUser Query: '{query}'\")\n",
    "\n",
    "# Retrieve\n",
    "print(\"\\n[Step 1] Retrieving relevant documents...\")\n",
    "retrieved = retrieve(query, vector_db, top_k=2)\n",
    "print(f\"Retrieved {len(retrieved)} documents\")\n",
    "\n",
    "for i, (chunk, score) in enumerate(retrieved, 1):\n",
    "    print(f\"  {i}. {chunk.doc_title} (relevance: {score:.3f})\")\n",
    "\n",
    "# Generate\n",
    "print(\"\\n[Step 2] Generating answer with context...\")\n",
    "prompt, answer = generate_answer(query, retrieved)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "print(answer)\n",
    "\n",
    "# 7. RAG COMPARISON\n",
    "print(\"\\n\\n7. RAG vs NO RAG COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nScenario: User asks 'How many vacation days do I get?'\")\n",
    "\n",
    "print(\"\\nWithout RAG (LLM only):\")\n",
    "print(\"  - LLM might guess: '10-20 days depending on company'\")\n",
    "print(\"  - Generic answer, not specific to your company\")\n",
    "print(\"  - No source attribution\")\n",
    "print(\"  - Could be wrong or outdated\")\n",
    "\n",
    "print(\"\\nWith RAG:\")\n",
    "print(\"  - Retrieves actual policy document\")\n",
    "print(\"  - Answer: 'You receive 15 days of PTO per year'\")\n",
    "print(\"  - Specific, accurate, current\")\n",
    "print(\"  - Cites source: 'Vacation Policy'\")\n",
    "print(\"  - Grounded in real company data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ RAG = Retrieval + Augmented + Generation\")\n",
    "print(\"âœ“ Step 1: Load & chunk documents â†’ Create knowledge base\")\n",
    "print(\"âœ“ Step 2: Generate embeddings â†’ Convert text to vectors\")\n",
    "print(\"âœ“ Step 3: Store in vector DB â†’ Enable fast search\")\n",
    "print(\"âœ“ Step 4: Retrieve relevant chunks â†’ Semantic search\")\n",
    "print(\"âœ“ Step 5: Build augmented prompt â†’ Context + Question\")\n",
    "print(\"âœ“ Step 6: LLM generates answer â†’ Grounded in context\")\n",
    "print(\"âœ“ Benefits: Accurate, up-to-date, source-backed answers\")\n",
    "print(\"âœ“ Use cases: Q&A, customer support, research, enterprise knowledge\")\n",
    "print(\"âœ“ Key success factors: Good chunking, quality embeddings, proper retrieval\")\n",
    "print(\"âœ“ Production tools: LangChain, LlamaIndex, OpenAI, Pinecone, Weaviate\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169846a0",
   "metadata": {},
   "source": [
    "# 2. RAG Architecture Overview\n",
    "\n",
    "## ğŸ“– What is RAG Architecture?\n",
    "\n",
    "**RAG Architecture** defines the complete system design for implementing Retrieval-Augmented Generation, including all components, their interactions, and data flow.\n",
    "\n",
    "**Complete RAG System Architecture:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DATA INGESTION LAYER                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Document Sources:                                           â”‚\n",
    "â”‚  â€¢ PDFs, Word, HTML, Markdown                                â”‚\n",
    "â”‚  â€¢ Databases (SQL, NoSQL)                                    â”‚\n",
    "â”‚  â€¢ APIs (REST, GraphQL)                                      â”‚\n",
    "â”‚  â€¢ Web scraping                                              â”‚\n",
    "â”‚  â€¢ File systems                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 PROCESSING LAYER                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Document Parsing                                         â”‚\n",
    "â”‚     - Extract text, tables, images                           â”‚\n",
    "â”‚     - OCR for scanned documents                              â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  2. Text Chunking                                            â”‚\n",
    "â”‚     - Split into manageable pieces (500-1000 tokens)         â”‚\n",
    "â”‚     - Strategies: fixed, recursive, semantic                 â”‚\n",
    "â”‚     - Overlap for context preservation                       â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  3. Metadata Extraction                                      â”‚\n",
    "â”‚     - Source, author, date, section                          â”‚\n",
    "â”‚     - Tags, categories                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  EMBEDDING LAYER                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Embedding Models:                                           â”‚\n",
    "â”‚  â€¢ OpenAI (text-embedding-3-small/large)                     â”‚\n",
    "â”‚  â€¢ Sentence Transformers (all-MiniLM-L6-v2)                  â”‚\n",
    "â”‚  â€¢ Cohere, Voyage AI                                         â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Convert: Text â†’ Dense Vectors (384-1536 dimensions)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   STORAGE LAYER                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Vector Databases:                                           â”‚\n",
    "â”‚  â€¢ Pinecone (cloud-native)                                   â”‚\n",
    "â”‚  â€¢ Weaviate (open-source)                                    â”‚\n",
    "â”‚  â€¢ Qdrant (high-performance)                                 â”‚\n",
    "â”‚  â€¢ Chroma (embedded)                                         â”‚\n",
    "â”‚  â€¢ FAISS (in-memory)                                         â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Capabilities:                                               â”‚\n",
    "â”‚  â€¢ Fast similarity search (ANN)                              â”‚\n",
    "â”‚  â€¢ Metadata filtering                                        â”‚\n",
    "â”‚  â€¢ Scalability                                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   RETRIEVAL LAYER                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Query Processing                                         â”‚\n",
    "â”‚     - Query understanding                                    â”‚\n",
    "â”‚     - Query expansion/rewriting                              â”‚\n",
    "â”‚     - Query embedding                                        â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  2. Search Strategies                                        â”‚\n",
    "â”‚     - Dense (semantic)                                       â”‚\n",
    "â”‚     - Sparse (keyword/BM25)                                  â”‚\n",
    "â”‚     - Hybrid (dense + sparse)                                â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  3. Re-ranking                                               â”‚\n",
    "â”‚     - Cross-encoder models                                   â”‚\n",
    "â”‚     - LLM-based re-ranking                                   â”‚\n",
    "â”‚     - Diversity algorithms (MMR)                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  GENERATION LAYER                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  LLMs:                                                       â”‚\n",
    "â”‚  â€¢ GPT-4, GPT-3.5-turbo (OpenAI)                             â”‚\n",
    "â”‚  â€¢ Claude 3 (Anthropic)                                      â”‚\n",
    "â”‚  â€¢ Llama 2/3 (Meta)                                          â”‚\n",
    "â”‚  â€¢ Mistral (Mistral AI)                                      â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Prompt Engineering:                                         â”‚\n",
    "â”‚  â€¢ System prompt + Context + Question                        â”‚\n",
    "â”‚  â€¢ Instructions for grounded answers                         â”‚\n",
    "â”‚  â€¢ Citation requirements                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   OUTPUT LAYER                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â€¢ Generated answer                                          â”‚\n",
    "â”‚  â€¢ Source citations                                          â”‚\n",
    "â”‚  â€¢ Confidence scores                                         â”‚\n",
    "â”‚  â€¢ Follow-up questions                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  CROSS-CUTTING CONCERNS                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â€¢ Caching (query results, embeddings)                       â”‚\n",
    "â”‚  â€¢ Monitoring & Logging                                      â”‚\n",
    "â”‚  â€¢ Evaluation (retrieval & generation quality)               â”‚\n",
    "â”‚  â€¢ Security & Access Control                                 â”‚\n",
    "â”‚  â€¢ Cost Management                                           â”‚\n",
    "â”‚  â€¢ Latency Optimization                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸ¯ Why This Architecture?\n",
    "\n",
    "### **Design Principles:**\n",
    "1. **Separation of Concerns** - Each layer has specific responsibility\n",
    "2. **Modularity** - Swap components (embedding model, vector DB, LLM)\n",
    "3. **Scalability** - Handle growing document collections\n",
    "4. **Flexibility** - Adapt to different use cases\n",
    "5. **Observability** - Monitor each stage\n",
    "6. **Cost Efficiency** - Optimize expensive operations\n",
    "\n",
    "## ğŸ’¡ Key Insights\n",
    "\n",
    "âœ… **Modular design** - Each component is replaceable  \n",
    "âœ… **Two-phase architecture** - Indexing (offline) + Query (online)  \n",
    "âœ… **Chunking is critical** - Affects retrieval quality  \n",
    "âœ… **Vector DB choice matters** - Based on scale, cost, features  \n",
    "âœ… **Hybrid retrieval** - Often better than dense-only  \n",
    "âœ… **Re-ranking improves accuracy** - But adds latency  \n",
    "âœ… **Caching is essential** - Reduces cost and latency  \n",
    "âœ… **Evaluation metrics** - Measure retrieval recall and answer quality  \n",
    "âœ… **Incremental updates** - Add/remove documents without re-indexing all  \n",
    "âœ… **Framework options** - LangChain, LlamaIndex, Haystack for rapid development"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
