{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db260de",
   "metadata": {},
   "source": [
    "# Data Cleaning Learning Guide\n",
    "\n",
    "## What is Data Cleaning?\n",
    "\n",
    "**Data Cleaning** (also called Data Cleansing or Data Scrubbing) is the process of detecting and correcting (or removing) corrupt, inaccurate, or irrelevant records from a dataset. It's one of the most critical and time-consuming steps in data analysis, often taking 50-80% of a data scientist's time.\n",
    "\n",
    "## Why is Data Cleaning Important?\n",
    "\n",
    "1. **Improve Data Quality** - Ensure accuracy, completeness, and reliability\n",
    "2. **Prevent Errors** - Avoid incorrect analysis and flawed conclusions\n",
    "3. **Enable Accurate Insights** - Make better decisions with trustworthy data\n",
    "4. **Boost Model Performance** - Clean data leads to better machine learning results\n",
    "5. **Save Time Later** - Fix issues early to avoid downstream problems\n",
    "6. **Meet Standards** - Comply with data quality requirements and regulations\n",
    "7. **Build Trust** - Stakeholders trust results from clean, validated data\n",
    "\n",
    "## What You'll Learn in This Notebook\n",
    "\n",
    "This comprehensive guide covers **15 essential data cleaning topics**:\n",
    "\n",
    "1. [Creating Sample \"Dirty\" Data](#1-creating-sample-dirty-data) - Understanding common data quality issues\n",
    "2. [Handling Missing Values](#2-handling-missing-values) - Detecting and filling gaps\n",
    "3. [Removing Duplicates](#3-removing-duplicates) - Identifying and eliminating redundancy\n",
    "4. [Data Type Conversion](#4-data-type-conversion) - Ensuring correct data types\n",
    "5. [String Cleaning & Standardization](#5-string-cleaning--standardization) - Text data normalization\n",
    "6. [Handling Outliers & Invalid Values](#6-handling-outliers--invalid-values) - Managing extreme values\n",
    "7. [Data Validation](#7-data-validation) - Implementing quality checks\n",
    "8. [Dealing with Inconsistent Data](#8-dealing-with-inconsistent-data) - Standardizing formats\n",
    "9. [Date & Time Data Cleaning](#9-date--time-data-cleaning) - Standardizing temporal data\n",
    "10. [Categorical Data Cleaning](#10-categorical-data-cleaning) - Handling categories and labels\n",
    "11. [Numerical Data Cleaning](#11-numerical-data-cleaning) - Cleaning numeric fields\n",
    "12. [Text Data Cleaning](#12-text-data-cleaning) - Advanced text processing\n",
    "13. [Standardization & Normalization](#13-standardization--normalization) - Scaling and transforming\n",
    "14. [Advanced Data Cleaning Techniques](#14-advanced-data-cleaning-techniques) - Professional methods\n",
    "15. [Data Cleaning Pipeline & Best Practices](#15-data-cleaning-pipeline--best-practices) - Professional workflows\n",
    "\n",
    "## Tools & Libraries Used\n",
    "\n",
    "- **pandas** - Core data manipulation and cleaning\n",
    "- **numpy** - Numerical operations and array handling\n",
    "- **re** - Regular expressions for text cleaning\n",
    "- **datetime** - Date and time parsing and validation\n",
    "- **matplotlib & seaborn** - Visualization for quality checks\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Follow sequentially** - Each section builds on previous concepts\n",
    "2. **Run all examples** - See real cleaning transformations in action\n",
    "3. **Apply to your data** - Adapt techniques to your own datasets\n",
    "4. **Document issues** - Keep track of data quality problems found\n",
    "5. **Create checklists** - Build your own data cleaning workflow\n",
    "\n",
    "## Common Data Quality Issues\n",
    "\n",
    "This notebook demonstrates how to handle:\n",
    "\n",
    "- âŒ **Missing values** - NaN, None, empty strings, placeholders\n",
    "- âŒ **Duplicates** - Exact and fuzzy duplicate records\n",
    "- âŒ **Inconsistent formatting** - Mixed case, spacing, date formats\n",
    "- âŒ **Invalid values** - Out of range, wrong type, impossible values\n",
    "- âŒ **Typos and errors** - Misspellings, transcription errors\n",
    "- âŒ **Outliers** - Extreme or anomalous values\n",
    "- âŒ **Encoding issues** - Special characters, UTF-8 problems\n",
    "- âŒ **Logical errors** - Violations of business rules\n",
    "\n",
    "## The Data Cleaning Workflow\n",
    "\n",
    "```\n",
    "Raw Data â†’ Inspect â†’ Detect Issues â†’ Clean â†’ Validate â†’ Document â†’ Clean Data\n",
    "```\n",
    "\n",
    "1. **Inspect**: Load and examine the data\n",
    "2. **Detect**: Identify quality issues systematically\n",
    "3. **Clean**: Apply appropriate cleaning techniques\n",
    "4. **Validate**: Verify cleaning worked correctly\n",
    "5. **Document**: Record all cleaning steps taken\n",
    "6. **Export**: Save cleaned data for analysis\n",
    "\n",
    "## Data Quality Dimensions\n",
    "\n",
    "Good data should be:\n",
    "\n",
    "- âœ… **Accurate** - Values reflect reality\n",
    "- âœ… **Complete** - All required data is present\n",
    "- âœ… **Consistent** - No contradictions within data\n",
    "- âœ… **Timely** - Data is current and up-to-date\n",
    "- âœ… **Valid** - Values conform to defined formats\n",
    "- âœ… **Unique** - No unwanted duplicates\n",
    "\n",
    "---\n",
    "\n",
    "Let's clean some data! ðŸ§¹âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP: Import Libraries for Data Cleaning\n",
    "# ============================================\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization (to see before/after)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Regular expressions for text cleaning\n",
    "import re\n",
    "\n",
    "# Date/time handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Warning suppression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fc636",
   "metadata": {},
   "source": [
    "## 1. Creating Sample \"Dirty\" Data\n",
    "\n",
    "**Why**: We need realistic messy data to practice cleaning techniques.\n",
    "\n",
    "**What makes data \"dirty\"**:\n",
    "- Missing values (NaN, None, empty strings)\n",
    "- Duplicates (exact and fuzzy)\n",
    "- Inconsistent formatting (case, spacing)\n",
    "- Invalid values (out of range, wrong type)\n",
    "- Typos and misspellings\n",
    "- Extra whitespace\n",
    "- Mixed data types in same column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef03ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. CREATE SAMPLE \"DIRTY\" DATA\n",
    "# ============================================\n",
    "\n",
    "# Create a messy dataset that mimics real-world data quality issues\n",
    "dirty_data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    'name': ['John Doe', 'jane smith', 'ALICE JOHNSON', '  Bob Brown  ', 'Charlie Davis',\n",
    "             'Charlie Davis', 'Emma Wilson', None, 'Frank Miller', 'Grace Lee', \n",
    "             'Henry Taylor', 'Ivy Anderson', 'Jack Thomas', 'Kate Jackson', 'Leo White', 'Mia Harris'],\n",
    "    'email': ['john@email.com', 'JANE@EMAIL.COM', 'alice@email.com', 'bob@email..com', \n",
    "              'charlie@email.com', 'charlie@email.com', '  emma@email.com  ', 'invalid_email',\n",
    "              'frank@email.com', np.nan, 'henry@email.com', 'ivy@email.com', \n",
    "              'jack@email.com', 'kate@email.com', '', 'mia@email.com'],\n",
    "    'age': [25, 30, '35', 28, 150, 150, 45, 22, -5, 40, 55, np.nan, 33, 29, 38, 42],\n",
    "    'salary': [50000, 60000, 70000, np.nan, 80000, 80000, 90000, 55000, \n",
    "               65000, 75000, 85000, 95000, '$105,000', '115000', 125000, np.nan],\n",
    "    'city': ['New York', 'new york', 'NEW YORK', 'Los Angeles', 'Chicago',\n",
    "             'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio',\n",
    "             'San Diego', None, 'Dallas', 'San Jose', 'Austin', 'Jacksonville'],\n",
    "    'country': ['USA', 'USA', 'usa', 'USA', 'US', 'US', 'United States', \n",
    "                'USA', 'USA', 'USA', 'USA', 'USA', 'U.S.A', 'USA', 'USA', np.nan],\n",
    "    'phone': ['123-456-7890', '(234) 567-8901', '3456789012', '456.789.0123',\n",
    "              '567-890-1234', '567-890-1234', '678 901 2345', None, '890-123-4567',\n",
    "              '901-234-5678', '012-345-6789', '123-456-7890', '234-567-8901',\n",
    "              '345-678-9012', '456-789-0123', '567-890-1234'],\n",
    "    'join_date': ['2020-01-15', '2020/02/20', '15-03-2020', '2020.04.10', \n",
    "                  '2020-05-05', '2020-05-05', '2020-06-12', '2020-07-18',\n",
    "                  'Invalid Date', '2020-09-25', '2020-10-30', np.nan,\n",
    "                  '2020-12-05', '2021-01-10', '2021-02-14', '2021-03-20'],\n",
    "    'department': ['Sales', 'sales', 'SALES', 'Marketing', 'IT', 'IT', \n",
    "                   'HR', 'Finance', 'Sales', 'Marketing', 'IT', 'HR',\n",
    "                   'Finance', '  Sales  ', 'Marketing', 'IT'],\n",
    "    'status': ['Active', 'active', 'ACTIVE', 'Inactive', 'Active', 'Active',\n",
    "               'Inactive', 'Active', '', 'Active', 'Inactive', 'Active',\n",
    "               'Active', 'Inactive', 'Active', np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dirty_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DIRTY DATA CREATED - INITIAL INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nâš ï¸  Data Quality Issues Present:\")\n",
    "print(\"   âœ— Missing values (NaN, None, empty strings)\")\n",
    "print(\"   âœ— Duplicate rows\")\n",
    "print(\"   âœ— Inconsistent case (lowercase, UPPERCASE, Mixed)\")\n",
    "print(\"   âœ— Extra whitespace\")\n",
    "print(\"   âœ— Invalid values (negative age, age > 120)\")\n",
    "print(\"   âœ— Inconsistent formats (dates, phone numbers, country names)\")\n",
    "print(\"   âœ— Mixed data types (age has string '35')\")\n",
    "print(\"   âœ— Special characters in numeric fields (salary: '$105,000')\")\n",
    "print(\"   âœ— Invalid email formats\")\n",
    "print(\"   âœ— Inconsistent categorical values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367415e",
   "metadata": {},
   "source": [
    "## 2. Handling Missing Values\n",
    "\n",
    "**Why**: Missing data can bias results, reduce statistical power, and cause errors in analysis.\n",
    "\n",
    "**When to use**:\n",
    "- When data has NaN, None, or empty values\n",
    "- Before any statistical analysis or modeling\n",
    "- When merging datasets with incomplete records\n",
    "\n",
    "**Strategies**:\n",
    "1. **Identify** - Detect all types of missing data\n",
    "2. **Analyze** - Understand patterns and causes\n",
    "3. **Decide** - Choose appropriate handling method:\n",
    "   - **Delete**: Remove rows/columns (if < 5% missing)\n",
    "   - **Impute**: Fill with mean, median, mode, or predicted values\n",
    "   - **Flag**: Create indicator for missingness\n",
    "   - **Keep**: Sometimes missingness is informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. HANDLING MISSING VALUES\n",
    "# ============================================\n",
    "\n",
    "df_missing = df.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS & HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Identify missing values (multiple types)\n",
    "print(\"\\n1. Detecting Missing Values:\")\n",
    "print(\"\\n   Standard NaN/None:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "print(\"\\n   Empty strings:\")\n",
    "empty_strings = (df_missing.select_dtypes(include=['object']) == '').sum()\n",
    "print(empty_strings[empty_strings > 0])\n",
    "\n",
    "print(\"\\n   Whitespace-only strings:\")\n",
    "def count_whitespace(series):\n",
    "    if series.dtype == 'object':\n",
    "        return series.str.strip().eq('').sum()\n",
    "    return 0\n",
    "\n",
    "whitespace_counts = {col: count_whitespace(df_missing[col]) for col in df_missing.select_dtypes(include=['object']).columns}\n",
    "print({k: v for k, v in whitespace_counts.items() if v > 0})\n",
    "\n",
    "# 2. Visualize missing data\n",
    "print(\"\\n2. Missing Data Visualization:\")\n",
    "missing_counts = df_missing.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df_missing) * 100).sort_values(ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "missing_percent[missing_percent > 0].plot(kind='barh', ax=axes[0], color='coral')\n",
    "axes[0].set_title('Missing Data Percentage by Column')\n",
    "axes[0].set_xlabel('Percentage Missing (%)')\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(df_missing.isnull(), yticklabels=False, cbar=True, cmap='viridis', ax=axes[1])\n",
    "axes[1].set_title('Missing Data Pattern (Yellow = Missing)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Replace empty strings and whitespace with NaN\n",
    "print(\"\\n3. Standardizing Missing Values (converting empty strings to NaN):\")\n",
    "df_missing = df_missing.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_missing = df_missing.replace('', np.nan)\n",
    "\n",
    "print(\"After standardization:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# 4. Strategy 1: Drop rows with missing values\n",
    "print(\"\\n4. Strategy 1: Drop Rows with Any Missing Value\")\n",
    "df_dropped_rows = df_missing.dropna()\n",
    "print(f\"   Before: {len(df_missing)} rows\")\n",
    "print(f\"   After: {len(df_dropped_rows)} rows\")\n",
    "print(f\"   Rows removed: {len(df_missing) - len(df_dropped_rows)}\")\n",
    "\n",
    "# 5. Strategy 2: Drop columns with too many missing values\n",
    "print(\"\\n5. Strategy 2: Drop Columns with >50% Missing\")\n",
    "threshold = 0.5\n",
    "df_dropped_cols = df_missing.loc[:, df_missing.isnull().mean() < threshold]\n",
    "print(f\"   Before: {df_missing.shape[1]} columns\")\n",
    "print(f\"   After: {df_dropped_cols.shape[1]} columns\")\n",
    "print(f\"   Columns removed: {set(df_missing.columns) - set(df_dropped_cols.columns)}\")\n",
    "\n",
    "# 6. Strategy 3: Impute with statistical measures\n",
    "print(\"\\n6. Strategy 3: Impute Missing Values\")\n",
    "df_imputed = df_missing.copy()\n",
    "\n",
    "# Impute numerical columns with median\n",
    "numerical_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    median_value = df_imputed[col].median()\n",
    "    df_imputed[col].fillna(median_value, inplace=True)\n",
    "    print(f\"   {col}: Filled with median = {median_value}\")\n",
    "\n",
    "# Impute categorical columns with mode\n",
    "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    mode_value = df_imputed[col].mode()[0] if len(df_imputed[col].mode()) > 0 else 'Unknown'\n",
    "    df_imputed[col].fillna(mode_value, inplace=True)\n",
    "    print(f\"   {col}: Filled with mode = '{mode_value}'\")\n",
    "\n",
    "print(f\"\\n   Missing values after imputation: {df_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# 7. Strategy 4: Forward fill / Backward fill (for time series)\n",
    "print(\"\\n7. Strategy 4: Forward Fill (propagate last valid value)\")\n",
    "df_ffill = df_missing.copy()\n",
    "df_ffill = df_ffill.fillna(method='ffill')\n",
    "print(f\"   Missing values after forward fill: {df_ffill.isnull().sum().sum()}\")\n",
    "\n",
    "# 8. Strategy 5: Create missing indicator\n",
    "print(\"\\n8. Strategy 5: Create Missing Indicator Variables\")\n",
    "df_with_indicators = df_missing.copy()\n",
    "\n",
    "for col in ['salary', 'city', 'country']:\n",
    "    if col in df_with_indicators.columns:\n",
    "        indicator_col = f'{col}_missing'\n",
    "        df_with_indicators[indicator_col] = df_with_indicators[col].isnull().astype(int)\n",
    "        print(f\"   Created: {indicator_col} (1=missing, 0=present)\")\n",
    "\n",
    "print(\"\\nSample with indicators:\")\n",
    "print(df_with_indicators[['salary', 'salary_missing', 'city', 'city_missing']].head())\n",
    "\n",
    "# 9. Advanced: Impute with interpolation\n",
    "print(\"\\n9. Advanced: Interpolation for Numeric Data\")\n",
    "df_interpolated = df_missing.copy()\n",
    "\n",
    "# Convert age to numeric first\n",
    "df_interpolated['age'] = pd.to_numeric(df_interpolated['age'], errors='coerce')\n",
    "\n",
    "# Interpolate\n",
    "df_interpolated['age'] = df_interpolated['age'].interpolate(method='linear')\n",
    "print(f\"   Age missing values after interpolation: {df_interpolated['age'].isnull().sum()}\")\n",
    "\n",
    "# 10. Summary\n",
    "print(\"\\n10. Missing Value Handling Summary:\")\n",
    "summary = pd.DataFrame({\n",
    "    'Method': ['Original', 'Drop Rows', 'Drop Columns', 'Impute', 'Forward Fill', 'Interpolate'],\n",
    "    'Rows': [len(df_missing), len(df_dropped_rows), len(df_dropped_cols), \n",
    "             len(df_imputed), len(df_ffill), len(df_interpolated)],\n",
    "    'Columns': [df_missing.shape[1], df_dropped_rows.shape[1], df_dropped_cols.shape[1],\n",
    "                df_imputed.shape[1], df_ffill.shape[1], df_interpolated.shape[1]],\n",
    "    'Missing_Values': [df_missing.isnull().sum().sum(), df_dropped_rows.isnull().sum().sum(),\n",
    "                      df_dropped_cols.isnull().sum().sum(), df_imputed.isnull().sum().sum(),\n",
    "                      df_ffill.isnull().sum().sum(), df_interpolated.isnull().sum().sum()]\n",
    "})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f35f5",
   "metadata": {},
   "source": [
    "## 3. Removing Duplicates\n",
    "\n",
    "**Why**: Duplicates distort analysis, inflate counts, and bias statistical measures.\n",
    "\n",
    "**When to use**:\n",
    "- When combining data from multiple sources\n",
    "- After data entry processes\n",
    "- When unique records are expected\n",
    "- Before aggregations and analysis\n",
    "\n",
    "**Types of Duplicates**:\n",
    "- **Exact duplicates**: All columns identical\n",
    "- **Partial duplicates**: Key columns identical (e.g., same customer_id)\n",
    "- **Fuzzy duplicates**: Similar but not identical (e.g., typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f18865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. REMOVING DUPLICATES\n",
    "# ============================================\n",
    "\n",
    "df_duplicates = df.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DUPLICATE DETECTION & REMOVAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check for exact duplicates (all columns)\n",
    "print(\"\\n1. Exact Duplicates (All Columns):\")\n",
    "exact_dupes = df_duplicates.duplicated()\n",
    "print(f\"   Number of duplicate rows: {exact_dupes.sum()}\")\n",
    "print(f\"   Percentage: {exact_dupes.sum() / len(df_duplicates) * 100:.1f}%\")\n",
    "\n",
    "if exact_dupes.sum() > 0:\n",
    "    print(\"\\n   Duplicate rows:\")\n",
    "    print(df_duplicates[exact_dupes])\n",
    "\n",
    "# 2. Check duplicates on specific columns (e.g., customer_id)\n",
    "print(\"\\n2. Duplicates Based on Key Column (customer_id):\")\n",
    "key_dupes = df_duplicates.duplicated(subset=['customer_id'])\n",
    "print(f\"   Duplicate customer_ids: {key_dupes.sum()}\")\n",
    "\n",
    "if key_dupes.sum() > 0:\n",
    "    print(\"\\n   Records with duplicate customer_ids:\")\n",
    "    duplicate_ids = df_duplicates[df_duplicates.duplicated(subset=['customer_id'], keep=False)]\n",
    "    print(duplicate_ids.sort_values('customer_id'))\n",
    "\n",
    "# 3. View all occurrences of duplicates (including first)\n",
    "print(\"\\n3. All Occurrences of Duplicates (including first):\")\n",
    "all_dupes = df_duplicates[df_duplicates.duplicated(subset=['customer_id'], keep=False)]\n",
    "print(all_dupes.sort_values('customer_id'))\n",
    "\n",
    "# 4. Remove exact duplicates - keep first occurrence\n",
    "print(\"\\n4. Remove Exact Duplicates (keep='first'):\")\n",
    "df_no_dupes_first = df_duplicates.drop_duplicates()\n",
    "print(f\"   Before: {len(df_duplicates)} rows\")\n",
    "print(f\"   After: {len(df_no_dupes_first)} rows\")\n",
    "print(f\"   Removed: {len(df_duplicates) - len(df_no_dupes_first)} rows\")\n",
    "\n",
    "# 5. Remove duplicates - keep last occurrence\n",
    "print(\"\\n5. Remove Duplicates (keep='last'):\")\n",
    "df_no_dupes_last = df_duplicates.drop_duplicates(keep='last')\n",
    "print(f\"   After (keep last): {len(df_no_dupes_last)} rows\")\n",
    "\n",
    "# 6. Remove duplicates based on specific columns\n",
    "print(\"\\n6. Remove Duplicates Based on customer_id:\")\n",
    "df_no_dupes_id = df_duplicates.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "print(f\"   Before: {len(df_duplicates)} rows\")\n",
    "print(f\"   After: {len(df_no_dupes_id)} rows\")\n",
    "print(f\"   Removed: {len(df_duplicates) - len(df_no_dupes_id)} rows\")\n",
    "\n",
    "# 7. Remove all occurrences of duplicates (keep=False)\n",
    "print(\"\\n7. Remove ALL Occurrences (keep=False):\")\n",
    "df_remove_all = df_duplicates.drop_duplicates(subset=['customer_id'], keep=False)\n",
    "print(f\"   After (removed all duplicate instances): {len(df_remove_all)} rows\")\n",
    "\n",
    "# 8. Identify duplicate groups\n",
    "print(\"\\n8. Identify Duplicate Groups:\")\n",
    "duplicate_groups = df_duplicates.groupby('customer_id').filter(lambda x: len(x) > 1)\n",
    "print(f\"   Number of records in duplicate groups: {len(duplicate_groups)}\")\n",
    "print(\"\\n   Duplicate groups:\")\n",
    "print(duplicate_groups.sort_values('customer_id'))\n",
    "\n",
    "# 9. Custom duplicate detection (case-insensitive email)\n",
    "print(\"\\n9. Custom Duplicate Detection (case-insensitive email):\")\n",
    "df_custom = df_duplicates.copy()\n",
    "df_custom['email_lower'] = df_custom['email'].str.lower().str.strip()\n",
    "email_dupes = df_custom.duplicated(subset=['email_lower'], keep=False)\n",
    "print(f\"   Duplicate emails (case-insensitive): {email_dupes.sum()}\")\n",
    "\n",
    "if email_dupes.sum() > 0:\n",
    "    print(\"\\n   Records with duplicate emails:\")\n",
    "    print(df_custom[email_dupes][['customer_id', 'name', 'email', 'email_lower']].sort_values('email_lower'))\n",
    "\n",
    "# 10. Fuzzy duplicate detection (name similarity)\n",
    "print(\"\\n10. Fuzzy Duplicate Detection (similar names):\")\n",
    "\n",
    "# Simple approach: check for names that differ only in whitespace or case\n",
    "df_fuzzy = df_duplicates.copy()\n",
    "df_fuzzy['name_normalized'] = df_fuzzy['name'].str.lower().str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "fuzzy_dupes = df_fuzzy.duplicated(subset=['name_normalized'], keep=False)\n",
    "\n",
    "print(f\"   Similar names found: {fuzzy_dupes.sum()}\")\n",
    "if fuzzy_dupes.sum() > 0:\n",
    "    print(\"\\n   Records with similar names:\")\n",
    "    print(df_fuzzy[fuzzy_dupes][['customer_id', 'name', 'name_normalized']].sort_values('name_normalized'))\n",
    "\n",
    "# 11. Remove duplicates and reset index\n",
    "print(\"\\n11. Remove Duplicates and Reset Index:\")\n",
    "df_clean = df_duplicates.drop_duplicates(subset=['customer_id']).reset_index(drop=True)\n",
    "print(f\"   Shape after cleanup: {df_clean.shape}\")\n",
    "print(\"\\n   Clean data (first 5 rows):\")\n",
    "print(df_clean.head())\n",
    "\n",
    "# 12. Summary\n",
    "print(\"\\n12. Duplicate Removal Summary:\")\n",
    "summary = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Original',\n",
    "        'Remove exact duplicates (all columns)',\n",
    "        'Remove duplicates by customer_id',\n",
    "        'Remove all duplicate occurrences',\n",
    "        'Custom (case-insensitive email)'\n",
    "    ],\n",
    "    'Rows': [\n",
    "        len(df_duplicates),\n",
    "        len(df_no_dupes_first),\n",
    "        len(df_no_dupes_id),\n",
    "        len(df_remove_all),\n",
    "        len(df_custom.drop_duplicates(subset=['email_lower']))\n",
    "    ],\n",
    "    'Rows_Removed': [\n",
    "        0,\n",
    "        len(df_duplicates) - len(df_no_dupes_first),\n",
    "        len(df_duplicates) - len(df_no_dupes_id),\n",
    "        len(df_duplicates) - len(df_remove_all),\n",
    "        len(df_custom) - len(df_custom.drop_duplicates(subset=['email_lower']))\n",
    "    ]\n",
    "})\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b8c12",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversion\n",
    "\n",
    "**Why**: Correct data types ensure proper operations, storage efficiency, and prevent errors.\n",
    "\n",
    "**When to use**:\n",
    "- After loading data (often defaults to object/string)\n",
    "- Before mathematical operations\n",
    "- Before date/time manipulations\n",
    "- To optimize memory usage\n",
    "\n",
    "**Common Conversions**:\n",
    "- String â†’ Numeric (int, float)\n",
    "- String â†’ DateTime\n",
    "- Numeric â†’ Category (for memory efficiency)\n",
    "- Object â†’ Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cba60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. DATA TYPE CONVERSION\n",
    "# ============================================\n",
    "\n",
    "df_types = df.drop_duplicates(subset=['customer_id']).reset_index(drop=True).copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA TYPE CONVERSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check current data types\n",
    "print(\"\\n1. Current Data Types:\")\n",
    "print(df_types.dtypes)\n",
    "\n",
    "print(\"\\n   Memory usage before conversion:\")\n",
    "print(f\"   Total: {df_types.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# 2. Convert age to numeric (handling errors)\n",
    "print(\"\\n2. Converting 'age' to Numeric:\")\n",
    "print(f\"   Before: {df_types['age'].dtype}\")\n",
    "print(f\"   Sample values: {df_types['age'].head()}\")\n",
    "\n",
    "# errors='coerce' converts invalid values to NaN\n",
    "df_types['age'] = pd.to_numeric(df_types['age'], errors='coerce')\n",
    "print(f\"   After: {df_types['age'].dtype}\")\n",
    "print(f\"   NaN values created: {df_types['age'].isnull().sum()}\")\n",
    "\n",
    "# 3. Clean and convert salary\n",
    "print(\"\\n3. Converting 'salary' to Numeric:\")\n",
    "print(f\"   Before: {df_types['salary'].dtype}\")\n",
    "print(f\"   Sample problematic value: {df_types[df_types['salary'].astype(str).str.contains('\\\\$', na=False)]['salary'].values}\")\n",
    "\n",
    "# Remove $, commas, and convert to numeric\n",
    "df_types['salary_clean'] = df_types['salary'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "df_types['salary'] = pd.to_numeric(df_types['salary_clean'], errors='coerce')\n",
    "df_types = df_types.drop('salary_clean', axis=1)\n",
    "\n",
    "print(f\"   After: {df_types['salary'].dtype}\")\n",
    "print(f\"   Sample values: {df_types['salary'].head()}\")\n",
    "\n",
    "# 4. Convert categorical columns to category type\n",
    "print(\"\\n4. Converting to Category Type (memory optimization):\")\n",
    "categorical_columns = ['city', 'country', 'department', 'status']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in df_types.columns:\n",
    "        memory_before = df_types[col].memory_usage(deep=True)\n",
    "        df_types[col] = df_types[col].astype('category')\n",
    "        memory_after = df_types[col].memory_usage(deep=True)\n",
    "        print(f\"   {col}: {memory_before} â†’ {memory_after} bytes (saved {memory_before - memory_after} bytes)\")\n",
    "\n",
    "# 5. Convert join_date to datetime\n",
    "print(\"\\n5. Converting 'join_date' to DateTime:\")\n",
    "print(f\"   Before: {df_types['join_date'].dtype}\")\n",
    "print(f\"   Sample values: {df_types['join_date'].head()}\")\n",
    "\n",
    "# Parse multiple date formats\n",
    "df_types['join_date'] = pd.to_datetime(df_types['join_date'], errors='coerce', \n",
    "                                       infer_datetime_format=True)\n",
    "print(f\"   After: {df_types['join_date'].dtype}\")\n",
    "print(f\"   Invalid dates converted to NaT: {df_types['join_date'].isnull().sum()}\")\n",
    "\n",
    "# 6. Extract date components\n",
    "print(\"\\n6. Extracting Date Components:\")\n",
    "df_types['join_year'] = df_types['join_date'].dt.year\n",
    "df_types['join_month'] = df_types['join_date'].dt.month\n",
    "df_types['join_day'] = df_types['join_date'].dt.day\n",
    "df_types['join_dayofweek'] = df_types['join_date'].dt.day_name()\n",
    "\n",
    "print(\"   New columns created:\")\n",
    "print(df_types[['join_date', 'join_year', 'join_month', 'join_day', 'join_dayofweek']].head())\n",
    "\n",
    "# 7. Convert to boolean\n",
    "print(\"\\n7. Creating Boolean Column:\")\n",
    "df_types['is_active'] = df_types['status'].str.lower() == 'active'\n",
    "print(\"   Status â†’ is_active:\")\n",
    "print(df_types[['status', 'is_active']].head())\n",
    "\n",
    "# 8. Downcast numeric types to save memory\n",
    "print(\"\\n8. Downcasting Numeric Types:\")\n",
    "print(\"   Before downcasting:\")\n",
    "print(df_types[['customer_id', 'age']].dtypes)\n",
    "\n",
    "# Downcast to smallest suitable type\n",
    "df_types['customer_id'] = pd.to_numeric(df_types['customer_id'], downcast='integer')\n",
    "df_types['age'] = pd.to_numeric(df_types['age'], downcast='integer')\n",
    "\n",
    "print(\"   After downcasting:\")\n",
    "print(df_types[['customer_id', 'age']].dtypes)\n",
    "\n",
    "# 9. Convert object to string (explicit)\n",
    "print(\"\\n9. Converting Object to String (explicit):\")\n",
    "string_cols = ['name', 'email', 'phone']\n",
    "for col in string_cols:\n",
    "    if col in df_types.columns and df_types[col].dtype == 'object':\n",
    "        df_types[col] = df_types[col].astype('string')\n",
    "        print(f\"   {col}: object â†’ string\")\n",
    "\n",
    "# 10. Summary after conversion\n",
    "print(\"\\n10. Data Types After Conversion:\")\n",
    "print(df_types.dtypes)\n",
    "\n",
    "print(\"\\n    Memory usage after conversion:\")\n",
    "print(f\"    Total: {df_types.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# 11. Type conversion with custom function\n",
    "print(\"\\n11. Custom Type Conversion Function:\")\n",
    "\n",
    "def convert_to_proper_types(df):\n",
    "    \"\"\"\n",
    "    Automatically detect and convert data types\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    for col in df_converted.columns:\n",
    "        # Try numeric conversion\n",
    "        if df_converted[col].dtype == 'object':\n",
    "            # Try converting to numeric\n",
    "            try:\n",
    "                numeric_col = pd.to_numeric(df_converted[col], errors='coerce')\n",
    "                if numeric_col.notna().sum() / len(df_converted) > 0.5:  # If >50% successfully converted\n",
    "                    df_converted[col] = numeric_col\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try datetime conversion\n",
    "            try:\n",
    "                datetime_col = pd.to_datetime(df_converted[col], errors='coerce')\n",
    "                if datetime_col.notna().sum() / len(df_converted) > 0.5:\n",
    "                    df_converted[col] = datetime_col\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Convert low-cardinality strings to category\n",
    "            if df_converted[col].nunique() / len(df_converted) < 0.5:\n",
    "                df_converted[col] = df_converted[col].astype('category')\n",
    "    \n",
    "    return df_converted\n",
    "\n",
    "print(\"   Function created: convert_to_proper_types()\")\n",
    "print(\"   Use this to automatically detect and convert types!\")\n",
    "\n",
    "# 12. Data type validation\n",
    "print(\"\\n12. Type Validation:\")\n",
    "expected_types = {\n",
    "    'customer_id': ['int8', 'int16', 'int32', 'int64'],\n",
    "    'age': ['int8', 'int16', 'int32', 'int64', 'float64'],\n",
    "    'salary': ['float64'],\n",
    "    'join_date': ['datetime64[ns]'],\n",
    "    'department': ['category']\n",
    "}\n",
    "\n",
    "print(\"   Validating expected types:\")\n",
    "for col, expected in expected_types.items():\n",
    "    if col in df_types.columns:\n",
    "        actual = str(df_types[col].dtype)\n",
    "        status = \"âœ“\" if actual in expected else \"âœ—\"\n",
    "        print(f\"   {status} {col}: expected {expected}, got {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1c97f",
   "metadata": {},
   "source": [
    "## 5. String Cleaning & Standardization\n",
    "\n",
    "**Why**: Inconsistent text causes duplicate detection failures and analysis errors.\n",
    "\n",
    "**When to use**:\n",
    "- Before grouping or aggregating\n",
    "- When matching records\n",
    "- Before categorical encoding\n",
    "- When cleaning user-entered data\n",
    "\n",
    "**Common Issues**:\n",
    "- Inconsistent case (John vs JOHN vs john)\n",
    "- Extra whitespace\n",
    "- Special characters\n",
    "- Typos and misspellings\n",
    "- Inconsistent formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4883a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. STRING CLEANING & STANDARDIZATION\n",
    "# ============================================\n",
    "\n",
    "df_strings = df.drop_duplicates(subset=['customer_id']).reset_index(drop=True).copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRING CLEANING & STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Remove leading/trailing whitespace\n",
    "print(\"\\n1. Removing Leading/Trailing Whitespace:\")\n",
    "print(\"   Before:\")\n",
    "print(df_strings[['name', 'email', 'city']].head())\n",
    "\n",
    "df_strings['name'] = df_strings['name'].str.strip()\n",
    "df_strings['email'] = df_strings['email'].str.strip()\n",
    "df_strings['city'] = df_strings['city'].str.strip()\n",
    "df_strings['department'] = df_strings['department'].str.strip()\n",
    "\n",
    "print(\"\\n   After strip():\")\n",
    "print(df_strings[['name', 'email', 'city']].head())\n",
    "\n",
    "# 2. Standardize case\n",
    "print(\"\\n2. Standardizing Case:\")\n",
    "print(\"   Original cities:\", df_strings['city'].unique())\n",
    "\n",
    "# Title case for names\n",
    "df_strings['name_clean'] = df_strings['name'].str.title()\n",
    "\n",
    "# Lowercase for emails\n",
    "df_strings['email_clean'] = df_strings['email'].str.lower()\n",
    "\n",
    "# Title case for cities\n",
    "df_strings['city_clean'] = df_strings['city'].str.title()\n",
    "\n",
    "# Uppercase for country codes\n",
    "df_strings['country_clean'] = df_strings['country'].str.upper()\n",
    "\n",
    "print(\"\\n   Cleaned:\")\n",
    "print(df_strings[['name', 'name_clean', 'city', 'city_clean']].head())\n",
    "\n",
    "# 3. Remove extra spaces between words\n",
    "print(\"\\n3. Removing Extra Spaces:\")\n",
    "sample_text = \"  John    Doe  \"\n",
    "print(f\"   Before: '{sample_text}'\")\n",
    "cleaned = re.sub(r'\\s+', ' ', sample_text).strip()\n",
    "print(f\"   After: '{cleaned}'\")\n",
    "\n",
    "df_strings['name_clean'] = df_strings['name_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
    "print(\"\\n   Applied to names:\")\n",
    "print(df_strings[['name', 'name_clean']].head())\n",
    "\n",
    "# 4. Replace special characters\n",
    "print(\"\\n4. Replacing Special Characters:\")\n",
    "\n",
    "# Replace dots in email domains\n",
    "print(\"   Before (email):\", df_strings['email'].iloc[3])\n",
    "df_strings['email_clean'] = df_strings['email_clean'].str.replace(r'\\.\\.+', '.', regex=True)\n",
    "print(\"   After (fixed double dots):\", df_strings['email_clean'].iloc[3])\n",
    "\n",
    "# Remove special characters from phone\n",
    "print(\"\\n   Standardizing phone numbers:\")\n",
    "print(\"   Before:\", df_strings['phone'].head())\n",
    "\n",
    "df_strings['phone_clean'] = df_strings['phone'].str.replace(r'[^0-9]', '', regex=True)\n",
    "print(\"   After (digits only):\", df_strings['phone_clean'].head())\n",
    "\n",
    "# 5. Standardize categorical values\n",
    "print(\"\\n5. Standardizing Categorical Values:\")\n",
    "\n",
    "# Create mapping for country variations\n",
    "country_mapping = {\n",
    "    'USA': 'USA',\n",
    "    'US': 'USA',\n",
    "    'U.S.A': 'USA',\n",
    "    'UNITED STATES': 'USA'\n",
    "}\n",
    "\n",
    "print(\"   Before:\", df_strings['country'].value_counts())\n",
    "\n",
    "df_strings['country_clean'] = df_strings['country'].str.upper().map(country_mapping)\n",
    "df_strings['country_clean'] = df_strings['country_clean'].fillna(df_strings['country'].str.upper())\n",
    "\n",
    "print(\"\\n   After mapping:\")\n",
    "print(df_strings['country_clean'].value_counts())\n",
    "\n",
    "# 6. Fix typos and common misspellings\n",
    "print(\"\\n6. Fixing Common Misspellings:\")\n",
    "\n",
    "# Department standardization\n",
    "dept_mapping = {\n",
    "    'SALES': 'Sales',\n",
    "    'MARKETING': 'Marketing',\n",
    "    'IT': 'IT',\n",
    "    'HR': 'HR',\n",
    "    'FINANCE': 'Finance'\n",
    "}\n",
    "\n",
    "df_strings['department_clean'] = df_strings['department'].str.upper().map(dept_mapping)\n",
    "print(\"   Department values after standardization:\")\n",
    "print(df_strings['department_clean'].value_counts())\n",
    "\n",
    "# 7. Validate email format\n",
    "print(\"\\n7. Validating Email Format:\")\n",
    "\n",
    "def is_valid_email(email):\n",
    "    \"\"\"Check if email format is valid\"\"\"\n",
    "    if pd.isna(email):\n",
    "        return False\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, str(email)))\n",
    "\n",
    "df_strings['email_valid'] = df_strings['email_clean'].apply(is_valid_email)\n",
    "\n",
    "print(f\"   Valid emails: {df_strings['email_valid'].sum()}\")\n",
    "print(f\"   Invalid emails: {(~df_strings['email_valid']).sum()}\")\n",
    "\n",
    "print(\"\\n   Invalid email examples:\")\n",
    "print(df_strings[~df_strings['email_valid']][['name_clean', 'email', 'email_clean']])\n",
    "\n",
    "# 8. Standardize phone format\n",
    "print(\"\\n8. Standardizing Phone Format:\")\n",
    "\n",
    "def format_phone(phone):\n",
    "    \"\"\"Format phone number as XXX-XXX-XXXX\"\"\"\n",
    "    if pd.isna(phone):\n",
    "        return None\n",
    "    \n",
    "    # Remove all non-digits\n",
    "    digits = re.sub(r'[^0-9]', '', str(phone))\n",
    "    \n",
    "    # Format if 10 digits\n",
    "    if len(digits) == 10:\n",
    "        return f\"{digits[:3]}-{digits[3:6]}-{digits[6:]}\"\n",
    "    else:\n",
    "        return digits\n",
    "\n",
    "df_strings['phone_formatted'] = df_strings['phone'].apply(format_phone)\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for i in range(min(5, len(df_strings))):\n",
    "    print(f\"   {df_strings['phone'].iloc[i]} â†’ {df_strings['phone_formatted'].iloc[i]}\")\n",
    "\n",
    "# 9. Remove non-ASCII characters\n",
    "print(\"\\n9. Removing Non-ASCII Characters:\")\n",
    "\n",
    "sample_with_unicode = \"JosÃ© GarcÃ­aâ„¢\"\n",
    "print(f\"   Before: {sample_with_unicode}\")\n",
    "\n",
    "cleaned_ascii = sample_with_unicode.encode('ascii', 'ignore').decode('ascii')\n",
    "print(f\"   After: {cleaned_ascii}\")\n",
    "\n",
    "# Apply to name column\n",
    "df_strings['name_ascii'] = df_strings['name_clean'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "# 10. Consistent capitalization for abbreviations\n",
    "print(\"\\n10. Handling Abbreviations:\")\n",
    "\n",
    "def standardize_abbreviations(text):\n",
    "    \"\"\"Standardize common abbreviations\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # Replace common abbreviations\n",
    "    replacements = {\n",
    "        r'\\bSt\\b': 'Street',\n",
    "        r'\\bAve\\b': 'Avenue',\n",
    "        r'\\bDr\\b': 'Drive',\n",
    "        r'\\bRd\\b': 'Road',\n",
    "        r'\\bBlvd\\b': 'Boulevard'\n",
    "    }\n",
    "    \n",
    "    result = str(text)\n",
    "    for pattern, replacement in replacements.items():\n",
    "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "address = \"123 Main St, Apt 4B\"\n",
    "print(f\"   Before: {address}\")\n",
    "print(f\"   After: {standardize_abbreviations(address)}\")\n",
    "\n",
    "# 11. Comprehensive string cleaning function\n",
    "print(\"\\n11. Comprehensive String Cleaning Function:\")\n",
    "\n",
    "def clean_string(text, \n",
    "                 strip=True, \n",
    "                 lower=False, \n",
    "                 upper=False, \n",
    "                 title=False,\n",
    "                 remove_special=False,\n",
    "                 remove_digits=False,\n",
    "                 remove_extra_spaces=True):\n",
    "    \"\"\"\n",
    "    Comprehensive string cleaning function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to clean\n",
    "    strip : bool\n",
    "        Remove leading/trailing whitespace\n",
    "    lower : bool\n",
    "        Convert to lowercase\n",
    "    upper : bool\n",
    "        Convert to uppercase\n",
    "    title : bool\n",
    "        Convert to title case\n",
    "    remove_special : bool\n",
    "        Remove special characters\n",
    "    remove_digits : bool\n",
    "        Remove digits\n",
    "    remove_extra_spaces : bool\n",
    "        Replace multiple spaces with single space\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Cleaned text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    result = str(text)\n",
    "    \n",
    "    if strip:\n",
    "        result = result.strip()\n",
    "    \n",
    "    if remove_special:\n",
    "        result = re.sub(r'[^a-zA-Z0-9\\s]', '', result)\n",
    "    \n",
    "    if remove_digits:\n",
    "        result = re.sub(r'\\d', '', result)\n",
    "    \n",
    "    if remove_extra_spaces:\n",
    "        result = re.sub(r'\\s+', ' ', result)\n",
    "    \n",
    "    if lower:\n",
    "        result = result.lower()\n",
    "    elif upper:\n",
    "        result = result.upper()\n",
    "    elif title:\n",
    "        result = result.title()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "test_text = \"  John@123  DOE!!  \"\n",
    "print(f\"   Original: '{test_text}'\")\n",
    "print(f\"   Cleaned: '{clean_string(test_text, title=True, remove_special=True, remove_digits=True)}'\")\n",
    "\n",
    "# 12. Apply comprehensive cleaning\n",
    "print(\"\\n12. Final Cleaned Data Sample:\")\n",
    "final_clean_cols = ['name_clean', 'email_clean', 'city_clean', 'country_clean', \n",
    "                    'phone_formatted', 'department_clean']\n",
    "\n",
    "print(df_strings[final_clean_cols].head(10))\n",
    "\n",
    "# 13. Summary of string cleaning operations\n",
    "print(\"\\n13. String Cleaning Summary:\")\n",
    "print(\"   âœ“ Whitespace removed (leading/trailing)\")\n",
    "print(\"   âœ“ Case standardized (title/lower/upper as appropriate)\")\n",
    "print(\"   âœ“ Extra spaces removed\")\n",
    "print(\"   âœ“ Special characters handled\")\n",
    "print(\"   âœ“ Phone numbers formatted consistently\")\n",
    "print(\"   âœ“ Email validation performed\")\n",
    "print(\"   âœ“ Categorical values standardized\")\n",
    "print(\"   âœ“ Country codes unified\")\n",
    "print(\"   âœ“ Department names standardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b26878",
   "metadata": {},
   "source": [
    "## 6. Handling Outliers & Invalid Values\n",
    "\n",
    "**Why**: Outliers and invalid values can skew analysis and break models.\n",
    "\n",
    "**When to use**:\n",
    "- Before statistical analysis\n",
    "- When preparing data for modeling\n",
    "- After data integration\n",
    "- When detecting data entry errors\n",
    "\n",
    "**Approaches**:\n",
    "- **Detect**: IQR, Z-score, domain knowledge\n",
    "- **Handle**: Remove, cap, transform, or keep (if legitimate)\n",
    "- **Validate**: Check against business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4013e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. HANDLING OUTLIERS & INVALID VALUES\n",
    "# ============================================\n",
    "\n",
    "df_outliers = df.drop_duplicates(subset=['customer_id']).reset_index(drop=True).copy()\n",
    "\n",
    "# Convert age and salary to numeric\n",
    "df_outliers['age'] = pd.to_numeric(df_outliers['age'], errors='coerce')\n",
    "df_outliers['salary'] = df_outliers['salary'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "df_outliers['salary'] = pd.to_numeric(df_outliers['salary'], errors='coerce')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HANDLING OUTLIERS & INVALID VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Identify invalid values (domain knowledge)\n",
    "print(\"\\n1. Identifying Invalid Values (Business Rules):\")\n",
    "\n",
    "# Age should be between 18 and 100\n",
    "print(\"\\n   Age validation (valid range: 18-100):\")\n",
    "invalid_age = (df_outliers['age'] < 18) | (df_outliers['age'] > 100)\n",
    "print(f\"   Invalid ages: {invalid_age.sum()}\")\n",
    "if invalid_age.sum() > 0:\n",
    "    print(df_outliers[invalid_age][['customer_id', 'name', 'age']])\n",
    "\n",
    "# Salary should be positive and reasonable (e.g., 10000-500000)\n",
    "print(\"\\n   Salary validation (valid range: $10,000-$500,000):\")\n",
    "invalid_salary = (df_outliers['salary'] < 10000) | (df_outliers['salary'] > 500000)\n",
    "print(f\"   Invalid salaries: {invalid_salary.sum()}\")\n",
    "if invalid_salary.sum() > 0:\n",
    "    print(df_outliers[invalid_salary][['customer_id', 'name', 'salary']])\n",
    "\n",
    "# 2. Detect outliers using IQR method\n",
    "print(\"\\n2. Detecting Outliers (IQR Method):\")\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Age outliers\n",
    "age_outliers, age_lower, age_upper = detect_outliers_iqr(df_outliers, 'age')\n",
    "print(f\"\\n   Age outliers (IQR method):\")\n",
    "print(f\"   Lower bound: {age_lower:.1f}, Upper bound: {age_upper:.1f}\")\n",
    "print(f\"   Outliers found: {age_outliers.sum()}\")\n",
    "\n",
    "# Salary outliers\n",
    "salary_outliers, salary_lower, salary_upper = detect_outliers_iqr(df_outliers, 'salary')\n",
    "print(f\"\\n   Salary outliers (IQR method):\")\n",
    "print(f\"   Lower bound: ${salary_lower:,.0f}, Upper bound: ${salary_upper:,.0f}\")\n",
    "print(f\"   Outliers found: {salary_outliers.sum()}\")\n",
    "\n",
    "# 3. Visualize outliers\n",
    "print(\"\\n3. Visualizing Outliers:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Age boxplot\n",
    "df_outliers.boxplot(column='age', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Age Distribution (Box Plot)')\n",
    "axes[0, 0].set_ylabel('Age')\n",
    "axes[0, 0].axhline(y=age_lower, color='r', linestyle='--', label='Lower bound')\n",
    "axes[0, 0].axhline(y=age_upper, color='r', linestyle='--', label='Upper bound')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Age histogram\n",
    "df_outliers['age'].hist(bins=20, ax=axes[0, 1], edgecolor='black')\n",
    "axes[0, 1].set_title('Age Distribution (Histogram)')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Salary boxplot\n",
    "df_outliers.boxplot(column='salary', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Salary Distribution (Box Plot)')\n",
    "axes[1, 0].set_ylabel('Salary')\n",
    "\n",
    "# Salary histogram\n",
    "df_outliers['salary'].hist(bins=20, ax=axes[1, 1], edgecolor='black')\n",
    "axes[1, 1].set_title('Salary Distribution (Histogram)')\n",
    "axes[1, 1].set_xlabel('Salary')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Handle invalid values - Method 1: Remove\n",
    "print(\"\\n4. Method 1: Remove Invalid Values\")\n",
    "df_removed = df_outliers.copy()\n",
    "\n",
    "# Remove invalid ages\n",
    "df_removed = df_removed[(df_removed['age'] >= 18) & (df_removed['age'] <= 100)]\n",
    "print(f\"   Rows after removing invalid ages: {len(df_removed)}\")\n",
    "\n",
    "# Remove invalid salaries\n",
    "df_removed = df_removed[(df_removed['salary'] >= 10000) & (df_removed['salary'] <= 500000)]\n",
    "print(f\"   Rows after removing invalid salaries: {len(df_removed)}\")\n",
    "\n",
    "# 5. Handle outliers - Method 2: Cap (Winsorization)\n",
    "print(\"\\n5. Method 2: Cap Outliers (Winsorization)\")\n",
    "df_capped = df_outliers.copy()\n",
    "\n",
    "# Cap age at valid limits\n",
    "df_capped['age_capped'] = df_capped['age'].clip(lower=18, upper=100)\n",
    "print(f\"   Age: min={df_capped['age_capped'].min():.0f}, max={df_capped['age_capped'].max():.0f}\")\n",
    "\n",
    "# Cap salary at IQR bounds\n",
    "df_capped['salary_capped'] = df_capped['salary'].clip(lower=salary_lower, upper=salary_upper)\n",
    "print(f\"   Salary: min=${df_capped['salary_capped'].min():,.0f}, max=${df_capped['salary_capped'].max():,.0f}\")\n",
    "\n",
    "# 6. Handle outliers - Method 3: Transform\n",
    "print(\"\\n6. Method 3: Transform Data (Log Transformation)\")\n",
    "df_transformed = df_outliers.copy()\n",
    "\n",
    "# Log transform salary to reduce impact of outliers\n",
    "df_transformed['salary_log'] = np.log1p(df_transformed['salary'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df_outliers['salary'].hist(bins=20, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('Original Salary Distribution')\n",
    "axes[0].set_xlabel('Salary')\n",
    "\n",
    "df_transformed['salary_log'].hist(bins=20, ax=axes[1], edgecolor='black', color='green')\n",
    "axes[1].set_title('Log-Transformed Salary')\n",
    "axes[1].set_xlabel('Log(Salary)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Handle outliers - Method 4: Replace with median\n",
    "print(\"\\n7. Method 4: Replace Outliers with Median\")\n",
    "df_replaced = df_outliers.copy()\n",
    "\n",
    "# Replace age outliers with median\n",
    "age_median = df_replaced['age'].median()\n",
    "df_replaced.loc[age_outliers, 'age'] = age_median\n",
    "print(f\"   Age outliers replaced with median: {age_median:.0f}\")\n",
    "\n",
    "# Replace salary outliers with median\n",
    "salary_median = df_replaced['salary'].median()\n",
    "df_replaced.loc[salary_outliers, 'salary'] = salary_median\n",
    "print(f\"   Salary outliers replaced with median: ${salary_median:,.0f}\")\n",
    "\n",
    "# 8. Create outlier flags\n",
    "print(\"\\n8. Creating Outlier Indicator Flags\")\n",
    "df_flagged = df_outliers.copy()\n",
    "\n",
    "df_flagged['age_outlier'] = age_outliers.astype(int)\n",
    "df_flagged['salary_outlier'] = salary_outliers.astype(int)\n",
    "\n",
    "print(\"   Sample with flags:\")\n",
    "print(df_flagged[['name', 'age', 'age_outlier', 'salary', 'salary_outlier']].head(10))\n",
    "\n",
    "# 9. Validate against multiple rules\n",
    "print(\"\\n9. Multi-Rule Validation\")\n",
    "\n",
    "def validate_record(row):\n",
    "    \"\"\"Validate a record against multiple business rules\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Age validation\n",
    "    if pd.notna(row['age']):\n",
    "        if row['age'] < 18:\n",
    "            issues.append('Age too young')\n",
    "        elif row['age'] > 100:\n",
    "            issues.append('Age too old')\n",
    "    \n",
    "    # Salary validation\n",
    "    if pd.notna(row['salary']):\n",
    "        if row['salary'] < 10000:\n",
    "            issues.append('Salary too low')\n",
    "        elif row['salary'] > 500000:\n",
    "            issues.append('Salary too high')\n",
    "    \n",
    "    # Email validation\n",
    "    if pd.notna(row['email']):\n",
    "        if '@' not in str(row['email']):\n",
    "            issues.append('Invalid email format')\n",
    "    \n",
    "    return '; '.join(issues) if issues else 'Valid'\n",
    "\n",
    "df_outliers['validation_status'] = df_outliers.apply(validate_record, axis=1)\n",
    "\n",
    "print(\"   Validation results:\")\n",
    "print(df_outliers['validation_status'].value_counts())\n",
    "\n",
    "print(\"\\n   Records with issues:\")\n",
    "invalid_records = df_outliers[df_outliers['validation_status'] != 'Valid']\n",
    "print(invalid_records[['customer_id', 'name', 'age', 'salary', 'email', 'validation_status']])\n",
    "\n",
    "# 10. Summary of outlier handling methods\n",
    "print(\"\\n10. Outlier Handling Methods Summary:\")\n",
    "summary = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Original',\n",
    "        'Remove Invalid',\n",
    "        'Cap (Winsorize)',\n",
    "        'Log Transform',\n",
    "        'Replace with Median',\n",
    "        'Flag Only'\n",
    "    ],\n",
    "    'Rows': [\n",
    "        len(df_outliers),\n",
    "        len(df_removed),\n",
    "        len(df_capped),\n",
    "        len(df_transformed),\n",
    "        len(df_replaced),\n",
    "        len(df_flagged)\n",
    "    ],\n",
    "    'Age_Range': [\n",
    "        f\"{df_outliers['age'].min():.0f}-{df_outliers['age'].max():.0f}\",\n",
    "        f\"{df_removed['age'].min():.0f}-{df_removed['age'].max():.0f}\",\n",
    "        f\"{df_capped['age_capped'].min():.0f}-{df_capped['age_capped'].max():.0f}\",\n",
    "        f\"{df_outliers['age'].min():.0f}-{df_outliers['age'].max():.0f}\",\n",
    "        f\"{df_replaced['age'].min():.0f}-{df_replaced['age'].max():.0f}\",\n",
    "        f\"{df_outliers['age'].min():.0f}-{df_outliers['age'].max():.0f}\"\n",
    "    ],\n",
    "    'Description': [\n",
    "        'No changes',\n",
    "        'Removes rows with invalid values',\n",
    "        'Caps extreme values at boundaries',\n",
    "        'Log transformation to reduce skew',\n",
    "        'Replaces outliers with median',\n",
    "        'Keeps data, adds indicator columns'\n",
    "    ]\n",
    "})\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec2af6",
   "metadata": {},
   "source": [
    "## 7. Data Validation\n",
    "\n",
    "**What is it?**\n",
    "Data validation ensures that data meets specific business rules, constraints, and requirements before being used in analysis or modeling.\n",
    "\n",
    "**Why use it?**\n",
    "- Ensures data quality and reliability\n",
    "- Prevents errors in downstream processes\n",
    "- Maintains data integrity across systems\n",
    "- Catches data entry errors early\n",
    "- Ensures compliance with business rules\n",
    "\n",
    "**When to use it?**\n",
    "- After data cleaning operations\n",
    "- Before loading data into databases\n",
    "- When receiving data from external sources\n",
    "- During ETL/ELT processes\n",
    "- Before analysis or modeling\n",
    "\n",
    "**Validation Types:**\n",
    "1. **Type validation**: Ensure correct data types\n",
    "2. **Range validation**: Values within acceptable ranges\n",
    "3. **Format validation**: Data follows expected patterns\n",
    "4. **Uniqueness validation**: Keys are unique\n",
    "5. **Referential integrity**: Foreign keys exist in referenced tables\n",
    "6. **Business rule validation**: Custom domain-specific rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. DATA VALIDATION\n",
    "# ============================================\n",
    "\n",
    "df_validate = df.drop_duplicates(subset=['customer_id']).reset_index(drop=True).copy()\n",
    "\n",
    "# Clean data first\n",
    "df_validate['age'] = pd.to_numeric(df_validate['age'], errors='coerce')\n",
    "df_validate['salary'] = df_validate['salary'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "df_validate['salary'] = pd.to_numeric(df_validate['salary'], errors='coerce')\n",
    "df_validate['email'] = df_validate['email'].str.strip().str.lower()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Type validation\n",
    "print(\"\\n1. Type Validation:\")\n",
    "\n",
    "def validate_types(df):\n",
    "    \"\"\"Validate data types of columns\"\"\"\n",
    "    expected_types = {\n",
    "        'customer_id': ['int64', 'float64'],\n",
    "        'name': ['object', 'string'],\n",
    "        'email': ['object', 'string'],\n",
    "        'age': ['int64', 'float64'],\n",
    "        'salary': ['int64', 'float64'],\n",
    "        'city': ['object', 'string'],\n",
    "        'country': ['object', 'string'],\n",
    "        'department': ['object', 'string']\n",
    "    }\n",
    "    \n",
    "    issues = []\n",
    "    for col, expected in expected_types.items():\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype.name not in expected:\n",
    "                issues.append(f\"{col}: expected {expected}, got {df[col].dtype.name}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "type_issues = validate_types(df_validate)\n",
    "if type_issues:\n",
    "    print(\"   Type validation issues:\")\n",
    "    for issue in type_issues:\n",
    "        print(f\"   - {issue}\")\n",
    "else:\n",
    "    print(\"   âœ“ All columns have correct types\")\n",
    "\n",
    "# 2. Range validation\n",
    "print(\"\\n2. Range Validation:\")\n",
    "\n",
    "def validate_ranges(df):\n",
    "    \"\"\"Validate that numeric values are within acceptable ranges\"\"\"\n",
    "    rules = {\n",
    "        'age': (18, 100),\n",
    "        'salary': (10000, 500000),\n",
    "        'customer_id': (1, 999999)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for col, (min_val, max_val) in rules.items():\n",
    "        if col in df.columns:\n",
    "            below_min = (df[col] < min_val).sum()\n",
    "            above_max = (df[col] > max_val).sum()\n",
    "            valid = len(df) - below_min - above_max - df[col].isna().sum()\n",
    "            \n",
    "            results[col] = {\n",
    "                'valid': valid,\n",
    "                'below_min': below_min,\n",
    "                'above_max': above_max,\n",
    "                'missing': df[col].isna().sum(),\n",
    "                'range': f\"{min_val}-{max_val}\"\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "range_results = validate_ranges(df_validate)\n",
    "for col, stats in range_results.items():\n",
    "    print(f\"\\n   {col} (range: {stats['range']}):\")\n",
    "    print(f\"   Valid: {stats['valid']}, Below min: {stats['below_min']}, \" \n",
    "          f\"Above max: {stats['above_max']}, Missing: {stats['missing']}\")\n",
    "\n",
    "# 3. Format validation\n",
    "print(\"\\n3. Format Validation:\")\n",
    "\n",
    "def validate_email(email):\n",
    "    \"\"\"Validate email format\"\"\"\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if pd.isna(email):\n",
    "        return False\n",
    "    return bool(re.match(pattern, str(email)))\n",
    "\n",
    "def validate_phone(phone):\n",
    "    \"\"\"Validate phone format (10 digits)\"\"\"\n",
    "    if pd.isna(phone):\n",
    "        return False\n",
    "    digits = re.sub(r'\\D', '', str(phone))\n",
    "    return len(digits) == 10\n",
    "\n",
    "# Validate emails\n",
    "df_validate['email_valid'] = df_validate['email'].apply(validate_email)\n",
    "invalid_emails = (~df_validate['email_valid']).sum()\n",
    "print(f\"   Email validation: {invalid_emails} invalid formats\")\n",
    "if invalid_emails > 0:\n",
    "    print(\"   Invalid emails:\")\n",
    "    print(df_validate[~df_validate['email_valid']][['name', 'email']])\n",
    "\n",
    "# Validate phone numbers\n",
    "df_validate['phone_valid'] = df_validate['phone'].apply(validate_phone)\n",
    "invalid_phones = (~df_validate['phone_valid']).sum()\n",
    "print(f\"\\n   Phone validation: {invalid_phones} invalid formats\")\n",
    "if invalid_phones > 0:\n",
    "    print(\"   Invalid phones:\")\n",
    "    print(df_validate[~df_validate['phone_valid']][['name', 'phone']])\n",
    "\n",
    "# 4. Uniqueness validation\n",
    "print(\"\\n4. Uniqueness Validation:\")\n",
    "\n",
    "def check_uniqueness(df, columns):\n",
    "    \"\"\"Check if specified columns have unique values\"\"\"\n",
    "    results = {}\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            total = len(df)\n",
    "            unique = df[col].nunique()\n",
    "            duplicates = total - unique\n",
    "            results[col] = {\n",
    "                'total': total,\n",
    "                'unique': unique,\n",
    "                'duplicates': duplicates,\n",
    "                'is_unique': duplicates == 0\n",
    "            }\n",
    "    return results\n",
    "\n",
    "unique_cols = ['customer_id', 'email']\n",
    "uniqueness_results = check_uniqueness(df_validate, unique_cols)\n",
    "\n",
    "for col, stats in uniqueness_results.items():\n",
    "    status = \"âœ“ UNIQUE\" if stats['is_unique'] else \"âœ— HAS DUPLICATES\"\n",
    "    print(f\"   {col}: {status}\")\n",
    "    print(f\"   Total: {stats['total']}, Unique: {stats['unique']}, Duplicates: {stats['duplicates']}\")\n",
    "\n",
    "# 5. Allowed values validation\n",
    "print(\"\\n5. Allowed Values Validation:\")\n",
    "\n",
    "allowed_values = {\n",
    "    'department': ['sales', 'engineering', 'hr', 'marketing', 'finance'],\n",
    "    'country': ['usa', 'uk', 'canada', 'australia']\n",
    "}\n",
    "\n",
    "def validate_allowed_values(df, rules):\n",
    "    \"\"\"Check if values are in allowed list\"\"\"\n",
    "    results = {}\n",
    "    for col, allowed in rules.items():\n",
    "        if col in df.columns:\n",
    "            # Normalize for comparison\n",
    "            df_values = df[col].str.lower().str.strip()\n",
    "            invalid = ~df_values.isin(allowed) & df_values.notna()\n",
    "            invalid_count = invalid.sum()\n",
    "            \n",
    "            results[col] = {\n",
    "                'invalid_count': invalid_count,\n",
    "                'invalid_values': df[invalid][col].unique().tolist()\n",
    "            }\n",
    "    return results\n",
    "\n",
    "allowed_results = validate_allowed_values(df_validate, allowed_values)\n",
    "\n",
    "for col, stats in allowed_results.items():\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"   Invalid count: {stats['invalid_count']}\")\n",
    "    if stats['invalid_values']:\n",
    "        print(f\"   Invalid values: {stats['invalid_values']}\")\n",
    "\n",
    "# 6. Completeness validation\n",
    "print(\"\\n6. Completeness Validation:\")\n",
    "\n",
    "def validate_completeness(df, required_columns):\n",
    "    \"\"\"Check that required columns have no missing values\"\"\"\n",
    "    results = {}\n",
    "    for col in required_columns:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isna().sum()\n",
    "            completeness = (1 - missing / len(df)) * 100\n",
    "            results[col] = {\n",
    "                'missing': missing,\n",
    "                'completeness': completeness,\n",
    "                'is_complete': missing == 0\n",
    "            }\n",
    "    return results\n",
    "\n",
    "required_cols = ['customer_id', 'name', 'email', 'department']\n",
    "completeness_results = validate_completeness(df_validate, required_cols)\n",
    "\n",
    "for col, stats in completeness_results.items():\n",
    "    status = \"âœ“ COMPLETE\" if stats['is_complete'] else \"âœ— INCOMPLETE\"\n",
    "    print(f\"   {col}: {status}\")\n",
    "    print(f\"   Missing: {stats['missing']}, Completeness: {stats['completeness']:.1f}%\")\n",
    "\n",
    "# 7. Business rule validation\n",
    "print(\"\\n7. Business Rule Validation:\")\n",
    "\n",
    "def validate_business_rules(row):\n",
    "    \"\"\"Validate custom business rules\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Rule 1: Sales department should have salary > 40000\n",
    "    if pd.notna(row['department']) and pd.notna(row['salary']):\n",
    "        if row['department'].lower() == 'sales' and row['salary'] < 40000:\n",
    "            errors.append(\"Sales dept salary must be >= $40,000\")\n",
    "    \n",
    "    # Rule 2: Engineering department should have salary > 60000\n",
    "    if pd.notna(row['department']) and pd.notna(row['salary']):\n",
    "        if row['department'].lower() == 'engineering' and row['salary'] < 60000:\n",
    "            errors.append(\"Engineering dept salary must be >= $60,000\")\n",
    "    \n",
    "    # Rule 3: Age and experience correlation (if we had experience field)\n",
    "    # Young people (< 25) shouldn't have very high salaries (> 100000)\n",
    "    if pd.notna(row['age']) and pd.notna(row['salary']):\n",
    "        if row['age'] < 25 and row['salary'] > 100000:\n",
    "            errors.append(\"Salary unusually high for age\")\n",
    "    \n",
    "    return errors if errors else ['Valid']\n",
    "\n",
    "df_validate['business_rule_errors'] = df_validate.apply(validate_business_rules, axis=1)\n",
    "df_validate['business_rules_valid'] = df_validate['business_rule_errors'].apply(lambda x: x == ['Valid'])\n",
    "\n",
    "violations = (~df_validate['business_rules_valid']).sum()\n",
    "print(f\"   Business rule violations: {violations}\")\n",
    "if violations > 0:\n",
    "    print(\"\\n   Records with violations:\")\n",
    "    print(df_validate[~df_validate['business_rules_valid']][['name', 'department', 'age', 'salary', 'business_rule_errors']])\n",
    "\n",
    "# 8. Create comprehensive validation report\n",
    "print(\"\\n8. Comprehensive Validation Report:\")\n",
    "\n",
    "def create_validation_report(df):\n",
    "    \"\"\"Create a comprehensive validation report\"\"\"\n",
    "    report = {\n",
    "        'Total Records': len(df),\n",
    "        'Type Issues': len(validate_types(df)),\n",
    "        'Invalid Emails': (~df['email_valid']).sum() if 'email_valid' in df else 0,\n",
    "        'Invalid Phones': (~df['phone_valid']).sum() if 'phone_valid' in df else 0,\n",
    "        'Duplicate Customer IDs': len(df) - df['customer_id'].nunique(),\n",
    "        'Missing Required Fields': sum([df[col].isna().sum() for col in ['customer_id', 'name', 'email'] if col in df.columns]),\n",
    "        'Business Rule Violations': (~df['business_rules_valid']).sum() if 'business_rules_valid' in df else 0,\n",
    "        'Overall Valid Records': 0\n",
    "    }\n",
    "    \n",
    "    # Calculate overall valid records (no issues in any category)\n",
    "    valid_mask = (\n",
    "        df['email_valid'] & \n",
    "        df['phone_valid'] & \n",
    "        df['business_rules_valid']\n",
    "    )\n",
    "    report['Overall Valid Records'] = valid_mask.sum()\n",
    "    report['Data Quality Score'] = f\"{(valid_mask.sum() / len(df) * 100):.1f}%\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "validation_report = create_validation_report(df_validate)\n",
    "\n",
    "print(\"\\n   VALIDATION SUMMARY:\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "for key, value in validation_report.items():\n",
    "    print(f\"   {key:.<40} {value:>15}\")\n",
    "\n",
    "# 9. Flag all validation issues\n",
    "print(\"\\n9. Creating Validation Flags:\")\n",
    "\n",
    "df_validate['has_type_issues'] = False  # Would be True if types don't match\n",
    "df_validate['has_range_issues'] = (\n",
    "    (df_validate['age'] < 18) | (df_validate['age'] > 100) |\n",
    "    (df_validate['salary'] < 10000) | (df_validate['salary'] > 500000)\n",
    ")\n",
    "df_validate['has_format_issues'] = ~df_validate['email_valid'] | ~df_validate['phone_valid']\n",
    "df_validate['has_business_issues'] = ~df_validate['business_rules_valid']\n",
    "\n",
    "# Overall validation flag\n",
    "df_validate['is_valid'] = (\n",
    "    ~df_validate['has_type_issues'] &\n",
    "    ~df_validate['has_range_issues'] &\n",
    "    ~df_validate['has_format_issues'] &\n",
    "    ~df_validate['has_business_issues']\n",
    ")\n",
    "\n",
    "print(\"   Validation status:\")\n",
    "print(df_validate['is_valid'].value_counts())\n",
    "\n",
    "print(\"\\n   Sample of invalid records:\")\n",
    "invalid_sample = df_validate[~df_validate['is_valid']].head()\n",
    "print(invalid_sample[['name', 'email', 'has_range_issues', 'has_format_issues', 'has_business_issues']])\n",
    "\n",
    "print(\"\\n   âœ“ Data validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8e953",
   "metadata": {},
   "source": [
    "## 8. Dealing with Inconsistent Data\n",
    "\n",
    "**What is it?**\n",
    "Inconsistent data refers to data that doesn't follow standard formats, naming conventions, or representations across the dataset.\n",
    "\n",
    "**Why use it?**\n",
    "- Standardizes data for analysis\n",
    "- Improves data quality and reliability\n",
    "- Enables accurate grouping and aggregation\n",
    "- Facilitates data integration from multiple sources\n",
    "- Reduces errors in reporting and analytics\n",
    "\n",
    "**When to use it?**\n",
    "- When merging data from multiple sources\n",
    "- Data has mixed formats or conventions\n",
    "- Categorical variables have variations\n",
    "- Units or scales differ across records\n",
    "- Case sensitivity issues exist\n",
    "\n",
    "**Common inconsistencies:**\n",
    "1. **Case variations**: \"USA\" vs \"usa\" vs \"Usa\"\n",
    "2. **Spelling variations**: \"New York\" vs \"NY\" vs \"New York City\"\n",
    "3. **Unit differences**: \"5000\" vs \"5K\" vs \"$5,000\"\n",
    "4. **Date formats**: \"2024-01-15\" vs \"01/15/2024\" vs \"15-Jan-2024\"\n",
    "5. **Abbreviations**: \"Street\" vs \"St.\" vs \"St\"\n",
    "6. **Encoding issues**: Special characters, whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. DEALING WITH INCONSISTENT DATA\n",
    "# ============================================\n",
    "\n",
    "# Create sample data with inconsistencies\n",
    "inconsistent_data = {\n",
    "    'product_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'product_name': ['iPhone 14', 'iphone 14', 'IPHONE 14', 'iPhone14', \n",
    "                     'Samsung Galaxy', 'samsung galaxy', 'Galaxy Samsung', 'SamsungGalaxy'],\n",
    "    'category': ['Electronics', 'electronics', 'ELECTRONICS', 'Electronics',\n",
    "                'Electronics', 'Electronic', 'electronics', 'Electronics'],\n",
    "    'price': ['$999', '999.00', '$999.00', '999',\n",
    "             '$799', '799.00', '$799.00', '799'],\n",
    "    'quantity': ['5', '5 units', '5pcs', '5 pieces',\n",
    "                '10', '10 units', '10pcs', '10 pieces'],\n",
    "    'weight': ['1.5kg', '1.5 kg', '1500g', '1.5',\n",
    "              '2kg', '2.0 kg', '2000g', '2.0'],\n",
    "    'country': ['USA', 'usa', 'U.S.A', 'United States',\n",
    "               'UK', 'United Kingdom', 'U.K.', 'uk'],\n",
    "    'status': ['Active', 'active', 'ACTIVE', 'Active',\n",
    "              'Inactive', 'inactive', 'INACTIVE', 'In-Active']\n",
    "}\n",
    "\n",
    "df_inconsistent = pd.DataFrame(inconsistent_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEALING WITH INCONSISTENT DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Original Data with Inconsistencies:\")\n",
    "print(df_inconsistent)\n",
    "\n",
    "# 2. Standardize case\n",
    "print(\"\\n2. Standardizing Case:\")\n",
    "\n",
    "df_clean = df_inconsistent.copy()\n",
    "\n",
    "# Standardize text columns to title case\n",
    "df_clean['product_name'] = df_clean['product_name'].str.lower().str.title()\n",
    "df_clean['category'] = df_clean['category'].str.lower().str.title()\n",
    "df_clean['status'] = df_clean['status'].str.lower().str.title()\n",
    "\n",
    "print(\"   After case standardization:\")\n",
    "print(df_clean[['product_name', 'category', 'status']])\n",
    "\n",
    "# 3. Remove whitespace inconsistencies\n",
    "print(\"\\n3. Removing Whitespace Inconsistencies:\")\n",
    "\n",
    "# Remove extra spaces\n",
    "df_clean['product_name'] = df_clean['product_name'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "df_clean['weight'] = df_clean['weight'].str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "print(\"   Product names after whitespace removal:\")\n",
    "print(df_clean['product_name'].unique())\n",
    "\n",
    "# 4. Standardize product names\n",
    "print(\"\\n4. Standardizing Product Names:\")\n",
    "\n",
    "# Create mapping for product variations\n",
    "product_mapping = {\n",
    "    'iphone 14': 'iPhone 14',\n",
    "    'iphone14': 'iPhone 14',\n",
    "    'samsung galaxy': 'Samsung Galaxy',\n",
    "    'galaxy samsung': 'Samsung Galaxy',\n",
    "    'samsunggalaxy': 'Samsung Galaxy'\n",
    "}\n",
    "\n",
    "# Apply standardization (case-insensitive)\n",
    "df_clean['product_name_std'] = df_clean['product_name'].str.lower().map(product_mapping)\n",
    "df_clean['product_name_std'] = df_clean['product_name_std'].fillna(df_clean['product_name'])\n",
    "\n",
    "print(\"   Original â†’ Standardized:\")\n",
    "for orig, std in zip(df_inconsistent['product_name'], df_clean['product_name_std']):\n",
    "    print(f\"   {orig:20} â†’ {std}\")\n",
    "\n",
    "# 5. Standardize price format\n",
    "print(\"\\n5. Standardizing Price Format:\")\n",
    "\n",
    "def standardize_price(price):\n",
    "    \"\"\"Convert various price formats to float\"\"\"\n",
    "    # Remove currency symbols and spaces\n",
    "    price_str = str(price).replace('$', '').replace(',', '').strip()\n",
    "    try:\n",
    "        return float(price_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_clean['price_std'] = df_clean['price'].apply(standardize_price)\n",
    "\n",
    "print(\"   Original â†’ Standardized:\")\n",
    "for orig, std in zip(df_inconsistent['price'], df_clean['price_std']):\n",
    "    print(f\"   {orig:10} â†’ ${std:.2f}\")\n",
    "\n",
    "# 6. Standardize quantity format\n",
    "print(\"\\n6. Standardizing Quantity Format:\")\n",
    "\n",
    "def extract_quantity(qty):\n",
    "    \"\"\"Extract numeric quantity from various formats\"\"\"\n",
    "    # Extract digits only\n",
    "    numbers = re.findall(r'\\d+', str(qty))\n",
    "    return int(numbers[0]) if numbers else None\n",
    "\n",
    "df_clean['quantity_std'] = df_clean['quantity'].apply(extract_quantity)\n",
    "\n",
    "print(\"   Original â†’ Standardized:\")\n",
    "for orig, std in zip(df_inconsistent['quantity'], df_clean['quantity_std']):\n",
    "    print(f\"   {orig:15} â†’ {std}\")\n",
    "\n",
    "# 7. Standardize weight to kg\n",
    "print(\"\\n7. Standardizing Weight (convert to kg):\")\n",
    "\n",
    "def standardize_weight(weight):\n",
    "    \"\"\"Convert various weight formats to kg\"\"\"\n",
    "    weight_str = str(weight).lower().strip()\n",
    "    \n",
    "    # Extract number\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', weight_str)\n",
    "    if not numbers:\n",
    "        return None\n",
    "    \n",
    "    value = float(numbers[0])\n",
    "    \n",
    "    # Convert based on unit\n",
    "    if 'g' in weight_str and 'kg' not in weight_str:\n",
    "        value = value / 1000  # grams to kg\n",
    "    elif 'lb' in weight_str:\n",
    "        value = value * 0.453592  # pounds to kg\n",
    "    \n",
    "    return round(value, 2)\n",
    "\n",
    "df_clean['weight_kg'] = df_clean['weight'].apply(standardize_weight)\n",
    "\n",
    "print(\"   Original â†’ Standardized (kg):\")\n",
    "for orig, std in zip(df_inconsistent['weight'], df_clean['weight_kg']):\n",
    "    print(f\"   {orig:10} â†’ {std} kg\")\n",
    "\n",
    "# 8. Standardize country names\n",
    "print(\"\\n8. Standardizing Country Names:\")\n",
    "\n",
    "# Create comprehensive country mapping\n",
    "country_mapping = {\n",
    "    'usa': 'United States',\n",
    "    'u.s.a': 'United States',\n",
    "    'u.s.': 'United States',\n",
    "    'united states': 'United States',\n",
    "    'us': 'United States',\n",
    "    'uk': 'United Kingdom',\n",
    "    'u.k.': 'United Kingdom',\n",
    "    'united kingdom': 'United Kingdom',\n",
    "    'britain': 'United Kingdom',\n",
    "    'great britain': 'United Kingdom'\n",
    "}\n",
    "\n",
    "df_clean['country_std'] = df_clean['country'].str.lower().map(country_mapping)\n",
    "df_clean['country_std'] = df_clean['country_std'].fillna(df_clean['country'])\n",
    "\n",
    "print(\"   Original â†’ Standardized:\")\n",
    "for orig, std in zip(df_inconsistent['country'], df_clean['country_std']):\n",
    "    print(f\"   {orig:20} â†’ {std}\")\n",
    "\n",
    "# 9. Standardize status values\n",
    "print(\"\\n9. Standardizing Status Values:\")\n",
    "\n",
    "# Normalize status\n",
    "df_clean['status_std'] = df_clean['status'].str.lower().str.replace('-', '').str.replace('_', '')\n",
    "\n",
    "# Map to standard values\n",
    "status_mapping = {\n",
    "    'active': 'Active',\n",
    "    'inactive': 'Inactive'\n",
    "}\n",
    "\n",
    "df_clean['status_std'] = df_clean['status_std'].map(status_mapping)\n",
    "\n",
    "print(\"   Original â†’ Standardized:\")\n",
    "for orig, std in zip(df_inconsistent['status'], df_clean['status_std']):\n",
    "    print(f\"   {orig:15} â†’ {std}\")\n",
    "\n",
    "# 10. Standardize category names\n",
    "print(\"\\n10. Standardizing Category Names:\")\n",
    "\n",
    "# Fix common misspellings and variations\n",
    "category_mapping = {\n",
    "    'electronics': 'Electronics',\n",
    "    'electronic': 'Electronics',  # Singular to plural\n",
    "    'electronix': 'Electronics',  # Common misspelling\n",
    "}\n",
    "\n",
    "df_clean['category_std'] = df_clean['category'].str.lower().map(category_mapping)\n",
    "df_clean['category_std'] = df_clean['category_std'].fillna(df_clean['category'])\n",
    "\n",
    "print(\"   Unique categories before standardization:\", df_inconsistent['category'].nunique())\n",
    "print(\"   Unique categories after standardization:\", df_clean['category_std'].nunique())\n",
    "print(\"   Standardized categories:\", df_clean['category_std'].unique())\n",
    "\n",
    "# 11. Create final cleaned dataset\n",
    "print(\"\\n11. Final Cleaned Dataset:\")\n",
    "\n",
    "df_final = df_clean[[\n",
    "    'product_id',\n",
    "    'product_name_std',\n",
    "    'category_std',\n",
    "    'price_std',\n",
    "    'quantity_std',\n",
    "    'weight_kg',\n",
    "    'country_std',\n",
    "    'status_std'\n",
    "]].rename(columns={\n",
    "    'product_name_std': 'product_name',\n",
    "    'category_std': 'category',\n",
    "    'price_std': 'price',\n",
    "    'quantity_std': 'quantity',\n",
    "    'weight_kg': 'weight_kg',\n",
    "    'country_std': 'country',\n",
    "    'status_std': 'status'\n",
    "})\n",
    "\n",
    "print(df_final)\n",
    "\n",
    "# 12. Compare before and after\n",
    "print(\"\\n12. Comparison: Before vs After\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Unique Product Names',\n",
    "        'Unique Categories',\n",
    "        'Unique Countries',\n",
    "        'Unique Status Values',\n",
    "        'Price Format',\n",
    "        'Quantity Format',\n",
    "        'Weight Format'\n",
    "    ],\n",
    "    'Before': [\n",
    "        df_inconsistent['product_name'].nunique(),\n",
    "        df_inconsistent['category'].nunique(),\n",
    "        df_inconsistent['country'].nunique(),\n",
    "        df_inconsistent['status'].nunique(),\n",
    "        'Mixed',\n",
    "        'Mixed',\n",
    "        'Mixed'\n",
    "    ],\n",
    "    'After': [\n",
    "        df_final['product_name'].nunique(),\n",
    "        df_final['category'].nunique(),\n",
    "        df_final['country'].nunique(),\n",
    "        df_final['status'].nunique(),\n",
    "        'Numeric (USD)',\n",
    "        'Integer',\n",
    "        'Float (kg)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# 13. Create a standardization function\n",
    "print(\"\\n13. Reusable Standardization Function:\")\n",
    "\n",
    "def standardize_dataset(df, mappings):\n",
    "    \"\"\"\n",
    "    Generic function to standardize a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to standardize\n",
    "    - mappings: dict with column names as keys and standardization rules as values\n",
    "    \"\"\"\n",
    "    df_std = df.copy()\n",
    "    \n",
    "    for column, rules in mappings.items():\n",
    "        if column not in df_std.columns:\n",
    "            continue\n",
    "            \n",
    "        rule_type = rules.get('type', 'mapping')\n",
    "        \n",
    "        if rule_type == 'mapping':\n",
    "            # Use provided mapping dictionary\n",
    "            mapping = rules.get('mapping', {})\n",
    "            df_std[column] = df_std[column].str.lower().map(mapping)\n",
    "            df_std[column] = df_std[column].fillna(df[column])\n",
    "            \n",
    "        elif rule_type == 'case':\n",
    "            # Standardize case\n",
    "            case_type = rules.get('case', 'lower')\n",
    "            if case_type == 'lower':\n",
    "                df_std[column] = df_std[column].str.lower()\n",
    "            elif case_type == 'upper':\n",
    "                df_std[column] = df_std[column].str.upper()\n",
    "            elif case_type == 'title':\n",
    "                df_std[column] = df_std[column].str.title()\n",
    "                \n",
    "        elif rule_type == 'numeric':\n",
    "            # Extract numeric values\n",
    "            df_std[column] = pd.to_numeric(\n",
    "                df_std[column].astype(str).str.extract(r'(\\d+\\.?\\d*)', expand=False),\n",
    "                errors='coerce'\n",
    "            )\n",
    "    \n",
    "    return df_std\n",
    "\n",
    "# Example usage\n",
    "standardization_rules = {\n",
    "    'country': {\n",
    "        'type': 'mapping',\n",
    "        'mapping': country_mapping\n",
    "    },\n",
    "    'status': {\n",
    "        'type': 'case',\n",
    "        'case': 'title'\n",
    "    },\n",
    "    'price': {\n",
    "        'type': 'numeric'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"   âœ“ Standardization function created\")\n",
    "print(\"   Usage: standardize_dataset(df, standardization_rules)\")\n",
    "\n",
    "print(\"\\n   âœ“ Data inconsistency handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245e3af",
   "metadata": {},
   "source": [
    "## 9. Date & Time Data Cleaning\n",
    "\n",
    "**What is it?**\n",
    "Date and time cleaning involves parsing, validating, and standardizing temporal data into consistent formats.\n",
    "\n",
    "**Why use it?**\n",
    "- Enables time-based analysis and calculations\n",
    "- Ensures consistency across time zones\n",
    "- Handles various date formats\n",
    "- Extracts meaningful temporal features\n",
    "- Fixes invalid or ambiguous dates\n",
    "\n",
    "**When to use it?**\n",
    "- Data has mixed date formats\n",
    "- Time zone conversions needed\n",
    "- Invalid dates present\n",
    "- Need to extract temporal features (day, month, year)\n",
    "- Handling timestamps from different systems\n",
    "- Time series analysis\n",
    "\n",
    "**Common date/time issues:**\n",
    "1. **Format variations**: \"2024-01-15\", \"01/15/2024\", \"15-Jan-2024\"\n",
    "2. **Time zones**: UTC, EST, PST conversions\n",
    "3. **Invalid dates**: \"2024-02-30\", \"13/45/2024\"\n",
    "4. **Missing time components**: Dates without times\n",
    "5. **Ambiguous formats**: \"01/02/2024\" (Jan 2 or Feb 1?)\n",
    "6. **Epoch timestamps**: Unix timestamps vs datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb1de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. DATE & TIME DATA CLEANING\n",
    "# ============================================\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create sample data with date/time issues\n",
    "datetime_data = {\n",
    "    'transaction_id': range(1, 13),\n",
    "    'date_str': [\n",
    "        '2024-01-15',           # ISO format\n",
    "        '01/15/2024',           # US format\n",
    "        '15-Jan-2024',          # Day-Month-Year with month name\n",
    "        '2024/01/15',           # ISO with slashes\n",
    "        '15.01.2024',           # European format\n",
    "        '2024-02-30',           # Invalid date\n",
    "        'Jan 15, 2024',         # Month Day, Year\n",
    "        '2024-01-15 14:30:00',  # With time\n",
    "        '15/01/2024',           # Ambiguous (could be Jan 15 or May 1)\n",
    "        '2024-W03-1',           # Week format\n",
    "        '',                     # Missing\n",
    "        '1705334400'            # Unix timestamp\n",
    "    ],\n",
    "    'time_str': [\n",
    "        '14:30:00',\n",
    "        '2:30 PM',\n",
    "        '14:30',\n",
    "        '02:30:00 PM',\n",
    "        '143000',\n",
    "        '25:70:00',  # Invalid time\n",
    "        '14.30.00',\n",
    "        '14:30:00.000',\n",
    "        '2:30p',\n",
    "        '14h30m',\n",
    "        '',\n",
    "        '1705334400'\n",
    "    ],\n",
    "    'timezone': [\n",
    "        'UTC', 'EST', 'PST', 'UTC', 'CET',\n",
    "        'UTC', 'EST', 'PST', 'UTC', 'EST',\n",
    "        'UTC', 'UTC'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_datetime = pd.DataFrame(datetime_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATE & TIME DATA CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Original Data with Various Date/Time Formats:\")\n",
    "print(df_datetime)\n",
    "\n",
    "# 2. Parse multiple date formats\n",
    "print(\"\\n2. Parsing Multiple Date Formats:\")\n",
    "\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"Try multiple date formats to parse a date string\"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = [\n",
    "        '%Y-%m-%d',           # 2024-01-15\n",
    "        '%m/%d/%Y',           # 01/15/2024\n",
    "        '%d-%b-%Y',           # 15-Jan-2024\n",
    "        '%Y/%m/%d',           # 2024/01/15\n",
    "        '%d.%m.%Y',           # 15.01.2024\n",
    "        '%b %d, %Y',          # Jan 15, 2024\n",
    "        '%Y-%m-%d %H:%M:%S',  # 2024-01-15 14:30:00\n",
    "        '%d/%m/%Y',           # 15/01/2024 (European)\n",
    "    ]\n",
    "    \n",
    "    # Try Unix timestamp\n",
    "    try:\n",
    "        timestamp = float(date_str)\n",
    "        if timestamp > 1000000000:  # Likely a Unix timestamp\n",
    "            return pd.to_datetime(timestamp, unit='s')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try each format\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # If all fail, return NaT\n",
    "    return pd.NaT\n",
    "\n",
    "df_datetime['date_parsed'] = df_datetime['date_str'].apply(parse_date_flexible)\n",
    "\n",
    "print(\"   Original â†’ Parsed:\")\n",
    "for orig, parsed in zip(df_datetime['date_str'], df_datetime['date_parsed']):\n",
    "    status = \"âœ“\" if pd.notna(parsed) else \"âœ—\"\n",
    "    print(f\"   {status} {str(orig):25} â†’ {parsed}\")\n",
    "\n",
    "# 3. Identify invalid dates\n",
    "print(\"\\n3. Identifying Invalid Dates:\")\n",
    "\n",
    "invalid_dates = df_datetime['date_parsed'].isna()\n",
    "print(f\"   Invalid/unparseable dates: {invalid_dates.sum()}\")\n",
    "if invalid_dates.sum() > 0:\n",
    "    print(\"   Invalid date strings:\")\n",
    "    print(df_datetime[invalid_dates][['transaction_id', 'date_str']])\n",
    "\n",
    "# 4. Parse time formats\n",
    "print(\"\\n4. Parsing Time Formats:\")\n",
    "\n",
    "def parse_time_flexible(time_str):\n",
    "    \"\"\"Parse various time formats\"\"\"\n",
    "    if pd.isna(time_str) or time_str == '':\n",
    "        return pd.NaT\n",
    "    \n",
    "    time_str = str(time_str).strip()\n",
    "    \n",
    "    # Try Unix timestamp\n",
    "    try:\n",
    "        timestamp = float(time_str)\n",
    "        if timestamp > 1000000000:\n",
    "            dt = pd.to_datetime(timestamp, unit='s')\n",
    "            return dt.time()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Handle formats like '143000' (HHMMSS)\n",
    "    if time_str.isdigit() and len(time_str) == 6:\n",
    "        try:\n",
    "            return datetime.strptime(time_str, '%H%M%S').time()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Handle formats like '14h30m'\n",
    "    if 'h' in time_str.lower() and 'm' in time_str.lower():\n",
    "        try:\n",
    "            parts = time_str.lower().replace('h', ':').replace('m', '').split(':')\n",
    "            return datetime.strptime(f\"{parts[0]}:{parts[1]}:00\", '%H:%M:%S').time()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Standard formats\n",
    "    formats = [\n",
    "        '%H:%M:%S',           # 14:30:00\n",
    "        '%I:%M %p',           # 2:30 PM\n",
    "        '%H:%M',              # 14:30\n",
    "        '%I:%M:%S %p',        # 02:30:00 PM\n",
    "        '%H.%M.%S',           # 14.30.00\n",
    "        '%H:%M:%S.%f',        # 14:30:00.000\n",
    "        '%I:%M%p',            # 2:30PM\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(time_str, fmt).time()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return pd.NaT\n",
    "\n",
    "df_datetime['time_parsed'] = df_datetime['time_str'].apply(parse_time_flexible)\n",
    "\n",
    "print(\"   Original â†’ Parsed:\")\n",
    "for orig, parsed in zip(df_datetime['time_str'], df_datetime['time_parsed']):\n",
    "    status = \"âœ“\" if pd.notna(parsed) else \"âœ—\"\n",
    "    print(f\"   {status} {str(orig):20} â†’ {parsed}\")\n",
    "\n",
    "# 5. Combine date and time\n",
    "print(\"\\n5. Combining Date and Time:\")\n",
    "\n",
    "def combine_datetime(date, time):\n",
    "    \"\"\"Combine date and time into datetime\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return pd.NaT\n",
    "    \n",
    "    if pd.isna(time):\n",
    "        # Return date with midnight time\n",
    "        return pd.to_datetime(date)\n",
    "    \n",
    "    # Combine date and time\n",
    "    return pd.to_datetime(f\"{date.date()} {time}\")\n",
    "\n",
    "df_datetime['datetime_combined'] = df_datetime.apply(\n",
    "    lambda row: combine_datetime(row['date_parsed'], row['time_parsed']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"   Combined datetime:\")\n",
    "print(df_datetime[['date_parsed', 'time_parsed', 'datetime_combined']].head(10))\n",
    "\n",
    "# 6. Extract date components\n",
    "print(\"\\n6. Extracting Date Components:\")\n",
    "\n",
    "df_datetime['year'] = df_datetime['date_parsed'].dt.year\n",
    "df_datetime['month'] = df_datetime['date_parsed'].dt.month\n",
    "df_datetime['day'] = df_datetime['date_parsed'].dt.day\n",
    "df_datetime['day_of_week'] = df_datetime['date_parsed'].dt.day_name()\n",
    "df_datetime['week_of_year'] = df_datetime['date_parsed'].dt.isocalendar().week\n",
    "df_datetime['quarter'] = df_datetime['date_parsed'].dt.quarter\n",
    "df_datetime['is_weekend'] = df_datetime['date_parsed'].dt.dayofweek >= 5\n",
    "\n",
    "print(\"   Date components extracted:\")\n",
    "print(df_datetime[['date_parsed', 'year', 'month', 'day', 'day_of_week', 'is_weekend']].head(8))\n",
    "\n",
    "# 7. Handle time zones\n",
    "print(\"\\n7. Handling Time Zones:\")\n",
    "\n",
    "# Create timezone-aware datetime (for valid dates)\n",
    "df_datetime['datetime_utc'] = pd.to_datetime(df_datetime['datetime_combined'], utc=True, errors='coerce')\n",
    "\n",
    "print(\"   UTC datetime:\")\n",
    "print(df_datetime[['datetime_combined', 'timezone', 'datetime_utc']].head(8))\n",
    "\n",
    "# 8. Calculate time differences\n",
    "print(\"\\n8. Calculating Time Differences:\")\n",
    "\n",
    "# Create a reference date\n",
    "reference_date = pd.to_datetime('2024-01-15')\n",
    "\n",
    "df_datetime['days_from_reference'] = (df_datetime['date_parsed'] - reference_date).dt.days\n",
    "\n",
    "print(\"   Days from reference date (2024-01-15):\")\n",
    "print(df_datetime[['date_parsed', 'days_from_reference']].head(8))\n",
    "\n",
    "# 9. Validate date ranges\n",
    "print(\"\\n9. Validating Date Ranges:\")\n",
    "\n",
    "def validate_date_range(date, min_date='2020-01-01', max_date='2030-12-31'):\n",
    "    \"\"\"Check if date is within valid range\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return False\n",
    "    \n",
    "    min_dt = pd.to_datetime(min_date)\n",
    "    max_dt = pd.to_datetime(max_date)\n",
    "    \n",
    "    return min_dt <= date <= max_dt\n",
    "\n",
    "df_datetime['date_valid'] = df_datetime['date_parsed'].apply(validate_date_range)\n",
    "\n",
    "print(f\"   Valid dates (2020-2030): {df_datetime['date_valid'].sum()}\")\n",
    "print(f\"   Invalid/out-of-range dates: {(~df_datetime['date_valid']).sum()}\")\n",
    "\n",
    "# 10. Handle missing dates\n",
    "print(\"\\n10. Handling Missing Dates:\")\n",
    "\n",
    "df_datetime_filled = df_datetime.copy()\n",
    "\n",
    "# Strategy 1: Fill with reference date\n",
    "df_datetime_filled['date_filled_ref'] = df_datetime_filled['date_parsed'].fillna(reference_date)\n",
    "\n",
    "# Strategy 2: Forward fill\n",
    "df_datetime_filled['date_filled_ffill'] = df_datetime_filled['date_parsed'].fillna(method='ffill')\n",
    "\n",
    "# Strategy 3: Backward fill\n",
    "df_datetime_filled['date_filled_bfill'] = df_datetime_filled['date_parsed'].fillna(method='bfill')\n",
    "\n",
    "print(\"   Missing date handling strategies:\")\n",
    "print(df_datetime_filled[['date_str', 'date_parsed', 'date_filled_ref', 'date_filled_ffill']].tail(5))\n",
    "\n",
    "# 11. Standardize to ISO format\n",
    "print(\"\\n11. Standardizing to ISO Format:\")\n",
    "\n",
    "df_datetime['date_iso'] = df_datetime['date_parsed'].dt.strftime('%Y-%m-%d')\n",
    "df_datetime['datetime_iso'] = df_datetime['datetime_combined'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(\"   Standardized formats:\")\n",
    "print(df_datetime[['date_str', 'date_iso', 'datetime_iso']].head(8))\n",
    "\n",
    "# 12. Create date bins\n",
    "print(\"\\n12. Creating Date Categories/Bins:\")\n",
    "\n",
    "# Categorize by month\n",
    "df_datetime['month_name'] = df_datetime['date_parsed'].dt.strftime('%B')\n",
    "\n",
    "# Categorize by season (Northern Hemisphere)\n",
    "def get_season(month):\n",
    "    if pd.isna(month):\n",
    "        return 'Unknown'\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_datetime['season'] = df_datetime['month'].apply(get_season)\n",
    "\n",
    "# Categorize by time of day\n",
    "def get_time_of_day(time):\n",
    "    if pd.isna(time):\n",
    "        return 'Unknown'\n",
    "    hour = time.hour if hasattr(time, 'hour') else 0\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_datetime['time_of_day'] = df_datetime['time_parsed'].apply(get_time_of_day)\n",
    "\n",
    "print(\"   Temporal categories:\")\n",
    "print(df_datetime[['date_parsed', 'month_name', 'season', 'time_of_day']].head(8))\n",
    "\n",
    "# 13. Summary\n",
    "print(\"\\n13. Date/Time Cleaning Summary:\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Records',\n",
    "        'Successfully Parsed Dates',\n",
    "        'Failed to Parse Dates',\n",
    "        'Successfully Parsed Times',\n",
    "        'Failed to Parse Times',\n",
    "        'Valid Dates (in range)',\n",
    "        'Invalid Dates (out of range)',\n",
    "        'Dates with Timezone Info'\n",
    "    ],\n",
    "    'Count': [\n",
    "        len(df_datetime),\n",
    "        df_datetime['date_parsed'].notna().sum(),\n",
    "        df_datetime['date_parsed'].isna().sum(),\n",
    "        df_datetime['time_parsed'].notna().sum(),\n",
    "        df_datetime['time_parsed'].isna().sum(),\n",
    "        df_datetime['date_valid'].sum(),\n",
    "        (~df_datetime['date_valid']).sum(),\n",
    "        df_datetime['datetime_utc'].notna().sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Date & time cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbacdc",
   "metadata": {},
   "source": [
    "## 10. Categorical Data Cleaning\n",
    "\n",
    "**What is it?**\n",
    "Categorical data cleaning involves standardizing, encoding, and optimizing categorical variables for analysis and machine learning.\n",
    "\n",
    "**Why use it?**\n",
    "- Reduces cardinality (number of unique values)\n",
    "- Standardizes category names\n",
    "- Handles misspellings and variations\n",
    "- Prepares data for machine learning models\n",
    "- Improves memory efficiency\n",
    "- Enables proper grouping and analysis\n",
    "\n",
    "**When to use it?**\n",
    "- Categories have typos or variations\n",
    "- Too many unique categories (high cardinality)\n",
    "- Need to encode for machine learning\n",
    "- Categories need hierarchical grouping\n",
    "- Memory optimization needed\n",
    "- Rare categories need grouping\n",
    "\n",
    "**Common categorical operations:**\n",
    "1. **Standardization**: Fix typos, case, spacing\n",
    "2. **Encoding**: Convert to numeric (label, one-hot, target)\n",
    "3. **Grouping**: Combine rare/similar categories\n",
    "4. **Validation**: Ensure only allowed values\n",
    "5. **Ordering**: Create ordinal categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. CATEGORICAL DATA CLEANING\n",
    "# ============================================\n",
    "\n",
    "# Create sample categorical data with issues\n",
    "categorical_data = {\n",
    "    'customer_id': range(1, 21),\n",
    "    'education': [\n",
    "        'High School', 'high school', 'HS', 'High Scool',\n",
    "        'Bachelor', 'bachelors', \"Bachelor's\", 'BS',\n",
    "        'Master', 'masters', \"Master's\", 'MS', 'MBA',\n",
    "        'PhD', 'phd', 'Doctorate',\n",
    "        'Associate', 'associates', 'AA',\n",
    "        'Some College'\n",
    "    ],\n",
    "    'income_level': [\n",
    "        'Low', 'low', 'LOW', 'Medium',\n",
    "        'medium', 'MEDIUM', 'High', 'high',\n",
    "        'HIGH', 'Very High', 'very high',\n",
    "        'Low', 'Medium', 'High',\n",
    "        'low', 'medium', 'high',\n",
    "        'Very Low', 'very low', 'Medium'\n",
    "    ],\n",
    "    'job_title': [\n",
    "        'Software Engineer', 'software engineer', 'SWE',\n",
    "        'Data Scientist', 'data scientist', 'DS',\n",
    "        'Product Manager', 'product manager', 'PM',\n",
    "        'Marketing Manager', 'marketing mgr', 'Mktg Mgr',\n",
    "        'Sales Rep', 'sales representative', 'Sales',\n",
    "        'CEO', 'Chief Executive Officer', 'ceo',\n",
    "        'Analyst', 'Data Analyst'\n",
    "    ],\n",
    "    'city': [\n",
    "        'New York', 'NYC', 'new york', 'New York City',\n",
    "        'Los Angeles', 'LA', 'los angeles',\n",
    "        'San Francisco', 'SF', 'san francisco',\n",
    "        'Chicago', 'chicago',\n",
    "        'Boston', 'boston',\n",
    "        'Seattle', 'seattle',\n",
    "        'Austin', 'austin',\n",
    "        'Denver', 'denver'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(categorical_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL DATA CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Original Categorical Data:\")\n",
    "print(df_categorical)\n",
    "\n",
    "print(\"\\n   Initial Category Counts:\")\n",
    "for col in ['education', 'income_level', 'job_title', 'city']:\n",
    "    print(f\"   {col}: {df_categorical[col].nunique()} unique values\")\n",
    "\n",
    "# 2. Standardize case and spacing\n",
    "print(\"\\n2. Standardizing Case and Spacing:\")\n",
    "\n",
    "df_clean = df_categorical.copy()\n",
    "\n",
    "# Standardize to title case\n",
    "for col in ['education', 'income_level', 'job_title', 'city']:\n",
    "    df_clean[f'{col}_std'] = df_clean[col].str.strip().str.title()\n",
    "\n",
    "print(\"   After case standardization:\")\n",
    "for col in ['education', 'income_level', 'job_title', 'city']:\n",
    "    print(f\"   {col}: {df_clean[f'{col}_std'].nunique()} unique values\")\n",
    "\n",
    "# 3. Fix typos and variations using mapping\n",
    "print(\"\\n3. Fixing Typos and Variations:\")\n",
    "\n",
    "# Education mapping\n",
    "education_mapping = {\n",
    "    'high school': 'High School',\n",
    "    'hs': 'High School',\n",
    "    'high scool': 'High School',  # typo\n",
    "    'bachelor': 'Bachelor\\'s Degree',\n",
    "    'bachelors': 'Bachelor\\'s Degree',\n",
    "    \"bachelor's\": 'Bachelor\\'s Degree',\n",
    "    'bs': 'Bachelor\\'s Degree',\n",
    "    'master': 'Master\\'s Degree',\n",
    "    'masters': 'Master\\'s Degree',\n",
    "    \"master's\": 'Master\\'s Degree',\n",
    "    'ms': 'Master\\'s Degree',\n",
    "    'mba': 'Master\\'s Degree',\n",
    "    'phd': 'Doctorate',\n",
    "    'doctorate': 'Doctorate',\n",
    "    'associate': 'Associate Degree',\n",
    "    'associates': 'Associate Degree',\n",
    "    'aa': 'Associate Degree',\n",
    "    'some college': 'Some College'\n",
    "}\n",
    "\n",
    "df_clean['education_clean'] = df_clean['education'].str.lower().map(education_mapping)\n",
    "df_clean['education_clean'] = df_clean['education_clean'].fillna(df_clean['education'])\n",
    "\n",
    "print(f\"   Education: {df_categorical['education'].nunique()} â†’ {df_clean['education_clean'].nunique()} categories\")\n",
    "print(f\"   Unique values: {sorted(df_clean['education_clean'].unique())}\")\n",
    "\n",
    "# Income level mapping\n",
    "income_mapping = {\n",
    "    'low': 'Low',\n",
    "    'very low': 'Low',\n",
    "    'medium': 'Medium',\n",
    "    'high': 'High',\n",
    "    'very high': 'High'\n",
    "}\n",
    "\n",
    "df_clean['income_clean'] = df_clean['income_level'].str.lower().map(income_mapping)\n",
    "\n",
    "print(f\"\\n   Income Level: {df_categorical['income_level'].nunique()} â†’ {df_clean['income_clean'].nunique()} categories\")\n",
    "print(f\"   Unique values: {sorted(df_clean['income_clean'].unique())}\")\n",
    "\n",
    "# 4. Standardize abbreviations\n",
    "print(\"\\n4. Standardizing Abbreviations:\")\n",
    "\n",
    "# Job title mapping\n",
    "job_mapping = {\n",
    "    'software engineer': 'Software Engineer',\n",
    "    'swe': 'Software Engineer',\n",
    "    'data scientist': 'Data Scientist',\n",
    "    'ds': 'Data Scientist',\n",
    "    'product manager': 'Product Manager',\n",
    "    'pm': 'Product Manager',\n",
    "    'marketing manager': 'Marketing Manager',\n",
    "    'marketing mgr': 'Marketing Manager',\n",
    "    'mktg mgr': 'Marketing Manager',\n",
    "    'sales rep': 'Sales Representative',\n",
    "    'sales representative': 'Sales Representative',\n",
    "    'sales': 'Sales Representative',\n",
    "    'ceo': 'Chief Executive Officer',\n",
    "    'chief executive officer': 'Chief Executive Officer',\n",
    "    'analyst': 'Analyst',\n",
    "    'data analyst': 'Data Analyst'\n",
    "}\n",
    "\n",
    "df_clean['job_title_clean'] = df_clean['job_title'].str.lower().map(job_mapping)\n",
    "df_clean['job_title_clean'] = df_clean['job_title_clean'].fillna(df_clean['job_title'])\n",
    "\n",
    "print(f\"   Job Title: {df_categorical['job_title'].nunique()} â†’ {df_clean['job_title_clean'].nunique()} categories\")\n",
    "\n",
    "# City mapping\n",
    "city_mapping = {\n",
    "    'new york': 'New York',\n",
    "    'nyc': 'New York',\n",
    "    'new york city': 'New York',\n",
    "    'los angeles': 'Los Angeles',\n",
    "    'la': 'Los Angeles',\n",
    "    'san francisco': 'San Francisco',\n",
    "    'sf': 'San Francisco',\n",
    "    'chicago': 'Chicago',\n",
    "    'boston': 'Boston',\n",
    "    'seattle': 'Seattle',\n",
    "    'austin': 'Austin',\n",
    "    'denver': 'Denver'\n",
    "}\n",
    "\n",
    "df_clean['city_clean'] = df_clean['city'].str.lower().map(city_mapping)\n",
    "\n",
    "print(f\"   City: {df_categorical['city'].nunique()} â†’ {df_clean['city_clean'].nunique()} categories\")\n",
    "\n",
    "# 5. Create hierarchical categories\n",
    "print(\"\\n5. Creating Hierarchical Categories:\")\n",
    "\n",
    "# Group education into broader categories\n",
    "def categorize_education(education):\n",
    "    if education in ['High School', 'Some College']:\n",
    "        return 'Undergraduate Level'\n",
    "    elif education == 'Associate Degree':\n",
    "        return 'Associate Level'\n",
    "    elif education == 'Bachelor\\'s Degree':\n",
    "        return 'Bachelor Level'\n",
    "    elif education == 'Master\\'s Degree':\n",
    "        return 'Graduate Level'\n",
    "    elif education == 'Doctorate':\n",
    "        return 'Doctoral Level'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_clean['education_level'] = df_clean['education_clean'].apply(categorize_education)\n",
    "\n",
    "print(\"   Education hierarchy:\")\n",
    "print(df_clean.groupby(['education_level', 'education_clean']).size())\n",
    "\n",
    "# 6. Handle rare categories\n",
    "print(\"\\n6. Handling Rare Categories:\")\n",
    "\n",
    "# Identify rare job titles (appearing less than 3 times)\n",
    "value_counts = df_clean['job_title_clean'].value_counts()\n",
    "rare_threshold = 3\n",
    "\n",
    "rare_jobs = value_counts[value_counts < rare_threshold].index.tolist()\n",
    "print(f\"   Rare job titles (< {rare_threshold} occurrences): {rare_jobs}\")\n",
    "\n",
    "# Group rare categories\n",
    "df_clean['job_title_grouped'] = df_clean['job_title_clean'].apply(\n",
    "    lambda x: 'Other' if x in rare_jobs else x\n",
    ")\n",
    "\n",
    "print(f\"\\n   Job titles before grouping: {df_clean['job_title_clean'].nunique()}\")\n",
    "print(f\"   Job titles after grouping: {df_clean['job_title_grouped'].nunique()}\")\n",
    "\n",
    "# 7. Label Encoding\n",
    "print(\"\\n7. Label Encoding (Ordinal):\")\n",
    "\n",
    "# Create ordinal encoding for education\n",
    "education_order = [\n",
    "    'High School',\n",
    "    'Some College',\n",
    "    'Associate Degree',\n",
    "    'Bachelor\\'s Degree',\n",
    "    'Master\\'s Degree',\n",
    "    'Doctorate'\n",
    "]\n",
    "\n",
    "df_clean['education_encoded'] = pd.Categorical(\n",
    "    df_clean['education_clean'],\n",
    "    categories=education_order,\n",
    "    ordered=True\n",
    ").codes\n",
    "\n",
    "print(\"   Education Label Encoding:\")\n",
    "encoding_map = df_clean[['education_clean', 'education_encoded']].drop_duplicates().sort_values('education_encoded')\n",
    "print(encoding_map)\n",
    "\n",
    "# Income level encoding\n",
    "income_order = ['Low', 'Medium', 'High']\n",
    "df_clean['income_encoded'] = pd.Categorical(\n",
    "    df_clean['income_clean'],\n",
    "    categories=income_order,\n",
    "    ordered=True\n",
    ").codes\n",
    "\n",
    "print(\"\\n   Income Level Label Encoding:\")\n",
    "print(df_clean[['income_clean', 'income_encoded']].drop_duplicates().sort_values('income_encoded'))\n",
    "\n",
    "# 8. One-Hot Encoding\n",
    "print(\"\\n8. One-Hot Encoding (Nominal):\")\n",
    "\n",
    "# One-hot encode city\n",
    "city_dummies = pd.get_dummies(df_clean['city_clean'], prefix='city')\n",
    "print(f\"   Created {len(city_dummies.columns)} binary columns for cities:\")\n",
    "print(f\"   {list(city_dummies.columns)}\")\n",
    "\n",
    "print(\"\\n   Sample of one-hot encoded data:\")\n",
    "print(city_dummies.head())\n",
    "\n",
    "# 9. Frequency Encoding\n",
    "print(\"\\n9. Frequency Encoding:\")\n",
    "\n",
    "# Encode based on frequency\n",
    "job_freq = df_clean['job_title_grouped'].value_counts(normalize=True)\n",
    "df_clean['job_frequency'] = df_clean['job_title_grouped'].map(job_freq)\n",
    "\n",
    "print(\"   Job Title Frequency Encoding:\")\n",
    "print(df_clean[['job_title_grouped', 'job_frequency']].drop_duplicates().sort_values('job_frequency', ascending=False))\n",
    "\n",
    "# 10. Convert to category dtype for memory optimization\n",
    "print(\"\\n10. Memory Optimization with Category Dtype:\")\n",
    "\n",
    "# Check memory before\n",
    "memory_before = df_clean[['education_clean', 'income_clean', 'job_title_clean', 'city_clean']].memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to category\n",
    "for col in ['education_clean', 'income_clean', 'job_title_grouped', 'city_clean']:\n",
    "    df_clean[f'{col}_cat'] = df_clean[col].astype('category')\n",
    "\n",
    "# Check memory after\n",
    "memory_after = df_clean[['education_clean_cat', 'income_clean_cat', 'job_title_grouped_cat', 'city_clean_cat']].memory_usage(deep=True).sum()\n",
    "\n",
    "print(f\"   Memory before: {memory_before:,} bytes\")\n",
    "print(f\"   Memory after: {memory_after:,} bytes\")\n",
    "print(f\"   Savings: {memory_before - memory_after:,} bytes ({(1 - memory_after/memory_before)*100:.1f}% reduction)\")\n",
    "\n",
    "# 11. Validate categories\n",
    "print(\"\\n11. Validating Categories:\")\n",
    "\n",
    "def validate_categories(df, column, allowed_values):\n",
    "    \"\"\"Check if all values are in allowed list\"\"\"\n",
    "    invalid = ~df[column].isin(allowed_values)\n",
    "    invalid_count = invalid.sum()\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'total': len(df),\n",
    "        'valid': len(df) - invalid_count,\n",
    "        'invalid': invalid_count,\n",
    "        'invalid_values': df[invalid][column].unique().tolist() if invalid_count > 0 else []\n",
    "    }\n",
    "\n",
    "allowed_education = list(education_order)\n",
    "validation = validate_categories(df_clean, 'education_clean', allowed_education)\n",
    "\n",
    "print(f\"   Education validation:\")\n",
    "print(f\"   Valid: {validation['valid']}/{validation['total']}\")\n",
    "if validation['invalid'] > 0:\n",
    "    print(f\"   Invalid values: {validation['invalid_values']}\")\n",
    "\n",
    "# 12. Summary\n",
    "print(\"\\n12. Categorical Cleaning Summary:\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Column': ['Education', 'Income Level', 'Job Title', 'City'],\n",
    "    'Original_Unique': [\n",
    "        df_categorical['education'].nunique(),\n",
    "        df_categorical['income_level'].nunique(),\n",
    "        df_categorical['job_title'].nunique(),\n",
    "        df_categorical['city'].nunique()\n",
    "    ],\n",
    "    'Cleaned_Unique': [\n",
    "        df_clean['education_clean'].nunique(),\n",
    "        df_clean['income_clean'].nunique(),\n",
    "        df_clean['job_title_clean'].nunique(),\n",
    "        df_clean['city_clean'].nunique()\n",
    "    ],\n",
    "    'Grouped_Unique': [\n",
    "        df_clean['education_level'].nunique(),\n",
    "        df_clean['income_clean'].nunique(),\n",
    "        df_clean['job_title_grouped'].nunique(),\n",
    "        df_clean['city_clean'].nunique()\n",
    "    ],\n",
    "    'Encoding': [\n",
    "        'Label (Ordinal)',\n",
    "        'Label (Ordinal)',\n",
    "        'Frequency',\n",
    "        'One-Hot'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Categorical data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323dce3d",
   "metadata": {},
   "source": [
    "## 11. Numerical Data Cleaning\n",
    "\n",
    "**What is it?**\n",
    "Numerical data cleaning involves handling errors, inconsistencies, and anomalies in numeric columns.\n",
    "\n",
    "**Why use it?**\n",
    "- Ensures accurate calculations\n",
    "- Handles precision and rounding issues\n",
    "- Converts units consistently\n",
    "- Removes impossible values\n",
    "- Prepares data for statistical analysis\n",
    "- Handles special numeric values (infinity, NaN)\n",
    "\n",
    "**When to use it?**\n",
    "- Mixed numeric formats present\n",
    "- Unit conversions needed\n",
    "- Precision issues exist\n",
    "- Negative values where impossible\n",
    "- Need to bin continuous variables\n",
    "- Scale differences across features\n",
    "\n",
    "**Common numerical operations:**\n",
    "1. **Type conversion**: String to numeric, handle errors\n",
    "2. **Unit standardization**: Convert to common units\n",
    "3. **Rounding**: Control decimal precision\n",
    "4. **Binning**: Discretize continuous variables\n",
    "5. **Validation**: Check ranges and constraints\n",
    "6. **Special values**: Handle infinity, NaN, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 11. NUMERICAL DATA CLEANING\n",
    "# ============================================\n",
    "\n",
    "# Create sample numerical data with issues\n",
    "numerical_data = {\n",
    "    'product_id': range(1, 16),\n",
    "    'price': [\n",
    "        '19.99', '29.99', 'N/A', '39.99',\n",
    "        '49.99', 'inf', '59.99', '69.99',\n",
    "        '-10.50', '79.99', '89.99', '99.99',\n",
    "        '109.9999', '119.99', '0'\n",
    "    ],\n",
    "    'quantity': [\n",
    "        100, 200, 150, 175,\n",
    "        -50, 225, 250, 275,\n",
    "        300, 999999, 350, 375,\n",
    "        400, 425, 0\n",
    "    ],\n",
    "    'weight': [\n",
    "        '1.5kg', '2.3 kg', '500g', '1200g',\n",
    "        '3.5 kg', '2kg', '800g', '1.8kg',\n",
    "        '2.5 kg', '3000g', '1.2 kg', '900g',\n",
    "        '2.2kg', '1.7 kg', '0.5kg'\n",
    "    ],\n",
    "    'rating': [\n",
    "        4.5, 4.7, 3.8, 4.2,\n",
    "        4.9, 5.5, 4.1, 3.9,\n",
    "        4.6, 0, 4.3, 4.8,\n",
    "        -1.5, 4.4, np.inf\n",
    "    ],\n",
    "    'discount_pct': [\n",
    "        '10%', '15%', '20%', '5%',\n",
    "        '0%', '25%', '30%', '12%',\n",
    "        '8%', '150%', '18%', '22%',\n",
    "        '-5%', '50%', '100%'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_numerical = pd.DataFrame(numerical_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NUMERICAL DATA CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Original Numerical Data:\")\n",
    "print(df_numerical)\n",
    "print(\"\\n   Data types:\")\n",
    "print(df_numerical.dtypes)\n",
    "\n",
    "# 2. Convert strings to numeric\n",
    "print(\"\\n2. Converting Strings to Numeric:\")\n",
    "\n",
    "df_clean = df_numerical.copy()\n",
    "\n",
    "# Convert price (handle 'N/A', 'inf', etc.)\n",
    "df_clean['price_numeric'] = pd.to_numeric(df_clean['price'], errors='coerce')\n",
    "\n",
    "print(\"   Price conversion:\")\n",
    "for orig, converted in zip(df_numerical['price'], df_clean['price_numeric']):\n",
    "    status = \"âœ“\" if pd.notna(converted) and np.isfinite(converted) else \"âœ—\"\n",
    "    print(f\"   {status} {str(orig):10} â†’ {converted}\")\n",
    "\n",
    "# Convert discount percentage\n",
    "df_clean['discount_numeric'] = df_clean['discount_pct'].str.replace('%', '').astype(float)\n",
    "\n",
    "print(\"\\n   Discount conversion:\")\n",
    "print(df_clean[['discount_pct', 'discount_numeric']].head(8))\n",
    "\n",
    "# 3. Handle special numeric values\n",
    "print(\"\\n3. Handling Special Numeric Values:\")\n",
    "\n",
    "# Check for infinity\n",
    "has_inf = np.isinf(df_clean['price_numeric']).sum()\n",
    "has_ninf = np.isneginf(df_clean['price_numeric']).sum()\n",
    "has_nan = df_clean['price_numeric'].isna().sum()\n",
    "\n",
    "print(f\"   Price column:\")\n",
    "print(f\"   - Infinity values: {has_inf}\")\n",
    "print(f\"   - Negative infinity: {has_ninf}\")\n",
    "print(f\"   - NaN values: {has_nan}\")\n",
    "\n",
    "# Replace infinity with NaN\n",
    "df_clean['price_clean'] = df_clean['price_numeric'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Replace NaN with median\n",
    "price_median = df_clean['price_clean'].median()\n",
    "df_clean['price_clean'] = df_clean['price_clean'].fillna(price_median)\n",
    "\n",
    "print(f\"   Replaced inf/NaN with median: ${price_median:.2f}\")\n",
    "\n",
    "# 4. Validate numeric ranges\n",
    "print(\"\\n4. Validating Numeric Ranges:\")\n",
    "\n",
    "# Price should be positive and < 500\n",
    "invalid_price = (df_clean['price_clean'] < 0) | (df_clean['price_clean'] > 500)\n",
    "print(f\"   Invalid prices: {invalid_price.sum()}\")\n",
    "\n",
    "# Rating should be 0-5\n",
    "invalid_rating = (df_numerical['rating'] < 0) | (df_numerical['rating'] > 5)\n",
    "print(f\"   Invalid ratings: {invalid_rating.sum()}\")\n",
    "if invalid_rating.sum() > 0:\n",
    "    print(\"   Invalid rating values:\")\n",
    "    print(df_numerical[invalid_rating][['product_id', 'rating']])\n",
    "\n",
    "# Discount should be 0-100%\n",
    "invalid_discount = (df_clean['discount_numeric'] < 0) | (df_clean['discount_numeric'] > 100)\n",
    "print(f\"   Invalid discounts: {invalid_discount.sum()}\")\n",
    "if invalid_discount.sum() > 0:\n",
    "    print(\"   Invalid discount values:\")\n",
    "    print(df_clean[invalid_discount][['product_id', 'discount_pct', 'discount_numeric']])\n",
    "\n",
    "# 5. Handle negative values\n",
    "print(\"\\n5. Handling Negative Values:\")\n",
    "\n",
    "# Quantity should be non-negative\n",
    "negative_qty = df_clean['quantity'] < 0\n",
    "print(f\"   Negative quantities: {negative_qty.sum()}\")\n",
    "\n",
    "# Method 1: Replace with 0\n",
    "df_clean['quantity_clean'] = df_clean['quantity'].clip(lower=0)\n",
    "\n",
    "# Method 2: Take absolute value (if negative means return)\n",
    "df_clean['quantity_abs'] = df_clean['quantity'].abs()\n",
    "\n",
    "print(\"   Quantity handling:\")\n",
    "print(df_clean[['quantity', 'quantity_clean', 'quantity_abs']].head(10))\n",
    "\n",
    "# 6. Standardize units (weight to kg)\n",
    "print(\"\\n6. Standardizing Units (Weight to kg):\")\n",
    "\n",
    "def convert_weight_to_kg(weight):\n",
    "    \"\"\"Convert various weight formats to kg\"\"\"\n",
    "    weight_str = str(weight).lower().strip()\n",
    "    \n",
    "    # Extract number\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', weight_str)\n",
    "    if not numbers:\n",
    "        return np.nan\n",
    "    \n",
    "    value = float(numbers[0])\n",
    "    \n",
    "    # Convert based on unit\n",
    "    if 'g' in weight_str and 'kg' not in weight_str:\n",
    "        value = value / 1000  # grams to kg\n",
    "    elif 'lb' in weight_str or 'pound' in weight_str:\n",
    "        value = value * 0.453592  # pounds to kg\n",
    "    elif 'oz' in weight_str:\n",
    "        value = value * 0.0283495  # ounces to kg\n",
    "    \n",
    "    return round(value, 3)\n",
    "\n",
    "df_clean['weight_kg'] = df_clean['weight'].apply(convert_weight_to_kg)\n",
    "\n",
    "print(\"   Weight standardization:\")\n",
    "for orig, converted in zip(df_numerical['weight'], df_clean['weight_kg']):\n",
    "    print(f\"   {orig:10} â†’ {converted} kg\")\n",
    "\n",
    "# 7. Round and control precision\n",
    "print(\"\\n7. Controlling Precision:\")\n",
    "\n",
    "# Round price to 2 decimals\n",
    "df_clean['price_rounded'] = df_clean['price_clean'].round(2)\n",
    "\n",
    "# Round rating to 1 decimal\n",
    "df_clean['rating_clean'] = pd.to_numeric(df_numerical['rating'], errors='coerce')\n",
    "df_clean['rating_clean'] = df_clean['rating_clean'].clip(0, 5).round(1)\n",
    "\n",
    "print(\"   Precision control:\")\n",
    "print(df_clean[['price_clean', 'price_rounded', 'rating_clean']].head(10))\n",
    "\n",
    "# 8. Bin continuous variables\n",
    "print(\"\\n8. Binning Continuous Variables:\")\n",
    "\n",
    "# Bin prices into categories\n",
    "price_bins = [0, 30, 60, 90, 150]\n",
    "price_labels = ['Budget', 'Mid-Range', 'Premium', 'Luxury']\n",
    "df_clean['price_category'] = pd.cut(df_clean['price_rounded'], bins=price_bins, labels=price_labels)\n",
    "\n",
    "print(\"   Price categories:\")\n",
    "print(df_clean[['price_rounded', 'price_category']].head(10))\n",
    "print(\"\\n   Price category distribution:\")\n",
    "print(df_clean['price_category'].value_counts().sort_index())\n",
    "\n",
    "# Bin ratings\n",
    "rating_bins = [0, 2, 3, 4, 5]\n",
    "rating_labels = ['Poor', 'Fair', 'Good', 'Excellent']\n",
    "df_clean['rating_category'] = pd.cut(df_clean['rating_clean'], bins=rating_bins, labels=rating_labels)\n",
    "\n",
    "print(\"\\n   Rating categories:\")\n",
    "print(df_clean['rating_category'].value_counts().sort_index())\n",
    "\n",
    "# 9. Handle extreme outliers\n",
    "print(\"\\n9. Handling Extreme Outliers:\")\n",
    "\n",
    "# Identify extreme quantities\n",
    "q1 = df_clean['quantity_clean'].quantile(0.25)\n",
    "q3 = df_clean['quantity_clean'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 3 * iqr  # 3 IQR for extreme outliers\n",
    "upper_bound = q3 + 3 * iqr\n",
    "\n",
    "extreme_outliers = (df_clean['quantity_clean'] < lower_bound) | (df_clean['quantity_clean'] > upper_bound)\n",
    "print(f\"   Extreme quantity outliers: {extreme_outliers.sum()}\")\n",
    "\n",
    "if extreme_outliers.sum() > 0:\n",
    "    print(\"   Extreme values:\")\n",
    "    print(df_clean[extreme_outliers][['product_id', 'quantity_clean']])\n",
    "\n",
    "# Cap extreme values\n",
    "df_clean['quantity_capped'] = df_clean['quantity_clean'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(f\"\\n   Capped at: {lower_bound:.0f} - {upper_bound:.0f}\")\n",
    "\n",
    "# 10. Handle zeros\n",
    "print(\"\\n10. Handling Zero Values:\")\n",
    "\n",
    "zero_price = (df_clean['price_rounded'] == 0).sum()\n",
    "zero_qty = (df_clean['quantity_clean'] == 0).sum()\n",
    "zero_rating = (df_clean['rating_clean'] == 0).sum()\n",
    "\n",
    "print(f\"   Zero prices: {zero_price}\")\n",
    "print(f\"   Zero quantities: {zero_qty}\")\n",
    "print(f\"   Zero ratings: {zero_rating}\")\n",
    "\n",
    "# Replace zero prices with minimum non-zero price\n",
    "min_nonzero_price = df_clean[df_clean['price_rounded'] > 0]['price_rounded'].min()\n",
    "df_clean['price_final'] = df_clean['price_rounded'].replace(0, min_nonzero_price)\n",
    "\n",
    "print(f\"   Replaced zero prices with minimum: ${min_nonzero_price:.2f}\")\n",
    "\n",
    "# 11. Create derived numeric features\n",
    "print(\"\\n11. Creating Derived Numeric Features:\")\n",
    "\n",
    "# Calculate discounted price\n",
    "df_clean['discount_decimal'] = df_clean['discount_numeric'] / 100\n",
    "df_clean['discounted_price'] = df_clean['price_final'] * (1 - df_clean['discount_decimal'])\n",
    "df_clean['savings'] = df_clean['price_final'] - df_clean['discounted_price']\n",
    "\n",
    "print(\"   Price calculations:\")\n",
    "print(df_clean[['price_final', 'discount_numeric', 'discounted_price', 'savings']].head(8))\n",
    "\n",
    "# Calculate total value\n",
    "df_clean['total_value'] = df_clean['discounted_price'] * df_clean['quantity_clean']\n",
    "\n",
    "print(\"\\n   Total value per product:\")\n",
    "print(df_clean[['product_id', 'quantity_clean', 'discounted_price', 'total_value']].head(8))\n",
    "\n",
    "# 12. Normalize and compare\n",
    "print(\"\\n12. Normalization Examples:\")\n",
    "\n",
    "# Min-max normalization (0-1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_clean['price_normalized'] = scaler.fit_transform(df_clean[['price_final']])\n",
    "\n",
    "# Z-score normalization\n",
    "df_clean['price_zscore'] = (df_clean['price_final'] - df_clean['price_final'].mean()) / df_clean['price_final'].std()\n",
    "\n",
    "print(\"   Price normalization:\")\n",
    "print(df_clean[['price_final', 'price_normalized', 'price_zscore']].head(8))\n",
    "\n",
    "# 13. Summary statistics\n",
    "print(\"\\n13. Summary Statistics:\")\n",
    "\n",
    "numeric_cols = ['price_final', 'quantity_clean', 'weight_kg', 'rating_clean', 'discount_numeric']\n",
    "summary_stats = df_clean[numeric_cols].describe()\n",
    "\n",
    "print(summary_stats)\n",
    "\n",
    "# 14. Final validation\n",
    "print(\"\\n14. Final Data Validation:\")\n",
    "\n",
    "validation_results = {\n",
    "    'Column': [],\n",
    "    'Min': [],\n",
    "    'Max': [],\n",
    "    'Mean': [],\n",
    "    'Missing': [],\n",
    "    'Invalid': []\n",
    "}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    validation_results['Column'].append(col)\n",
    "    validation_results['Min'].append(f\"{df_clean[col].min():.2f}\")\n",
    "    validation_results['Max'].append(f\"{df_clean[col].max():.2f}\")\n",
    "    validation_results['Mean'].append(f\"{df_clean[col].mean():.2f}\")\n",
    "    validation_results['Missing'].append(df_clean[col].isna().sum())\n",
    "    \n",
    "    # Count invalid based on expected ranges\n",
    "    if col == 'rating_clean':\n",
    "        invalid = ((df_clean[col] < 0) | (df_clean[col] > 5)).sum()\n",
    "    elif col == 'discount_numeric':\n",
    "        invalid = ((df_clean[col] < 0) | (df_clean[col] > 100)).sum()\n",
    "    else:\n",
    "        invalid = (df_clean[col] < 0).sum()\n",
    "    \n",
    "    validation_results['Invalid'].append(invalid)\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(validation_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Numerical data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c913f",
   "metadata": {},
   "source": [
    "## 12. Text Data Cleaning\n",
    "\n",
    "**What is it?**\n",
    "Text data cleaning involves preprocessing and standardizing unstructured text for analysis, NLP, or machine learning.\n",
    "\n",
    "**Why use it?**\n",
    "- Improves text analysis accuracy\n",
    "- Reduces noise in text data\n",
    "- Standardizes text format\n",
    "- Prepares data for NLP models\n",
    "- Removes irrelevant information\n",
    "- Handles encoding issues\n",
    "\n",
    "**When to use it?**\n",
    "- Working with user-generated content\n",
    "- Natural Language Processing (NLP)\n",
    "- Text mining and sentiment analysis\n",
    "- Search and information retrieval\n",
    "- Text classification tasks\n",
    "- Topic modeling\n",
    "\n",
    "**Common text cleaning operations:**\n",
    "1. **Lowercasing**: Standardize case\n",
    "2. **Removing punctuation**: Clean special characters\n",
    "3. **Removing stopwords**: Filter common words\n",
    "4. **Tokenization**: Split into words/sentences\n",
    "5. **Stemming/Lemmatization**: Reduce words to root form\n",
    "6. **Removing HTML/URLs**: Clean web content\n",
    "7. **Handling emojis/special characters**\n",
    "8. **Spell checking and correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 12. TEXT DATA CLEANING\n",
    "# ============================================\n",
    "\n",
    "import string\n",
    "\n",
    "# Create sample text data with various issues\n",
    "text_data = {\n",
    "    'review_id': range(1, 11),\n",
    "    'review_text': [\n",
    "        'GREAT product!!! Really LOVE it!!! ðŸ˜ðŸ˜ðŸ˜',\n",
    "        '   Bad quality...    disappointing â˜¹ï¸  ',\n",
    "        'Visit our website: https://example.com for more info!!!',\n",
    "        'Email us at support@example.com for help',\n",
    "        '<p>HTML tags should be <b>removed</b></p>',\n",
    "        'Too many    spaces   and\\n\\nnewlines\\n',\n",
    "        'Misspelled: recieve, occured, definately',\n",
    "        'Abbreviations: u r so gr8! lol omg btw',\n",
    "        '$$$SPECIAL OFFER$$$ Buy NOW!!! 50% OFF!!!',\n",
    "        'Mixed languages: This is English. C\\'est franÃ§ais. ã“ã‚Œã¯æ—¥æœ¬èªžã§ã™ã€‚'\n",
    "    ],\n",
    "    'comment': [\n",
    "        'Good product',\n",
    "        'Not recommended...',\n",
    "        'Check website',\n",
    "        'Contact support',\n",
    "        'HTML content',\n",
    "        'Whitespace issues',\n",
    "        'Spelling errors',\n",
    "        'SMS language',\n",
    "        'Spam content',\n",
    "        'Multilingual'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_text = pd.DataFrame(text_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEXT DATA CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Original Text Data:\")\n",
    "for idx, row in df_text.iterrows():\n",
    "    print(f\"\\n{idx + 1}. {row['comment']}:\")\n",
    "    print(f\"   '{row['review_text']}'\")\n",
    "\n",
    "# 2. Basic text cleaning\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. Basic Text Cleaning:\")\n",
    "\n",
    "df_clean = df_text.copy()\n",
    "\n",
    "# Lowercase\n",
    "df_clean['text_lower'] = df_clean['review_text'].str.lower()\n",
    "\n",
    "# Strip whitespace\n",
    "df_clean['text_stripped'] = df_clean['text_lower'].str.strip()\n",
    "\n",
    "print(\"   After lowercase and strip:\")\n",
    "print(df_clean[['comment', 'text_stripped']].head(5))\n",
    "\n",
    "# 3. Remove URLs\n",
    "print(\"\\n3. Removing URLs:\")\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from text\"\"\"\n",
    "    # Pattern for http/https URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "df_clean['text_no_urls'] = df_clean['text_stripped'].apply(remove_urls)\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for orig, cleaned in zip(df_clean['text_stripped'].head(5), df_clean['text_no_urls'].head(5)):\n",
    "    if orig != cleaned:\n",
    "        print(f\"   '{orig}'\")\n",
    "        print(f\"   â†’ '{cleaned}'\")\n",
    "\n",
    "# 4. Remove emails\n",
    "print(\"\\n4. Removing Email Addresses:\")\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\"Remove email addresses from text\"\"\"\n",
    "    email_pattern = r'\\S+@\\S+'\n",
    "    return re.sub(email_pattern, '', text)\n",
    "\n",
    "df_clean['text_no_emails'] = df_clean['text_no_urls'].apply(remove_emails)\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for orig, cleaned in zip(df_clean['text_no_urls'][3:5], df_clean['text_no_emails'][3:5]):\n",
    "    if orig != cleaned:\n",
    "        print(f\"   '{orig}'\")\n",
    "        print(f\"   â†’ '{cleaned}'\")\n",
    "\n",
    "# 5. Remove HTML tags\n",
    "print(\"\\n5. Removing HTML Tags:\")\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"Remove HTML tags from text\"\"\"\n",
    "    html_pattern = r'<[^>]+>'\n",
    "    return re.sub(html_pattern, '', text)\n",
    "\n",
    "df_clean['text_no_html'] = df_clean['text_no_emails'].apply(remove_html)\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for orig, cleaned in zip(df_clean['text_no_emails'][4:6], df_clean['text_no_html'][4:6]):\n",
    "    if orig != cleaned:\n",
    "        print(f\"   '{orig}'\")\n",
    "        print(f\"   â†’ '{cleaned}'\")\n",
    "\n",
    "# 6. Remove extra whitespace\n",
    "print(\"\\n6. Removing Extra Whitespace:\")\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"Remove extra spaces, tabs, newlines\"\"\"\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "df_clean['text_clean_ws'] = df_clean['text_no_html'].apply(remove_extra_whitespace)\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for orig, cleaned in zip(df_clean['text_no_html'][5:7], df_clean['text_clean_ws'][5:7]):\n",
    "    if orig != cleaned:\n",
    "        print(f\"   '{orig}'\")\n",
    "        print(f\"   â†’ '{cleaned}'\")\n",
    "\n",
    "# 7. Remove punctuation\n",
    "print(\"\\n7. Removing Punctuation:\")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df_clean['text_no_punct'] = df_clean['text_clean_ws'].apply(remove_punctuation)\n",
    "\n",
    "print(\"   Sample with punctuation removed:\")\n",
    "print(df_clean[['comment', 'text_no_punct']].head(5))\n",
    "\n",
    "# 8. Remove numbers\n",
    "print(\"\\n8. Removing Numbers:\")\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\"Remove numbers from text\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "df_clean['text_no_numbers'] = df_clean['text_no_punct'].apply(remove_numbers)\n",
    "\n",
    "print(\"   After removing numbers:\")\n",
    "print(df_clean[['comment', 'text_no_numbers']].head(5))\n",
    "\n",
    "# 9. Remove emojis\n",
    "print(\"\\n9. Removing Emojis and Special Characters:\")\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis and special characters\"\"\"\n",
    "    # Pattern for emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df_clean['text_no_emojis'] = df_clean['text_clean_ws'].apply(remove_emojis)\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for idx in [0, 1]:\n",
    "    print(f\"   '{df_clean['text_clean_ws'].iloc[idx]}'\")\n",
    "    print(f\"   â†’ '{df_clean['text_no_emojis'].iloc[idx]}'\")\n",
    "\n",
    "# 10. Expand abbreviations\n",
    "print(\"\\n10. Expanding Common Abbreviations:\")\n",
    "\n",
    "abbreviations = {\n",
    "    ' u ': ' you ',\n",
    "    ' r ': ' are ',\n",
    "    ' gr8 ': ' great ',\n",
    "    ' lol ': ' laughing out loud ',\n",
    "    ' omg ': ' oh my god ',\n",
    "    ' btw ': ' by the way ',\n",
    "    ' asap ': ' as soon as possible ',\n",
    "    ' fyi ': ' for your information ',\n",
    "}\n",
    "\n",
    "def expand_abbreviations(text, abbrev_dict):\n",
    "    \"\"\"Expand common abbreviations\"\"\"\n",
    "    text = ' ' + text + ' '  # Add spaces for boundary matching\n",
    "    for abbrev, full in abbrev_dict.items():\n",
    "        text = text.replace(abbrev, full)\n",
    "    return text.strip()\n",
    "\n",
    "df_clean['text_expanded'] = df_clean['text_no_punct'].apply(\n",
    "    lambda x: expand_abbreviations(x, abbreviations)\n",
    ")\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "print(f\"   '{df_clean['text_no_punct'].iloc[7]}'\")\n",
    "print(f\"   â†’ '{df_clean['text_expanded'].iloc[7]}'\")\n",
    "\n",
    "# 11. Remove stopwords\n",
    "print(\"\\n11. Removing Stopwords:\")\n",
    "\n",
    "# Common English stopwords\n",
    "stopwords = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
    "    'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
    "    'to', 'was', 'will', 'with', 'this', 'but', 'they', 'have', 'had',\n",
    "    'what', 'when', 'where', 'who', 'which', 'why', 'how'\n",
    "}\n",
    "\n",
    "def remove_stopwords(text, stopwords_set):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_set]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df_clean['text_no_stopwords'] = df_clean['text_no_numbers'].apply(\n",
    "    lambda x: remove_stopwords(x, stopwords)\n",
    ")\n",
    "\n",
    "print(\"   Before â†’ After:\")\n",
    "for idx in [0, 1]:\n",
    "    print(f\"   '{df_clean['text_no_numbers'].iloc[idx]}'\")\n",
    "    print(f\"   â†’ '{df_clean['text_no_stopwords'].iloc[idx]}'\")\n",
    "\n",
    "# 12. Tokenization\n",
    "print(\"\\n12. Tokenization:\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Split text into tokens (words)\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "df_clean['tokens'] = df_clean['text_no_stopwords'].apply(tokenize_text)\n",
    "\n",
    "print(\"   Tokenized text (first 3):\")\n",
    "for idx in range(3):\n",
    "    print(f\"   {idx + 1}. {df_clean['tokens'].iloc[idx]}\")\n",
    "\n",
    "# 13. Count features\n",
    "print(\"\\n13. Text Feature Extraction:\")\n",
    "\n",
    "# Character count\n",
    "df_clean['char_count'] = df_clean['review_text'].str.len()\n",
    "\n",
    "# Word count\n",
    "df_clean['word_count'] = df_clean['review_text'].str.split().str.len()\n",
    "\n",
    "# Unique word count\n",
    "df_clean['unique_word_count'] = df_clean['tokens'].apply(lambda x: len(set(x)))\n",
    "\n",
    "# Average word length\n",
    "df_clean['avg_word_length'] = df_clean['review_text'].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()] if x.split() else [0])\n",
    ")\n",
    "\n",
    "print(\"   Text statistics:\")\n",
    "print(df_clean[['comment', 'char_count', 'word_count', 'unique_word_count', 'avg_word_length']].head(8))\n",
    "\n",
    "# 14. Detect language/encoding issues\n",
    "print(\"\\n14. Handling Encoding Issues:\")\n",
    "\n",
    "def keep_ascii_only(text):\n",
    "    \"\"\"Keep only ASCII characters\"\"\"\n",
    "    return ''.join(char for char in text if ord(char) < 128)\n",
    "\n",
    "df_clean['text_ascii'] = df_clean['text_clean_ws'].apply(keep_ascii_only)\n",
    "\n",
    "print(\"   Before â†’ After (non-ASCII removal):\")\n",
    "print(f\"   '{df_clean['text_clean_ws'].iloc[9]}'\")\n",
    "print(f\"   â†’ '{df_clean['text_ascii'].iloc[9]}'\")\n",
    "\n",
    "# 15. Create comprehensive text cleaning pipeline\n",
    "print(\"\\n15. Comprehensive Text Cleaning Pipeline:\")\n",
    "\n",
    "def clean_text_pipeline(text, \n",
    "                        lowercase=True,\n",
    "                        remove_url=True,\n",
    "                        remove_email=True,\n",
    "                        remove_html_tags=True,\n",
    "                        remove_punct=True,\n",
    "                        remove_num=True,\n",
    "                        remove_emoji=True,\n",
    "                        remove_extra_ws=True,\n",
    "                        remove_stops=False):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - text: Input text string\n",
    "    - lowercase: Convert to lowercase\n",
    "    - remove_url: Remove URLs\n",
    "    - remove_email: Remove email addresses\n",
    "    - remove_html_tags: Remove HTML tags\n",
    "    - remove_punct: Remove punctuation\n",
    "    - remove_num: Remove numbers\n",
    "    - remove_emoji: Remove emojis\n",
    "    - remove_extra_ws: Remove extra whitespace\n",
    "    - remove_stops: Remove stopwords\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    if remove_url:\n",
    "        text = remove_urls(text)\n",
    "    \n",
    "    # Remove emails\n",
    "    if remove_email:\n",
    "        text = remove_emails(text)\n",
    "    \n",
    "    # Remove HTML\n",
    "    if remove_html_tags:\n",
    "        text = remove_html(text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    if remove_emoji:\n",
    "        text = remove_emojis(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punct:\n",
    "        text = remove_punctuation(text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_num:\n",
    "        text = remove_numbers(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stops:\n",
    "        text = remove_stopwords(text, stopwords)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if remove_extra_ws:\n",
    "        text = remove_extra_whitespace(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply pipeline\n",
    "df_clean['text_final'] = df_clean['review_text'].apply(clean_text_pipeline)\n",
    "\n",
    "print(\"\\n   Original â†’ Final Cleaned Text:\")\n",
    "for idx in range(len(df_clean)):\n",
    "    print(f\"\\n   {idx + 1}. {df_clean['comment'].iloc[idx]}:\")\n",
    "    print(f\"   Original: '{df_clean['review_text'].iloc[idx]}'\")\n",
    "    print(f\"   Cleaned:  '{df_clean['text_final'].iloc[idx]}'\")\n",
    "\n",
    "# 16. Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"16. Text Cleaning Summary:\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Operation': [\n",
    "        'Lowercase',\n",
    "        'Remove URLs',\n",
    "        'Remove Emails',\n",
    "        'Remove HTML',\n",
    "        'Remove Emojis',\n",
    "        'Remove Punctuation',\n",
    "        'Remove Numbers',\n",
    "        'Remove Extra Whitespace',\n",
    "        'Remove Stopwords',\n",
    "        'Tokenization'\n",
    "    ],\n",
    "    'Purpose': [\n",
    "        'Standardize case',\n",
    "        'Remove web links',\n",
    "        'Remove email addresses',\n",
    "        'Clean HTML content',\n",
    "        'Remove emoji characters',\n",
    "        'Remove special characters',\n",
    "        'Remove numeric values',\n",
    "        'Clean spacing',\n",
    "        'Remove common words',\n",
    "        'Split into words'\n",
    "    ],\n",
    "    'Recommended_For': [\n",
    "        'All text analysis',\n",
    "        'User content, reviews',\n",
    "        'User content',\n",
    "        'Web-scraped data',\n",
    "        'Social media data',\n",
    "        'NLP, topic modeling',\n",
    "        'Text classification',\n",
    "        'All text analysis',\n",
    "        'NLP, keyword extraction',\n",
    "        'NLP, word analysis'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Text data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f72d45",
   "metadata": {},
   "source": [
    "## 13. Standardization & Normalization\n",
    "\n",
    "**What is it?**\n",
    "Standardization and normalization are techniques to scale numeric data to a common range or distribution.\n",
    "\n",
    "**Why use it?**\n",
    "- Makes features comparable\n",
    "- Improves ML model performance\n",
    "- Required for distance-based algorithms\n",
    "- Speeds up gradient descent\n",
    "- Prevents features from dominating\n",
    "- Handles different units/scales\n",
    "\n",
    "**When to use it?**\n",
    "- Before machine learning (especially SVM, KNN, neural networks)\n",
    "- Features have different scales\n",
    "- Distance-based algorithms\n",
    "- Gradient descent optimization\n",
    "- Comparing variables with different units\n",
    "- PCA and clustering\n",
    "\n",
    "**Common techniques:**\n",
    "1. **Min-Max Scaling**: Scale to [0, 1] or custom range\n",
    "2. **Z-Score (Standardization)**: Mean=0, Std=1\n",
    "3. **Robust Scaling**: Uses median and IQR (outlier-resistant)\n",
    "4. **Log Transformation**: Handle skewed data\n",
    "5. **Box-Cox**: Power transformation\n",
    "6. **Unit Vector**: Scale to unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff167801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 13. STANDARDIZATION & NORMALIZATION\n",
    "# ============================================\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n",
    "from scipy import stats\n",
    "\n",
    "# Create sample data with different scales\n",
    "np.random.seed(42)\n",
    "scaling_data = {\n",
    "    'age': np.random.randint(18, 80, 20),\n",
    "    'salary': np.random.randint(30000, 150000, 20),\n",
    "    'years_experience': np.random.randint(0, 30, 20),\n",
    "    'rating': np.random.uniform(1, 5, 20),\n",
    "    'transactions': np.random.randint(1, 1000, 20)\n",
    "}\n",
    "\n",
    "df_scaling = pd.DataFrame(scaling_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STANDARDIZATION & NORMALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Original Data (Different Scales):\")\n",
    "print(df_scaling.head(10))\n",
    "\n",
    "print(\"\\n   Descriptive Statistics:\")\n",
    "print(df_scaling.describe())\n",
    "\n",
    "# 2. Min-Max Scaling (Normalization)\n",
    "print(\"\\n2. Min-Max Scaling (0 to 1):\")\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(df_scaling),\n",
    "    columns=[f'{col}_minmax' for col in df_scaling.columns]\n",
    ")\n",
    "\n",
    "print(\"   Formula: X_scaled = (X - X_min) / (X_max - X_min)\")\n",
    "print(\"\\n   Min-Max scaled data:\")\n",
    "print(df_minmax.head(10))\n",
    "\n",
    "print(\"\\n   Min-Max statistics:\")\n",
    "print(df_minmax.describe())\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Original distributions\n",
    "for idx, col in enumerate(['age', 'salary', 'years_experience']):\n",
    "    df_scaling[col].hist(bins=15, ax=axes[0, idx], edgecolor='black')\n",
    "    axes[0, idx].set_title(f'Original: {col}')\n",
    "    axes[0, idx].set_xlabel(col)\n",
    "    axes[0, idx].set_ylabel('Frequency')\n",
    "\n",
    "# Min-Max scaled distributions\n",
    "for idx, col in enumerate(['age', 'salary', 'years_experience']):\n",
    "    df_minmax[f'{col}_minmax'].hist(bins=15, ax=axes[1, idx], edgecolor='black', color='green')\n",
    "    axes[1, idx].set_title(f'Min-Max Scaled: {col}')\n",
    "    axes[1, idx].set_xlabel(f'{col} (scaled)')\n",
    "    axes[1, idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Custom range Min-Max Scaling\n",
    "print(\"\\n3. Min-Max Scaling (Custom Range: -1 to 1):\")\n",
    "\n",
    "scaler_custom = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_custom = pd.DataFrame(\n",
    "    scaler_custom.fit_transform(df_scaling),\n",
    "    columns=[f'{col}_custom' for col in df_scaling.columns]\n",
    ")\n",
    "\n",
    "print(\"   Scaled to [-1, 1] range:\")\n",
    "print(df_custom.head(10))\n",
    "print(f\"\\n   Min: {df_custom.min().min():.2f}, Max: {df_custom.max().max():.2f}\")\n",
    "\n",
    "# 4. Z-Score Standardization\n",
    "print(\"\\n4. Z-Score Standardization (StandardScaler):\")\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "df_standard = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(df_scaling),\n",
    "    columns=[f'{col}_std' for col in df_scaling.columns]\n",
    ")\n",
    "\n",
    "print(\"   Formula: X_scaled = (X - mean) / std_dev\")\n",
    "print(\"\\n   Standardized data:\")\n",
    "print(df_standard.head(10))\n",
    "\n",
    "print(\"\\n   Standardized statistics (mean â‰ˆ 0, std â‰ˆ 1):\")\n",
    "print(df_standard.describe())\n",
    "\n",
    "# Visualize Z-score transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Salary before\n",
    "df_scaling['salary'].hist(bins=15, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('Original Salary Distribution')\n",
    "axes[0].set_xlabel('Salary')\n",
    "axes[0].axvline(df_scaling['salary'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].legend()\n",
    "\n",
    "# Salary after\n",
    "df_standard['salary_std'].hist(bins=15, ax=axes[1], edgecolor='black', color='green')\n",
    "axes[1].set_title('Standardized Salary (Z-Score)')\n",
    "axes[1].set_xlabel('Salary (z-score)')\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Mean = 0')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Robust Scaling\n",
    "print(\"\\n5. Robust Scaling (Outlier-Resistant):\")\n",
    "\n",
    "# Add some outliers\n",
    "df_with_outliers = df_scaling.copy()\n",
    "df_with_outliers.loc[0, 'salary'] = 500000  # Outlier\n",
    "\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust = pd.DataFrame(\n",
    "    scaler_robust.fit_transform(df_with_outliers),\n",
    "    columns=[f'{col}_robust' for col in df_scaling.columns]\n",
    ")\n",
    "\n",
    "print(\"   Formula: X_scaled = (X - median) / IQR\")\n",
    "print(\"   Uses median and IQR instead of mean and std\")\n",
    "print(\"\\n   Robust scaled data (with outlier):\")\n",
    "print(df_robust.head(10))\n",
    "\n",
    "# Compare StandardScaler vs RobustScaler with outliers\n",
    "df_standard_outlier = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(df_with_outliers),\n",
    "    columns=[f'{col}_std' for col in df_scaling.columns]\n",
    ")\n",
    "\n",
    "print(\"\\n   Comparison with outlier:\")\n",
    "print(\"   StandardScaler:\")\n",
    "print(f\"   Salary mean: {df_standard_outlier['salary_std'].mean():.2f}\")\n",
    "print(f\"   Salary std: {df_standard_outlier['salary_std'].std():.2f}\")\n",
    "print(\"\\n   RobustScaler:\")\n",
    "print(f\"   Salary median: {df_robust['salary_robust'].median():.2f}\")\n",
    "print(f\"   Salary IQR: {df_robust['salary_robust'].quantile(0.75) - df_robust['salary_robust'].quantile(0.25):.2f}\")\n",
    "\n",
    "# 6. Log Transformation\n",
    "print(\"\\n6. Log Transformation (for Skewed Data):\")\n",
    "\n",
    "# Create skewed data\n",
    "skewed_data = {\n",
    "    'income': [20000, 25000, 30000, 35000, 40000, 50000, 60000, 80000, 120000, 500000]\n",
    "}\n",
    "df_skewed = pd.DataFrame(skewed_data)\n",
    "\n",
    "# Apply log transformation\n",
    "df_skewed['income_log'] = np.log1p(df_skewed['income'])  # log1p = log(1 + x)\n",
    "\n",
    "# Calculate skewness\n",
    "skew_before = stats.skew(df_skewed['income'])\n",
    "skew_after = stats.skew(df_skewed['income_log'])\n",
    "\n",
    "print(f\"   Skewness before log: {skew_before:.2f}\")\n",
    "print(f\"   Skewness after log: {skew_after:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df_skewed['income'].hist(bins=10, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title(f'Original Income (Skew: {skew_before:.2f})')\n",
    "axes[0].set_xlabel('Income')\n",
    "\n",
    "df_skewed['income_log'].hist(bins=10, ax=axes[1], edgecolor='black', color='green')\n",
    "axes[1].set_title(f'Log-Transformed Income (Skew: {skew_after:.2f})')\n",
    "axes[1].set_xlabel('Log(Income)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Box-Cox Transformation\n",
    "print(\"\\n7. Box-Cox Transformation:\")\n",
    "\n",
    "# Box-Cox requires positive values\n",
    "positive_data = df_scaling['age'].copy()\n",
    "\n",
    "# Apply Box-Cox\n",
    "transformed_data, lambda_param = stats.boxcox(positive_data)\n",
    "\n",
    "print(f\"   Optimal lambda: {lambda_param:.4f}\")\n",
    "print(f\"   Skewness before: {stats.skew(positive_data):.4f}\")\n",
    "print(f\"   Skewness after: {stats.skew(transformed_data):.4f}\")\n",
    "\n",
    "# 8. Unit Vector Normalization (L2 Norm)\n",
    "print(\"\\n8. Unit Vector Normalization (L2 Norm):\")\n",
    "\n",
    "normalizer = Normalizer(norm='l2')\n",
    "df_normalized = pd.DataFrame(\n",
    "    normalizer.fit_transform(df_scaling),\n",
    "    columns=[f'{col}_l2' for col in df_scaling.columns]\n",
    ")\n",
    "\n",
    "print(\"   Formula: X_scaled = X / ||X||_2\")\n",
    "print(\"   Each row scaled to unit length\")\n",
    "print(\"\\n   L2 normalized data:\")\n",
    "print(df_normalized.head(10))\n",
    "\n",
    "# Check row norms (should be 1)\n",
    "row_norms = np.linalg.norm(df_normalized.values, axis=1)\n",
    "print(f\"\\n   Row norms (should be 1.0): {row_norms[:5]}\")\n",
    "\n",
    "# 9. Compare all scaling methods\n",
    "print(\"\\n9. Comparing All Scaling Methods:\")\n",
    "\n",
    "sample_idx = 0\n",
    "original_values = df_scaling.iloc[sample_idx]\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': df_scaling.columns,\n",
    "    'Original': original_values.values,\n",
    "    'Min-Max [0,1]': df_minmax.iloc[sample_idx].values,\n",
    "    'Min-Max [-1,1]': df_custom.iloc[sample_idx].values,\n",
    "    'Z-Score': df_standard.iloc[sample_idx].values,\n",
    "    'Robust': df_robust.iloc[sample_idx].values,\n",
    "    'L2 Norm': df_normalized.iloc[sample_idx].values\n",
    "})\n",
    "\n",
    "print(f\"\\n   Sample row {sample_idx} across all scaling methods:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# 10. When to use each method\n",
    "print(\"\\n10. Scaling Method Selection Guide:\")\n",
    "\n",
    "guide = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Min-Max [0,1]',\n",
    "        'Min-Max [a,b]',\n",
    "        'Z-Score (Standard)',\n",
    "        'Robust Scaler',\n",
    "        'Log Transform',\n",
    "        'Box-Cox',\n",
    "        'L2 Normalization'\n",
    "    ],\n",
    "    'Best_For': [\n",
    "        'Neural networks, image data',\n",
    "        'Bounded range requirements',\n",
    "        'Normally distributed data, linear models',\n",
    "        'Data with outliers',\n",
    "        'Right-skewed data (income, sales)',\n",
    "        'Severely skewed data',\n",
    "        'Text vectorization, cosine similarity'\n",
    "    ],\n",
    "    'Preserves': [\n",
    "        'Relationships',\n",
    "        'Relationships',\n",
    "        'Relationships (z-scores)',\n",
    "        'Relationships (outlier-resistant)',\n",
    "        'Order (reduces skew)',\n",
    "        'Order (optimizes normality)',\n",
    "        'Direction (unit vector)'\n",
    "    ],\n",
    "    'Sensitive_to_Outliers': [\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        'No',\n",
    "        'Partially',\n",
    "        'Partially',\n",
    "        'Yes'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(guide.to_string(index=False))\n",
    "\n",
    "# 11. Inverse transformation\n",
    "print(\"\\n11. Inverse Transformation (Reversing Scaling):\")\n",
    "\n",
    "# Scale data\n",
    "scaled_salary = scaler_minmax.fit_transform(df_scaling[['salary']])\n",
    "\n",
    "# Inverse transform (get back original values)\n",
    "original_salary = scaler_minmax.inverse_transform(scaled_salary)\n",
    "\n",
    "print(\"   Original â†’ Scaled â†’ Inverse:\")\n",
    "for i in range(5):\n",
    "    print(f\"   ${df_scaling['salary'].iloc[i]:,} â†’ {scaled_salary[i][0]:.4f} â†’ ${original_salary[i][0]:,.0f}\")\n",
    "\n",
    "# 12. Handling new data (transform only, no fit)\n",
    "print(\"\\n12. Scaling New Data (Transform Only):\")\n",
    "\n",
    "# New unseen data\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [25, 45],\n",
    "    'salary': [50000, 100000],\n",
    "    'years_experience': [2, 15],\n",
    "    'rating': [3.5, 4.8],\n",
    "    'transactions': [100, 500]\n",
    "})\n",
    "\n",
    "print(\"   New data:\")\n",
    "print(new_data)\n",
    "\n",
    "# Transform using already fitted scaler (don't fit again!)\n",
    "new_scaled = scaler_minmax.transform(new_data)\n",
    "new_scaled_df = pd.DataFrame(\n",
    "    new_scaled,\n",
    "    columns=[f'{col}_scaled' for col in new_data.columns]\n",
    ")\n",
    "\n",
    "print(\"\\n   New data scaled with existing scaler:\")\n",
    "print(new_scaled_df)\n",
    "\n",
    "# 13. Summary\n",
    "print(\"\\n13. Standardization & Normalization Summary:\")\n",
    "\n",
    "print(\"\\n   Key Differences:\")\n",
    "print(\"   â€¢ Normalization (Min-Max): Scales to fixed range [0,1] or [a,b]\")\n",
    "print(\"   â€¢ Standardization (Z-Score): Centers around 0 with unit variance\")\n",
    "print(\"   â€¢ Robust Scaling: Like standardization but uses median/IQR\")\n",
    "print(\"   â€¢ Log/Box-Cox: Transforms distribution shape (handles skewness)\")\n",
    "print(\"   â€¢ L2 Normalization: Scales each sample to unit norm\")\n",
    "\n",
    "print(\"\\n   Best Practices:\")\n",
    "print(\"   â€¢ Always fit on training data only, then transform test data\")\n",
    "print(\"   â€¢ Save scalers for production use (inverse_transform, new data)\")\n",
    "print(\"   â€¢ Choose based on data distribution and algorithm requirements\")\n",
    "print(\"   â€¢ Use Robust Scaler when outliers are present\")\n",
    "print(\"   â€¢ Use log/Box-Cox for skewed data before other scaling\")\n",
    "\n",
    "print(\"\\n   âœ“ Standardization & normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84be629",
   "metadata": {},
   "source": [
    "## 14. Advanced Data Cleaning Techniques\n",
    "\n",
    "**What is it?**\n",
    "Advanced techniques combine multiple cleaning methods and use sophisticated approaches for complex data quality issues.\n",
    "\n",
    "**Why use it?**\n",
    "- Handles complex data quality problems\n",
    "- Automates cleaning workflows\n",
    "- Improves accuracy beyond basic methods\n",
    "- Scales to large datasets\n",
    "- Detects subtle patterns and anomalies\n",
    "- Maintains data relationships\n",
    "\n",
    "**When to use it?**\n",
    "- Basic cleaning insufficient\n",
    "- Complex data quality issues\n",
    "- Large-scale data processing\n",
    "- Need automated solutions\n",
    "- Multiple related cleaning tasks\n",
    "- Production pipelines\n",
    "\n",
    "**Advanced techniques:**\n",
    "1. **Fuzzy matching**: Match similar but not identical strings\n",
    "2. **Automated anomaly detection**: ML-based outlier detection\n",
    "3. **Advanced imputation**: KNN, MICE, regression-based\n",
    "4. **Deduplication**: Record linkage, entity resolution\n",
    "5. **Data profiling**: Automated quality assessment\n",
    "6. **Constraint-based validation**: Complex business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 14. ADVANCED DATA CLEANING TECHNIQUES\n",
    "# ============================================\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED DATA CLEANING TECHNIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Fuzzy String Matching\n",
    "print(\"\\n1. Fuzzy String Matching:\")\n",
    "\n",
    "# Sample data with similar but not identical strings\n",
    "companies = [\n",
    "    'Microsoft Corporation',\n",
    "    'Microsoft Corp',\n",
    "    'Microsft Corporation',  # Typo\n",
    "    'Google LLC',\n",
    "    'Google Inc',\n",
    "    'Gogle LLC',  # Typo\n",
    "    'Amazon.com Inc',\n",
    "    'Amazon Inc',\n",
    "    'Apple Inc.',\n",
    "    'Apple Incorporated'\n",
    "]\n",
    "\n",
    "def fuzzy_similarity(str1, str2):\n",
    "    \"\"\"Calculate similarity ratio between two strings\"\"\"\n",
    "    return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
    "\n",
    "def find_fuzzy_matches(string, string_list, threshold=0.8):\n",
    "    \"\"\"Find similar strings above similarity threshold\"\"\"\n",
    "    matches = []\n",
    "    for s in string_list:\n",
    "        similarity = fuzzy_similarity(string, s)\n",
    "        if similarity >= threshold and s != string:\n",
    "            matches.append((s, similarity))\n",
    "    return sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"   Fuzzy matching examples (threshold=0.8):\")\n",
    "test_strings = ['Microsoft Corporation', 'Google LLC', 'Amazon Inc']\n",
    "for test in test_strings:\n",
    "    matches = find_fuzzy_matches(test, companies, threshold=0.8)\n",
    "    if matches:\n",
    "        print(f\"\\n   '{test}' matches:\")\n",
    "        for match, score in matches:\n",
    "            print(f\"     - '{match}' (similarity: {score:.2f})\")\n",
    "\n",
    "# Standardize using fuzzy matching\n",
    "def standardize_with_fuzzy(name, canonical_names, threshold=0.85):\n",
    "    \"\"\"Standardize name using fuzzy matching to canonical list\"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for canonical in canonical_names:\n",
    "        score = fuzzy_similarity(name, canonical)\n",
    "        if score > best_score and score >= threshold:\n",
    "            best_score = score\n",
    "            best_match = canonical\n",
    "    \n",
    "    return best_match if best_match else name\n",
    "\n",
    "canonical_companies = ['Microsoft Corporation', 'Google LLC', 'Amazon.com Inc', 'Apple Inc.']\n",
    "\n",
    "print(\"\\n   Standardization using fuzzy matching:\")\n",
    "for company in companies:\n",
    "    standardized = standardize_with_fuzzy(company, canonical_companies, threshold=0.75)\n",
    "    if company != standardized:\n",
    "        print(f\"   '{company}' â†’ '{standardized}'\")\n",
    "\n",
    "# 2. Advanced Imputation (KNN)\n",
    "print(\"\\n2. Advanced Imputation (K-Nearest Neighbors):\")\n",
    "\n",
    "# Create data with missing values\n",
    "impute_data = {\n",
    "    'age': [25, 30, np.nan, 40, 45, np.nan, 35, 50, 28, np.nan],\n",
    "    'salary': [50000, 60000, 55000, np.nan, 80000, 70000, np.nan, 90000, 52000, 58000],\n",
    "    'experience': [2, 5, 3, np.nan, 12, 8, 6, np.nan, 3, 4]\n",
    "}\n",
    "\n",
    "df_impute = pd.DataFrame(impute_data)\n",
    "\n",
    "print(\"   Original data with missing values:\")\n",
    "print(df_impute)\n",
    "\n",
    "# KNN Imputation (fills based on k nearest neighbors)\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn_imputed = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(df_impute),\n",
    "    columns=df_impute.columns\n",
    ")\n",
    "\n",
    "print(\"\\n   After KNN imputation (k=3):\")\n",
    "print(df_knn_imputed)\n",
    "\n",
    "# Compare with simple mean imputation\n",
    "df_mean_imputed = df_impute.fillna(df_impute.mean())\n",
    "\n",
    "print(\"\\n   Comparison (KNN vs Mean imputation):\")\n",
    "for idx in df_impute[df_impute.isna().any(axis=1)].index:\n",
    "    print(f\"\\n   Row {idx}:\")\n",
    "    print(f\"   Original: {df_impute.iloc[idx].to_dict()}\")\n",
    "    print(f\"   KNN:      {df_knn_imputed.iloc[idx].round(0).to_dict()}\")\n",
    "    print(f\"   Mean:     {df_mean_imputed.iloc[idx].round(0).to_dict()}\")\n",
    "\n",
    "# 3. Automated Anomaly Detection (Isolation Forest)\n",
    "print(\"\\n3. Automated Anomaly Detection (Isolation Forest):\")\n",
    "\n",
    "# Create data with anomalies\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(50, 10, 95)\n",
    "anomalies = np.array([5, 10, 90, 95, 100])  # Clear outliers\n",
    "all_data = np.concatenate([normal_data, anomalies])\n",
    "\n",
    "df_anomaly = pd.DataFrame({\n",
    "    'value': all_data,\n",
    "    'feature2': np.random.normal(100, 20, 100)\n",
    "})\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "predictions = iso_forest.fit_predict(df_anomaly)\n",
    "\n",
    "df_anomaly['is_anomaly'] = predictions == -1\n",
    "\n",
    "print(f\"   Total records: {len(df_anomaly)}\")\n",
    "print(f\"   Detected anomalies: {df_anomaly['is_anomaly'].sum()}\")\n",
    "\n",
    "print(\"\\n   Detected anomalies:\")\n",
    "anomalies_df = df_anomaly[df_anomaly['is_anomaly']].sort_values('value')\n",
    "print(anomalies_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df_anomaly[~df_anomaly['is_anomaly']]['value'],\n",
    "           df_anomaly[~df_anomaly['is_anomaly']]['feature2'],\n",
    "           c='blue', label='Normal', alpha=0.6)\n",
    "plt.scatter(df_anomaly[df_anomaly['is_anomaly']]['value'],\n",
    "           df_anomaly[df_anomaly['is_anomaly']]['feature2'],\n",
    "           c='red', label='Anomaly', s=100, marker='x')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Isolation Forest Anomaly Detection')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_anomaly['value'].hist(bins=30, edgecolor='black', alpha=0.7)\n",
    "for val in df_anomaly[df_anomaly['is_anomaly']]['value']:\n",
    "    plt.axvline(val, color='red', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution with Anomalies (red lines)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Advanced Deduplication\n",
    "print(\"\\n4. Advanced Deduplication (Record Linkage):\")\n",
    "\n",
    "# Sample duplicate records with variations\n",
    "records = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'name': [\n",
    "        'John Smith', 'John Smith', 'Jon Smith',  # Duplicates with typo\n",
    "        'Jane Doe', 'Jane M. Doe', 'J. Doe',      # Same person, different formats\n",
    "        'Bob Wilson', 'Robert Wilson',             # Nickname vs full name\n",
    "        'Alice Johnson', 'Alice Jonson'            # Typo\n",
    "    ],\n",
    "    'email': [\n",
    "        'john.smith@email.com', 'john.smith@email.com', 'j.smith@email.com',\n",
    "        'jane.doe@email.com', 'jane.doe@email.com', 'jane.doe@email.com',\n",
    "        'bob.w@email.com', 'robert.wilson@email.com',\n",
    "        'alice.j@email.com', 'alice.j@email.com'\n",
    "    ],\n",
    "    'phone': [\n",
    "        '555-1234', '555-1234', '555-1234',\n",
    "        '555-5678', '555-5678', '555-5678',\n",
    "        '555-9999', '555-9999',\n",
    "        '555-1111', '555-1111'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"   Original records:\")\n",
    "print(records)\n",
    "\n",
    "def calculate_record_similarity(rec1, rec2):\n",
    "    \"\"\"Calculate similarity between two records\"\"\"\n",
    "    name_sim = fuzzy_similarity(rec1['name'], rec2['name'])\n",
    "    email_sim = 1.0 if rec1['email'] == rec2['email'] else 0.0\n",
    "    phone_sim = 1.0 if rec1['phone'] == rec2['phone'] else 0.0\n",
    "    \n",
    "    # Weighted average\n",
    "    return (name_sim * 0.5 + email_sim * 0.3 + phone_sim * 0.2)\n",
    "\n",
    "def find_duplicate_groups(df, threshold=0.8):\n",
    "    \"\"\"Find groups of duplicate records\"\"\"\n",
    "    groups = []\n",
    "    processed = set()\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i in processed:\n",
    "            continue\n",
    "        \n",
    "        group = [i]\n",
    "        for j in range(i + 1, len(df)):\n",
    "            if j in processed:\n",
    "                continue\n",
    "            \n",
    "            sim = calculate_record_similarity(df.iloc[i], df.iloc[j])\n",
    "            if sim >= threshold:\n",
    "                group.append(j)\n",
    "                processed.add(j)\n",
    "        \n",
    "        if len(group) > 1:\n",
    "            groups.append(group)\n",
    "        processed.add(i)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "duplicate_groups = find_duplicate_groups(records, threshold=0.7)\n",
    "\n",
    "print(f\"\\n   Found {len(duplicate_groups)} duplicate groups:\")\n",
    "for idx, group in enumerate(duplicate_groups, 1):\n",
    "    print(f\"\\n   Group {idx}:\")\n",
    "    print(records.iloc[group][['name', 'email', 'phone']])\n",
    "\n",
    "# 5. Automated Data Profiling\n",
    "print(\"\\n5. Automated Data Profiling:\")\n",
    "\n",
    "# Create comprehensive profile of dataset\n",
    "def profile_dataframe(df):\n",
    "    \"\"\"Create comprehensive data quality profile\"\"\"\n",
    "    profile = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_usage': df.memory_usage(deep=True).sum(),\n",
    "        'column_profiles': {}\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_profile = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'missing_count': df[col].isna().sum(),\n",
    "            'missing_percentage': (df[col].isna().sum() / len(df)) * 100,\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'cardinality': (df[col].nunique() / len(df)) * 100\n",
    "        }\n",
    "        \n",
    "        # Numeric specific\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_profile.update({\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max(),\n",
    "                'mean': df[col].mean(),\n",
    "                'median': df[col].median(),\n",
    "                'std': df[col].std(),\n",
    "                'zeros': (df[col] == 0).sum()\n",
    "            })\n",
    "        \n",
    "        # String specific\n",
    "        elif pd.api.types.is_string_dtype(df[col]) or df[col].dtype == 'object':\n",
    "            col_profile.update({\n",
    "                'max_length': df[col].astype(str).str.len().max(),\n",
    "                'min_length': df[col].astype(str).str.len().min(),\n",
    "                'avg_length': df[col].astype(str).str.len().mean()\n",
    "            })\n",
    "        \n",
    "        profile['column_profiles'][col] = col_profile\n",
    "    \n",
    "    return profile\n",
    "\n",
    "# Profile the imputation dataset\n",
    "profile = profile_dataframe(df_impute)\n",
    "\n",
    "print(f\"   Dataset Profile:\")\n",
    "print(f\"   Total rows: {profile['total_rows']}\")\n",
    "print(f\"   Total columns: {profile['total_columns']}\")\n",
    "print(f\"   Memory usage: {profile['memory_usage']:,} bytes\")\n",
    "\n",
    "print(\"\\n   Column-level profiles:\")\n",
    "for col, col_prof in profile['column_profiles'].items():\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"     Type: {col_prof['dtype']}\")\n",
    "    print(f\"     Missing: {col_prof['missing_count']} ({col_prof['missing_percentage']:.1f}%)\")\n",
    "    print(f\"     Unique: {col_prof['unique_count']} ({col_prof['cardinality']:.1f}%)\")\n",
    "    if 'mean' in col_prof:\n",
    "        print(f\"     Mean: {col_prof['mean']:.2f}, Median: {col_prof['median']:.2f}\")\n",
    "        print(f\"     Range: [{col_prof['min']:.0f}, {col_prof['max']:.0f}]\")\n",
    "\n",
    "# 6. Constraint-Based Validation\n",
    "print(\"\\n6. Constraint-Based Validation (Complex Rules):\")\n",
    "\n",
    "# Sample data for validation\n",
    "validation_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'age': [25, 17, 45, 150, 30],\n",
    "    'income': [50000, 30000, 80000, 20000, 60000],\n",
    "    'credit_score': [700, 650, 800, 400, 720],\n",
    "    'loan_amount': [200000, 50000, 300000, 10000, 150000],\n",
    "    'employment_years': [5, 1, 20, -2, 8]\n",
    "})\n",
    "\n",
    "def validate_complex_rules(df):\n",
    "    \"\"\"Apply complex validation rules\"\"\"\n",
    "    violations = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        row_violations = []\n",
    "        \n",
    "        # Rule 1: Age must be 18-100\n",
    "        if row['age'] < 18 or row['age'] > 100:\n",
    "            row_violations.append('Invalid age')\n",
    "        \n",
    "        # Rule 2: Credit score must be 300-850\n",
    "        if row['credit_score'] < 300 or row['credit_score'] > 850:\n",
    "            row_violations.append('Invalid credit score')\n",
    "        \n",
    "        # Rule 3: Loan amount should not exceed 5x annual income\n",
    "        if row['loan_amount'] > row['income'] * 5:\n",
    "            row_violations.append('Loan amount too high relative to income')\n",
    "        \n",
    "        # Rule 4: Employment years should be positive and less than age-16\n",
    "        if row['employment_years'] < 0 or row['employment_years'] > (row['age'] - 16):\n",
    "            row_violations.append('Invalid employment years')\n",
    "        \n",
    "        # Rule 5: Minimum income for certain loan amounts\n",
    "        if row['loan_amount'] > 100000 and row['income'] < 40000:\n",
    "            row_violations.append('Income too low for loan amount')\n",
    "        \n",
    "        violations.append({\n",
    "            'customer_id': row['customer_id'],\n",
    "            'violations': row_violations if row_violations else ['Valid']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(violations)\n",
    "\n",
    "validation_results = validate_complex_rules(validation_data)\n",
    "\n",
    "print(\"   Validation Results:\")\n",
    "print(validation_results)\n",
    "\n",
    "print(\"\\n   Invalid records:\")\n",
    "invalid = validation_results[validation_results['violations'].apply(lambda x: x != ['Valid'])]\n",
    "for _, row in invalid.iterrows():\n",
    "    print(f\"\\n   Customer {row['customer_id']}:\")\n",
    "    for violation in row['violations']:\n",
    "        print(f\"     - {violation}\")\n",
    "\n",
    "# 7. Summary\n",
    "print(\"\\n7. Advanced Techniques Summary:\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Technique': [\n",
    "        'Fuzzy String Matching',\n",
    "        'KNN Imputation',\n",
    "        'Isolation Forest',\n",
    "        'Record Linkage',\n",
    "        'Data Profiling',\n",
    "        'Constraint Validation'\n",
    "    ],\n",
    "    'Use_Case': [\n",
    "        'Match similar text with typos',\n",
    "        'Fill missing values intelligently',\n",
    "        'Detect anomalies automatically',\n",
    "        'Find duplicate records',\n",
    "        'Assess data quality',\n",
    "        'Enforce complex business rules'\n",
    "    ],\n",
    "    'Complexity': [\n",
    "        'Medium',\n",
    "        'Medium',\n",
    "        'High',\n",
    "        'High',\n",
    "        'Low',\n",
    "        'Medium'\n",
    "    ],\n",
    "    'When_to_Use': [\n",
    "        'Name/address standardization',\n",
    "        'Missing data with patterns',\n",
    "        'Unknown outlier patterns',\n",
    "        'Multiple data sources',\n",
    "        'Data quality assessment',\n",
    "        'Business rule enforcement'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n   âœ“ Advanced data cleaning techniques complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c1e1f",
   "metadata": {},
   "source": [
    "## 15. Data Cleaning Pipeline & Best Practices\n",
    "\n",
    "**What is it?**\n",
    "A data cleaning pipeline is an automated, repeatable workflow that systematically applies cleaning operations to ensure consistent data quality.\n",
    "\n",
    "**Why use it?**\n",
    "- Ensures reproducibility\n",
    "- Automates repetitive tasks\n",
    "- Reduces human error\n",
    "- Enables version control\n",
    "- Scales to production\n",
    "- Maintains consistency across datasets\n",
    "- Facilitates collaboration\n",
    "\n",
    "**When to use it?**\n",
    "- Production environments\n",
    "- Regular data updates\n",
    "- Multiple data sources\n",
    "- Team collaboration\n",
    "- Need for reproducibility\n",
    "- Quality assurance required\n",
    "\n",
    "**Pipeline components:**\n",
    "1. **Data validation**: Check input data quality\n",
    "2. **Missing value handling**: Systematic imputation\n",
    "3. **Type conversion**: Standardize data types\n",
    "4. **Outlier treatment**: Detect and handle anomalies\n",
    "5. **Standardization**: Normalize formats\n",
    "6. **Deduplication**: Remove redundant records\n",
    "7. **Quality checks**: Validate output\n",
    "8. **Logging**: Track changes and issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 15. DATA CLEANING PIPELINE & BEST PRACTICES\n",
    "# ============================================\n",
    "\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING PIPELINE & BEST PRACTICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Create logging system\n",
    "print(\"\\n1. Setting Up Logging:\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('DataCleaning')\n",
    "\n",
    "print(\"   âœ“ Logging system configured\")\n",
    "\n",
    "# 2. Define data cleaning pipeline class\n",
    "print(\"\\n2. Building Comprehensive Data Cleaning Pipeline:\")\n",
    "\n",
    "class DataCleaningPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning pipeline with logging and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"DataCleaningPipeline\"):\n",
    "        self.name = name\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.cleaning_report = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'operations': [],\n",
    "            'original_shape': None,\n",
    "            'final_shape': None,\n",
    "            'changes': {}\n",
    "        }\n",
    "    \n",
    "    def log_operation(self, operation, details):\n",
    "        \"\"\"Log a cleaning operation\"\"\"\n",
    "        self.logger.info(f\"{operation}: {details}\")\n",
    "        self.cleaning_report['operations'].append({\n",
    "            'operation': operation,\n",
    "            'details': details,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "    \n",
    "    def validate_input(self, df):\n",
    "        \"\"\"Validate input data\"\"\"\n",
    "        self.log_operation(\"Input Validation\", f\"Shape: {df.shape}\")\n",
    "        self.cleaning_report['original_shape'] = df.shape\n",
    "        \n",
    "        # Check for empty dataframe\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Input dataframe is empty\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df, strategy='median', threshold=0.5):\n",
    "        \"\"\"Handle missing values with multiple strategies\"\"\"\n",
    "        missing_before = df.isna().sum().sum()\n",
    "        \n",
    "        # Drop columns with too many missing values\n",
    "        missing_pct = df.isna().sum() / len(df)\n",
    "        cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "        \n",
    "        if cols_to_drop:\n",
    "            df = df.drop(columns=cols_to_drop)\n",
    "            self.log_operation(\"Drop Columns\", f\"Dropped {len(cols_to_drop)} columns with >{threshold*100}% missing\")\n",
    "        \n",
    "        # Impute remaining missing values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        if strategy == 'median':\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        elif strategy == 'mean':\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "        \n",
    "        df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
    "        \n",
    "        missing_after = df.isna().sum().sum()\n",
    "        self.log_operation(\"Missing Values\", f\"Before: {missing_before}, After: {missing_after}\")\n",
    "        self.cleaning_report['changes']['missing_values'] = {\n",
    "            'before': int(missing_before),\n",
    "            'after': int(missing_after)\n",
    "        }\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df, subset=None):\n",
    "        \"\"\"Remove duplicate records\"\"\"\n",
    "        duplicates_before = df.duplicated().sum()\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        duplicates_after = df.duplicated().sum()\n",
    "        \n",
    "        self.log_operation(\"Remove Duplicates\", f\"Removed {duplicates_before} duplicates\")\n",
    "        self.cleaning_report['changes']['duplicates'] = {\n",
    "            'removed': int(duplicates_before)\n",
    "        }\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_outliers(self, df, columns=None, method='iqr', factor=1.5):\n",
    "        \"\"\"Handle outliers using IQR or Z-score method\"\"\"\n",
    "        if columns is None:\n",
    "            columns = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        outliers_count = 0\n",
    "        \n",
    "        for col in columns:\n",
    "            if method == 'iqr':\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower = Q1 - factor * IQR\n",
    "                upper = Q3 + factor * IQR\n",
    "                \n",
    "                outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "                outliers_count += outliers\n",
    "                \n",
    "                # Cap outliers\n",
    "                df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "        \n",
    "        self.log_operation(\"Handle Outliers\", f\"Capped {outliers_count} outlier values\")\n",
    "        self.cleaning_report['changes']['outliers'] = {\n",
    "            'capped': int(outliers_count)\n",
    "        }\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def standardize_text(self, df, columns=None):\n",
    "        \"\"\"Standardize text columns\"\"\"\n",
    "        if columns is None:\n",
    "            columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in columns:\n",
    "            # Strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Remove extra spaces\n",
    "            df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "            # Standardize case (title case)\n",
    "            df[col] = df[col].str.title()\n",
    "        \n",
    "        self.log_operation(\"Standardize Text\", f\"Standardized {len(columns)} text columns\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def convert_data_types(self, df, type_mapping=None):\n",
    "        \"\"\"Convert data types based on mapping\"\"\"\n",
    "        if type_mapping:\n",
    "            for col, dtype in type_mapping.items():\n",
    "                if col in df.columns:\n",
    "                    try:\n",
    "                        df[col] = df[col].astype(dtype)\n",
    "                        self.log_operation(\"Type Conversion\", f\"{col} â†’ {dtype}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Failed to convert {col} to {dtype}: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def validate_output(self, df, rules=None):\n",
    "        \"\"\"Validate output data against rules\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for remaining missing values\n",
    "        missing = df.isna().sum().sum()\n",
    "        if missing > 0:\n",
    "            issues.append(f\"Still has {missing} missing values\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            issues.append(f\"Still has {duplicates} duplicates\")\n",
    "        \n",
    "        # Custom rules\n",
    "        if rules:\n",
    "            for rule_name, rule_func in rules.items():\n",
    "                if not rule_func(df):\n",
    "                    issues.append(f\"Failed rule: {rule_name}\")\n",
    "        \n",
    "        if issues:\n",
    "            self.logger.warning(f\"Validation issues: {', '.join(issues)}\")\n",
    "        else:\n",
    "            self.log_operation(\"Output Validation\", \"All checks passed âœ“\")\n",
    "        \n",
    "        self.cleaning_report['validation_issues'] = issues\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean(self, df, **kwargs):\n",
    "        \"\"\"Execute full cleaning pipeline\"\"\"\n",
    "        self.logger.info(f\"Starting {self.name}\")\n",
    "        \n",
    "        # 1. Validate input\n",
    "        df = self.validate_input(df)\n",
    "        \n",
    "        # 2. Handle missing values\n",
    "        df = self.handle_missing_values(df, \n",
    "                                       strategy=kwargs.get('missing_strategy', 'median'),\n",
    "                                       threshold=kwargs.get('missing_threshold', 0.5))\n",
    "        \n",
    "        # 3. Remove duplicates\n",
    "        df = self.remove_duplicates(df, subset=kwargs.get('duplicate_subset'))\n",
    "        \n",
    "        # 4. Handle outliers\n",
    "        df = self.handle_outliers(df, \n",
    "                                 columns=kwargs.get('outlier_columns'),\n",
    "                                 method=kwargs.get('outlier_method', 'iqr'))\n",
    "        \n",
    "        # 5. Standardize text\n",
    "        df = self.standardize_text(df, columns=kwargs.get('text_columns'))\n",
    "        \n",
    "        # 6. Convert data types\n",
    "        df = self.convert_data_types(df, type_mapping=kwargs.get('type_mapping'))\n",
    "        \n",
    "        # 7. Validate output\n",
    "        df = self.validate_output(df, rules=kwargs.get('validation_rules'))\n",
    "        \n",
    "        # 8. Finalize report\n",
    "        self.cleaning_report['final_shape'] = df.shape\n",
    "        self.logger.info(f\"Completed {self.name}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Get cleaning report\"\"\"\n",
    "        return self.cleaning_report\n",
    "\n",
    "print(\"   âœ“ DataCleaningPipeline class created\")\n",
    "\n",
    "# 3. Test the pipeline with sample data\n",
    "print(\"\\n3. Testing Data Cleaning Pipeline:\")\n",
    "\n",
    "# Create messy sample data\n",
    "messy_data = {\n",
    "    'customer_id': [1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Duplicate\n",
    "    'name': ['  John Smith  ', 'jane DOE', 'jane DOE', 'Bob   Wilson', \n",
    "             'Alice Johnson', None, 'Charlie Brown', 'David Lee', 'Eva Martinez', 'Frank Zhang', 'Grace Kim'],\n",
    "    'age': [25, 30, 30, 1000, 45, 35, np.nan, 50, 28, 32, 41],  # Outlier, missing\n",
    "    'salary': [50000, 60000, 60000, 55000, 80000, np.nan, 70000, 90000, 52000, 58000, 75000],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'jane@email.com', 'bob@email.com',\n",
    "             'alice@email.com', 'charlie@email.com', 'charlie@email.com', \n",
    "             'david@email.com', 'eva@email.com', 'frank@email.com', 'grace@email.com'],\n",
    "    'city': ['New York', 'los angeles', 'los angeles', 'CHICAGO',\n",
    "            'Boston', 'Seattle', 'Austin', 'Denver', 'Miami', 'Portland', 'Phoenix']\n",
    "}\n",
    "\n",
    "df_messy = pd.DataFrame(messy_data)\n",
    "\n",
    "print(\"   Original messy data:\")\n",
    "print(df_messy)\n",
    "print(f\"\\n   Shape: {df_messy.shape}\")\n",
    "print(f\"   Missing values: {df_messy.isna().sum().sum()}\")\n",
    "print(f\"   Duplicates: {df_messy.duplicated().sum()}\")\n",
    "\n",
    "# 4. Apply pipeline\n",
    "print(\"\\n4. Applying Cleaning Pipeline:\")\n",
    "\n",
    "pipeline = DataCleaningPipeline(name=\"CustomerDataCleaning\")\n",
    "\n",
    "df_cleaned = pipeline.clean(\n",
    "    df_messy,\n",
    "    missing_strategy='median',\n",
    "    missing_threshold=0.8,\n",
    "    duplicate_subset=['customer_id'],\n",
    "    outlier_columns=['age', 'salary'],\n",
    "    outlier_method='iqr',\n",
    "    text_columns=['name', 'city', 'email']\n",
    ")\n",
    "\n",
    "print(\"\\n   Cleaned data:\")\n",
    "print(df_cleaned)\n",
    "print(f\"\\n   Shape: {df_cleaned.shape}\")\n",
    "print(f\"   Missing values: {df_cleaned.isna().sum().sum()}\")\n",
    "print(f\"   Duplicates: {df_cleaned.duplicated().sum()}\")\n",
    "\n",
    "# 5. Generate cleaning report\n",
    "print(\"\\n5. Cleaning Report:\")\n",
    "\n",
    "report = pipeline.get_report()\n",
    "print(f\"\\n   Pipeline: {pipeline.name}\")\n",
    "print(f\"   Timestamp: {report['timestamp']}\")\n",
    "print(f\"   Original shape: {report['original_shape']}\")\n",
    "print(f\"   Final shape: {report['final_shape']}\")\n",
    "\n",
    "print(\"\\n   Operations performed:\")\n",
    "for op in report['operations']:\n",
    "    print(f\"     - {op['operation']}: {op['details']}\")\n",
    "\n",
    "print(\"\\n   Changes summary:\")\n",
    "for change_type, change_data in report['changes'].items():\n",
    "    print(f\"     {change_type}: {change_data}\")\n",
    "\n",
    "# 6. Best Practices Summary\n",
    "print(\"\\n6. Data Cleaning Best Practices:\")\n",
    "\n",
    "best_practices = {\n",
    "    '1. Document Everything': [\n",
    "        'Keep detailed logs of all transformations',\n",
    "        'Document assumptions and decisions',\n",
    "        'Version control cleaning scripts'\n",
    "    ],\n",
    "    '2. Validate at Each Step': [\n",
    "        'Check data quality before and after each operation',\n",
    "        'Set up automated validation tests',\n",
    "        'Monitor data distributions'\n",
    "    ],\n",
    "    '3. Make it Reproducible': [\n",
    "        'Use pipelines and functions, not ad-hoc scripts',\n",
    "        'Save intermediate states',\n",
    "        'Use random seeds for reproducibility'\n",
    "    ],\n",
    "    '4. Handle Edge Cases': [\n",
    "        'Test with extreme values',\n",
    "        'Plan for new/unseen categories',\n",
    "        'Handle null, empty, and special values'\n",
    "    ],\n",
    "    '5. Preserve Original Data': [\n",
    "        'Never modify source data directly',\n",
    "        'Create copies for transformations',\n",
    "        'Keep backup of raw data'\n",
    "    ],\n",
    "    '6. Domain Knowledge': [\n",
    "        'Understand business context',\n",
    "        'Validate with domain experts',\n",
    "        'Set realistic constraints'\n",
    "    ],\n",
    "    '7. Automate Quality Checks': [\n",
    "        'Set up data quality metrics',\n",
    "        'Create automated alerts',\n",
    "        'Regular data profiling'\n",
    "    ],\n",
    "    '8. Performance Optimization': [\n",
    "        'Use vectorized operations',\n",
    "        'Process in chunks for large data',\n",
    "        'Optimize memory usage'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for practice, guidelines in best_practices.items():\n",
    "    print(f\"\\n   {practice}:\")\n",
    "    for guideline in guidelines:\n",
    "        print(f\"     â€¢ {guideline}\")\n",
    "\n",
    "# 7. Common Pitfalls to Avoid\n",
    "print(\"\\n7. Common Data Cleaning Pitfalls to Avoid:\")\n",
    "\n",
    "pitfalls = pd.DataFrame({\n",
    "    'Pitfall': [\n",
    "        'Data Leakage',\n",
    "        'Overfitting to Noise',\n",
    "        'Ignoring Duplicates',\n",
    "        'Arbitrary Outlier Removal',\n",
    "        'Inconsistent Transformations',\n",
    "        'Not Validating Assumptions',\n",
    "        'Losing Information',\n",
    "        'Ignoring Time Dependencies'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        'Biased model performance',\n",
    "        'Poor generalization',\n",
    "        'Inflated statistics',\n",
    "        'Loss of valuable data',\n",
    "        'Unreliable results',\n",
    "        'Incorrect conclusions',\n",
    "        'Reduced model accuracy',\n",
    "        'Invalid analysis'\n",
    "    ],\n",
    "    'Solution': [\n",
    "        'Clean train/test separately',\n",
    "        'Keep original data, validate changes',\n",
    "        'Always check for duplicates',\n",
    "        'Use statistical methods, justify removal',\n",
    "        'Use pipelines, maintain consistency',\n",
    "        'Test assumptions, validate with experts',\n",
    "        'Document what was removed and why',\n",
    "        'Preserve temporal ordering'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(pitfalls.to_string(index=False))\n",
    "\n",
    "# 8. Recommended Tools and Libraries\n",
    "print(\"\\n8. Recommended Python Libraries for Data Cleaning:\")\n",
    "\n",
    "libraries = pd.DataFrame({\n",
    "    'Library': [\n",
    "        'Pandas',\n",
    "        'NumPy',\n",
    "        'scikit-learn',\n",
    "        'Great Expectations',\n",
    "        'pandas-profiling',\n",
    "        'fuzzywuzzy',\n",
    "        'pyjanitor',\n",
    "        'datacleaner'\n",
    "    ],\n",
    "    'Purpose': [\n",
    "        'Core data manipulation',\n",
    "        'Numerical operations',\n",
    "        'Preprocessing, imputation, scaling',\n",
    "        'Data validation framework',\n",
    "        'Automated EDA and profiling',\n",
    "        'Fuzzy string matching',\n",
    "        'Clean API for cleaning operations',\n",
    "        'Automated cleaning'\n",
    "    ],\n",
    "    'Use_When': [\n",
    "        'Always (fundamental)',\n",
    "        'Numerical computations',\n",
    "        'ML pipelines, transformations',\n",
    "        'Production data validation',\n",
    "        'Initial data exploration',\n",
    "        'Text standardization',\n",
    "        'Method chaining workflows',\n",
    "        'Quick automated cleaning'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(libraries.to_string(index=False))\n",
    "\n",
    "# 9. Final Recommendations\n",
    "print(\"\\n9. Final Recommendations:\")\n",
    "\n",
    "print(\"\"\"\n",
    "   Data Cleaning Workflow:\n",
    "   \n",
    "   1. UNDERSTAND â†’ Explore and profile your data\n",
    "   2. PLAN â†’ Define cleaning strategy and rules\n",
    "   3. VALIDATE â†’ Check input data quality\n",
    "   4. CLEAN â†’ Apply systematic transformations\n",
    "   5. VERIFY â†’ Validate output meets requirements\n",
    "   6. DOCUMENT â†’ Log all changes and decisions\n",
    "   7. ITERATE â†’ Refine based on results\n",
    "   8. AUTOMATE â†’ Build reusable pipelines\n",
    "   \n",
    "   Remember:\n",
    "   â€¢ Data cleaning is iterative, not one-time\n",
    "   â€¢ Domain knowledge is as important as technical skills\n",
    "   â€¢ Good documentation prevents future headaches\n",
    "   â€¢ Automation ensures consistency and saves time\n",
    "   â€¢ Always validate assumptions with data and experts\n",
    "   â€¢ Clean data is the foundation of good analysis\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONGRATULATIONS! DATA CLEANING LEARNING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "You've completed the comprehensive Data Cleaning guide! You now know:\n",
    "\n",
    "âœ“ How to handle missing values (12+ strategies)\n",
    "âœ“ Detecting and removing duplicates (exact, partial, fuzzy)\n",
    "âœ“ Data type conversion and optimization\n",
    "âœ“ String cleaning and standardization\n",
    "âœ“ Outlier detection and handling (6+ methods)\n",
    "âœ“ Data validation (types, ranges, formats, business rules)\n",
    "âœ“ Handling inconsistent data across sources\n",
    "âœ“ Date & time cleaning (parsing, validation, extraction)\n",
    "âœ“ Categorical data cleaning (encoding, grouping, standardization)\n",
    "âœ“ Numerical data cleaning (units, precision, binning)\n",
    "âœ“ Text data cleaning (NLP preprocessing)\n",
    "âœ“ Standardization & normalization (6+ techniques)\n",
    "âœ“ Advanced techniques (fuzzy matching, ML imputation, anomaly detection)\n",
    "âœ“ Building production-ready cleaning pipelines\n",
    "âœ“ Best practices and common pitfalls\n",
    "\n",
    "Next Steps:\n",
    "â€¢ Practice with real-world messy datasets\n",
    "â€¢ Build cleaning pipelines for your projects\n",
    "â€¢ Learn about data quality frameworks (Great Expectations)\n",
    "â€¢ Study feature engineering for machine learning\n",
    "â€¢ Explore automated ML pipelines (MLOps)\n",
    "\n",
    "Happy Data Cleaning! ðŸ§¹âœ¨\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
