{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37c2eb3",
   "metadata": {},
   "source": [
    "# 1. Reading CSV Files\n",
    "\n",
    "## üìñ What is CSV Reading?\n",
    "\n",
    "CSV (Comma-Separated Values) is the most common format for storing tabular data. Reading CSV files is the **first step** in most data science projects.\n",
    "\n",
    "**CSV Structure:**\n",
    "```\n",
    "Name,Age,City\n",
    "John,25,New York\n",
    "Jane,30,London\n",
    "```\n",
    "\n",
    "**pandas.read_csv()** converts CSV to DataFrame - the fundamental data structure in Python data science.\n",
    "\n",
    "## üéØ Why Use CSV Files?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Universal Format** - Supported by all tools (Excel, Python, R, SQL)\n",
    "2. **Human-Readable** - Can open in any text editor\n",
    "3. **Lightweight** - Small file size, fast to transfer\n",
    "4. **Simple Structure** - Easy to understand and debug\n",
    "5. **No Proprietary Lock-in** - Open standard\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **No Data Types** - Everything stored as text\n",
    "2. **Slow for Large Data** - Billions of rows inefficient\n",
    "3. **No Compression** - Larger than binary formats\n",
    "4. **Limited Metadata** - No schema information\n",
    "\n",
    "## ‚è±Ô∏è When to Use CSV Files\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Starting a Data Science Project**\n",
    "- Example: Kaggle datasets, company exports\n",
    "- Why: Most datasets provided in CSV\n",
    "- Size: < 1 GB usually fine\n",
    "\n",
    "**2. Sharing Data Between Teams**\n",
    "- Example: Analytics to engineering team\n",
    "- Why: Everyone can open CSV files\n",
    "- Alternative: API for real-time data\n",
    "\n",
    "**3. Exporting Analysis Results**\n",
    "- Example: Send findings to stakeholders\n",
    "- Why: Opens in Excel easily\n",
    "- Use case: Reports, summaries\n",
    "\n",
    "**4. Small to Medium Datasets**\n",
    "- Example: 10,000 - 1,000,000 rows\n",
    "- Why: Fast enough, simple enough\n",
    "- Memory: Fits in RAM\n",
    "\n",
    "**5. Data Archiving**\n",
    "- Example: Historical records backup\n",
    "- Why: Future-proof format\n",
    "- Benefit: Readable in 50 years\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Very Large Datasets (> 5 GB)**\n",
    "- Example: Billions of transaction records\n",
    "- Better: Parquet, databases, data warehouses\n",
    "- Why: Too slow to load, memory issues\n",
    "\n",
    "**2. Complex Nested Data**\n",
    "- Example: Hierarchical JSON structures\n",
    "- Better: JSON, MongoDB, nested Parquet\n",
    "- Why: CSV is flat/tabular only\n",
    "\n",
    "**3. Real-Time Data Streaming**\n",
    "- Example: Live sensor data, stock prices\n",
    "- Better: Kafka, APIs, message queues\n",
    "- Why: CSV is for static snapshots\n",
    "\n",
    "**4. Need Data Type Enforcement**\n",
    "- Example: Critical financial calculations\n",
    "- Better: SQL databases with schemas\n",
    "- Why: CSV doesn't preserve types\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**pandas.read_csv() process:**\n",
    "1. Opens file and reads line by line\n",
    "2. Splits each line by delimiter (`,`)\n",
    "3. Infers data types automatically\n",
    "4. Creates DataFrame structure\n",
    "5. Loads into memory\n",
    "\n",
    "**Common Parameters:**\n",
    "- `sep` - Delimiter (default: `,`)\n",
    "- `header` - Row number for column names\n",
    "- `encoding` - Character encoding (utf-8, latin1)\n",
    "- `na_values` - What to treat as missing\n",
    "- `dtype` - Specify data types\n",
    "- `parse_dates` - Auto-convert date columns\n",
    "- `nrows` - Read only N rows (for testing)\n",
    "- `usecols` - Read only specific columns\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Data Science Projects** - 90% start with CSV\n",
    "2. **Business Analytics** - Sales reports, customer data\n",
    "3. **Financial Analysis** - Stock prices, transactions\n",
    "4. **Research** - Scientific datasets, surveys\n",
    "5. **Government Data** - Open data portals\n",
    "6. **E-commerce** - Product catalogs, orders\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Most common data format in data science  \n",
    "‚úÖ Always check first few rows: `df.head()`  \n",
    "‚úÖ Check data types: `df.dtypes`  \n",
    "‚úÖ For large files, use `nrows` parameter for testing  \n",
    "‚úÖ Specify `dtype` to save memory  \n",
    "‚úÖ Use `encoding='latin1'` if UTF-8 fails  \n",
    "‚úÖ For > 1 GB, consider Parquet format  \n",
    "‚úÖ Use `chunksize` for files too large for RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING CSV FILES - COMPLETE EXAMPLE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"READING CSV FILES - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. CREATE SAMPLE CSV DATA\n",
    "print(\"\\n1. CREATING SAMPLE CSV DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Sample CSV content\n",
    "csv_data = \"\"\"EmployeeID,Name,Age,Department,Salary,JoinDate,City\n",
    "1,John Smith,28,Engineering,75000,2020-01-15,New York\n",
    "2,Jane Doe,35,Marketing,65000,2019-03-20,London\n",
    "3,Bob Johnson,42,Sales,80000,2018-07-10,Paris\n",
    "4,Alice Williams,31,Engineering,78000,2020-05-01,Tokyo\n",
    "5,Charlie Brown,29,HR,55000,2021-02-14,New York\n",
    "6,Diana Prince,38,Marketing,70000,2019-11-30,London\n",
    "7,Eve Davis,26,Engineering,72000,2021-06-15,Berlin\n",
    "8,Frank Miller,45,Sales,85000,2017-09-22,Paris\n",
    "9,Grace Lee,33,Engineering,76000,2020-08-05,Tokyo\n",
    "10,Henry Wilson,40,Marketing,68000,2018-12-10,New York\"\"\"\n",
    "\n",
    "# Save to actual CSV file\n",
    "with open('sample_employees.csv', 'w') as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "print(\"‚úì Sample CSV file created: 'sample_employees.csv'\")\n",
    "print(\"\\nRaw CSV content (first 3 lines):\")\n",
    "print(csv_data.split('\\n')[:3])\n",
    "\n",
    "# 2. BASIC CSV READING\n",
    "print(\"\\n2. BASIC CSV READING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('sample_employees.csv')\n",
    "\n",
    "print(\"‚úì CSV loaded successfully!\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 3. ADVANCED CSV READING OPTIONS\n",
    "print(\"\\n3. ADVANCED CSV READING OPTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Option 1: Parse dates automatically\n",
    "print(\"\\nOption 1: Parse dates\")\n",
    "df_with_dates = pd.read_csv('sample_employees.csv', parse_dates=['JoinDate'])\n",
    "print(f\"JoinDate type: {df_with_dates['JoinDate'].dtype}\")\n",
    "print(f\"Sample date: {df_with_dates['JoinDate'].iloc[0]}\")\n",
    "\n",
    "# Option 2: Read only specific columns\n",
    "print(\"\\nOption 2: Read specific columns\")\n",
    "df_subset = pd.read_csv('sample_employees.csv', usecols=['Name', 'Department', 'Salary'])\n",
    "print(f\"Columns: {df_subset.columns.tolist()}\")\n",
    "print(df_subset.head(3))\n",
    "\n",
    "# Option 3: Read only first N rows\n",
    "print(\"\\nOption 3: Read first 3 rows only\")\n",
    "df_sample = pd.read_csv('sample_employees.csv', nrows=3)\n",
    "print(f\"Shape: {df_sample.shape}\")\n",
    "print(df_sample)\n",
    "\n",
    "# Option 4: Specify data types\n",
    "print(\"\\nOption 4: Specify data types\")\n",
    "dtype_dict = {\n",
    "    'EmployeeID': int,\n",
    "    'Name': str,\n",
    "    'Age': int,\n",
    "    'Department': 'category',  # Save memory\n",
    "    'Salary': float,\n",
    "    'City': 'category'\n",
    "}\n",
    "df_typed = pd.read_csv('sample_employees.csv', dtype=dtype_dict, parse_dates=['JoinDate'])\n",
    "print(\"Data types with optimization:\")\n",
    "print(df_typed.dtypes)\n",
    "\n",
    "# 4. MEMORY USAGE COMPARISON\n",
    "print(\"\\n4. MEMORY USAGE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"Without type specification: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"With type specification: {df_typed.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Memory saved: {((df.memory_usage(deep=True).sum() - df_typed.memory_usage(deep=True).sum()) / df.memory_usage(deep=True).sum() * 100):.1f}%\")\n",
    "\n",
    "# 5. HANDLING PROBLEMATIC CSV FILES\n",
    "print(\"\\n5. HANDLING PROBLEMATIC CSV FILES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create CSV with issues\n",
    "problematic_csv = \"\"\"Name;Age;Score\n",
    "John;25;85.5\n",
    "Jane;N/A;92.0\n",
    "Bob;30;N/A\n",
    "Alice;28;88.5\"\"\"\n",
    "\n",
    "with open('problematic.csv', 'w') as f:\n",
    "    f.write(problematic_csv)\n",
    "\n",
    "print(\"\\nProblem: Semicolon delimiter + missing values as 'N/A'\")\n",
    "\n",
    "# Solution: Specify delimiter and missing values\n",
    "df_fixed = pd.read_csv('problematic.csv', \n",
    "                       sep=';',  # Semicolon separator\n",
    "                       na_values=['N/A', 'NA', 'null'])  # Treat as missing\n",
    "\n",
    "print(\"\\nFixed data:\")\n",
    "print(df_fixed)\n",
    "print(f\"\\nMissing values:\\n{df_fixed.isnull().sum()}\")\n",
    "\n",
    "# 6. READING LARGE FILES EFFICIENTLY\n",
    "print(\"\\n6. READING LARGE FILES EFFICIENTLY (CHUNKS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nReading in chunks (useful for files > 1 GB):\")\n",
    "\n",
    "# Read in chunks of 3 rows\n",
    "chunk_size = 3\n",
    "chunk_counter = 0\n",
    "total_salary = 0\n",
    "\n",
    "for chunk in pd.read_csv('sample_employees.csv', chunksize=chunk_size):\n",
    "    chunk_counter += 1\n",
    "    chunk_salary = chunk['Salary'].sum()\n",
    "    total_salary += chunk_salary\n",
    "    print(f\"Chunk {chunk_counter}: {len(chunk)} rows, Total Salary: ${chunk_salary:,.2f}\")\n",
    "\n",
    "print(f\"\\nProcessed {chunk_counter} chunks\")\n",
    "print(f\"Total salary across all chunks: ${total_salary:,.2f}\")\n",
    "\n",
    "# 7. DATA EXPLORATION\n",
    "print(\"\\n7. INITIAL DATA EXPLORATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Use the properly typed dataframe\n",
    "df = df_typed.copy()\n",
    "\n",
    "print(\"\\nBasic Information:\")\n",
    "print(f\"Total Employees: {len(df)}\")\n",
    "print(f\"Departments: {df['Department'].nunique()}\")\n",
    "print(f\"Cities: {df['City'].nunique()}\")\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df[['Age', 'Salary']].describe())\n",
    "\n",
    "print(\"\\nDepartment Distribution:\")\n",
    "print(df['Department'].value_counts())\n",
    "\n",
    "print(\"\\nAverage Salary by Department:\")\n",
    "print(df.groupby('Department')['Salary'].mean().sort_values(ascending=False))\n",
    "\n",
    "# 8. VISUALIZATIONS\n",
    "print(\"\\n8. CREATING VISUALIZATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Salary Distribution\n",
    "axes[0, 0].hist(df['Salary'], bins=10, edgecolor='black', color='skyblue')\n",
    "axes[0, 0].set_xlabel('Salary ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Salary Distribution')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Age vs Salary\n",
    "axes[0, 1].scatter(df['Age'], df['Salary'], s=100, alpha=0.6, c=df['Age'], cmap='viridis')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Salary ($)')\n",
    "axes[0, 1].set_title('Age vs Salary')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Department Counts\n",
    "dept_counts = df['Department'].value_counts()\n",
    "axes[1, 0].bar(dept_counts.index, dept_counts.values, color='coral')\n",
    "axes[1, 0].set_xlabel('Department')\n",
    "axes[1, 0].set_ylabel('Number of Employees')\n",
    "axes[1, 0].set_title('Employees by Department')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Average Salary by Department\n",
    "avg_salary = df.groupby('Department')['Salary'].mean().sort_values()\n",
    "axes[1, 1].barh(avg_salary.index, avg_salary.values, color='lightgreen')\n",
    "axes[1, 1].set_xlabel('Average Salary ($)')\n",
    "axes[1, 1].set_title('Average Salary by Department')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualizations created!\")\n",
    "\n",
    "# 9. BEST PRACTICES CHECKLIST\n",
    "print(\"\\n9. CSV READING BEST PRACTICES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úì Always check first few rows: df.head()\n",
    "‚úì Verify data types: df.dtypes\n",
    "‚úì Check for missing values: df.isnull().sum()\n",
    "‚úì Look at data shape: df.shape\n",
    "‚úì Use nrows for large file testing\n",
    "‚úì Specify dtypes to save memory\n",
    "‚úì Parse dates with parse_dates parameter\n",
    "‚úì Use usecols to read only needed columns\n",
    "‚úì Handle different delimiters with sep parameter\n",
    "‚úì Use chunksize for files > RAM size\n",
    "‚úì Consider Parquet for files > 1 GB\n",
    "\"\"\")\n",
    "\n",
    "# Clean up files\n",
    "import os\n",
    "os.remove('sample_employees.csv')\n",
    "os.remove('problematic.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì CSV reading is the foundation of data science\")\n",
    "print(\"‚úì pandas.read_csv() is your primary tool\")\n",
    "print(\"‚úì Always explore data immediately after loading\")\n",
    "print(\"‚úì Use parameters to optimize performance\")\n",
    "print(\"‚úì For large files, use chunks or switch to Parquet\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e13ec4",
   "metadata": {},
   "source": [
    "# 2. Reading Excel Files\n",
    "\n",
    "## üìñ What is Excel Reading?\n",
    "\n",
    "Excel files (.xlsx, .xls) are **spreadsheet files** commonly used in business. Reading Excel files allows data scientists to work with data from business teams, finance departments, and non-technical stakeholders.\n",
    "\n",
    "**Excel Features:**\n",
    "- Multiple sheets in one file\n",
    "- Formatted cells (colors, fonts)\n",
    "- Formulas and calculations\n",
    "- Charts and pivot tables\n",
    "\n",
    "**pandas.read_excel()** extracts data while ignoring formatting.\n",
    "\n",
    "## üéØ Why Use Excel Files?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Business Standard** - Everyone uses Excel\n",
    "2. **Multiple Sheets** - Organize related data together\n",
    "3. **Human-Readable** - Easy to review manually\n",
    "4. **Rich Formatting** - Visual organization\n",
    "5. **Familiar Interface** - Non-technical users comfortable\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Slow Performance** - Much slower than CSV\n",
    "2. **Size Limit** - Excel has 1,048,576 row limit\n",
    "3. **Proprietary Format** - Requires special libraries\n",
    "4. **Corruption Risk** - Binary format can break\n",
    "5. **Large File Size** - Bigger than CSV for same data\n",
    "\n",
    "## ‚è±Ô∏è When to Use Excel Files\n",
    "\n",
    "### ‚úÖ **Use When:**\n",
    "\n",
    "**1. Receiving Data from Business Teams**\n",
    "- Example: Monthly sales reports from finance\n",
    "- Why: Business teams work in Excel\n",
    "- Reality: You'll convert to CSV/Parquet later\n",
    "\n",
    "**2. Multiple Related Tables**\n",
    "- Example: One file with Sales, Products, Customers sheets\n",
    "- Why: Keeps related data organized\n",
    "- Benefit: Single file to manage\n",
    "\n",
    "**3. Small Datasets (< 100K rows)**\n",
    "- Example: Customer list, product catalog\n",
    "- Why: Performance is acceptable\n",
    "- Size: < 10 MB file\n",
    "\n",
    "**4. Ad-hoc Analysis Sharing**\n",
    "- Example: Send results to manager\n",
    "- Why: They can open and sort in Excel\n",
    "- Use case: Quick reports\n",
    "\n",
    "**5. Data Entry Templates**\n",
    "- Example: Survey responses, manual data collection\n",
    "- Why: Easy for non-programmers to fill\n",
    "- Benefit: Validation rules in Excel\n",
    "\n",
    "### ‚ùå **Don't Use When:**\n",
    "\n",
    "**1. Large Datasets (> 100K rows)**\n",
    "- Example: Transaction logs, sensor data\n",
    "- Better: CSV, Parquet, databases\n",
    "- Why: Extremely slow to load\n",
    "\n",
    "**2. Automated Pipelines**\n",
    "- Example: Daily ETL processes\n",
    "- Better: CSV, Parquet, SQL\n",
    "- Why: Excel parsing is slow and fragile\n",
    "\n",
    "**3. Version Control**\n",
    "- Example: Tracking data changes in Git\n",
    "- Better: CSV (text-based)\n",
    "- Why: Excel is binary, can't diff\n",
    "\n",
    "**4. Performance Critical**\n",
    "- Example: Real-time processing\n",
    "- Better: Parquet, feather, binary formats\n",
    "- Why: 10-100x slower than alternatives\n",
    "\n",
    "## üìä How It Works\n",
    "\n",
    "**pandas.read_excel() process:**\n",
    "1. Opens Excel file using openpyxl/xlrd engine\n",
    "2. Parses binary XML structure\n",
    "3. Extracts specified sheet\n",
    "4. Ignores formatting, keeps only values\n",
    "5. Converts to DataFrame\n",
    "\n",
    "**Common Parameters:**\n",
    "- `sheet_name` - Which sheet to read (0, 'Sheet1', or list)\n",
    "- `header` - Row for column names\n",
    "- `usecols` - Which columns to read (\"A:D\" or list)\n",
    "- `skiprows` - Rows to skip\n",
    "- `dtype` - Specify data types\n",
    "- `engine` - 'openpyxl' (.xlsx) or 'xlrd' (.xls)\n",
    "\n",
    "## üåç Real-World Applications\n",
    "\n",
    "1. **Business Reports** - Sales, revenue, KPIs from teams\n",
    "2. **Financial Analysis** - Budget data, expense reports\n",
    "3. **Survey Data** - Manual data collection results\n",
    "4. **Inventory Management** - Product lists, stock levels\n",
    "5. **HR Analytics** - Employee data, performance reviews\n",
    "6. **Academic Research** - Small datasets from colleagues\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "‚úÖ Common in business, unavoidable in corporate settings  \n",
    "‚úÖ Much slower than CSV - use only when necessary  \n",
    "‚úÖ Can read multiple sheets at once  \n",
    "‚úÖ Install openpyxl: `pip install openpyxl`  \n",
    "‚úÖ For large Excel files, convert to CSV first  \n",
    "‚úÖ Use `sheet_name=None` to read all sheets  \n",
    "‚úÖ Specify `usecols` to read only needed columns  \n",
    "‚úÖ Always save cleaned data as CSV/Parquet for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416bc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING EXCEL FILES - COMPLETE EXAMPLE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"READING EXCEL FILES - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. CREATE SAMPLE EXCEL FILE WITH MULTIPLE SHEETS\n",
    "print(\"\\n1. CREATING SAMPLE EXCEL FILE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create sample data for multiple sheets\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=10),\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop', \n",
    "                'Mouse', 'Keyboard', 'Monitor', 'Laptop', 'Mouse'],\n",
    "    'Quantity': [2, 5, 3, 1, 3, 7, 2, 2, 1, 4],\n",
    "    'Price': [1200, 25, 75, 300, 1200, 25, 75, 300, 1200, 25],\n",
    "    'Region': ['North', 'South', 'East', 'West', 'North', \n",
    "               'South', 'East', 'West', 'North', 'South']\n",
    "})\n",
    "\n",
    "products_data = pd.DataFrame({\n",
    "    'ProductID': [1, 2, 3, 4],\n",
    "    'ProductName': ['Laptop', 'Mouse', 'Keyboard', 'Monitor'],\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics'],\n",
    "    'Stock': [50, 200, 150, 75],\n",
    "    'Supplier': ['Dell', 'Logitech', 'Microsoft', 'Samsung']\n",
    "})\n",
    "\n",
    "customers_data = pd.DataFrame({\n",
    "    'CustomerID': range(1, 6),\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Alice Williams', 'Charlie Brown'],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin'],\n",
    "    'TotalSpent': [5000, 3500, 4200, 6000, 2800]\n",
    "})\n",
    "\n",
    "# Create Excel file with multiple sheets\n",
    "excel_file = 'sample_business_data.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "    sales_data.to_excel(writer, sheet_name='Sales', index=False)\n",
    "    products_data.to_excel(writer, sheet_name='Products', index=False)\n",
    "    customers_data.to_excel(writer, sheet_name='Customers', index=False)\n",
    "\n",
    "print(f\"‚úì Excel file created: '{excel_file}'\")\n",
    "print(\"  - Sales sheet: 10 rows\")\n",
    "print(\"  - Products sheet: 4 rows\")\n",
    "print(\"  - Customers sheet: 5 rows\")\n",
    "\n",
    "# 2. BASIC EXCEL READING\n",
    "print(\"\\n2. BASIC EXCEL READING (Single Sheet)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Read first sheet (default)\n",
    "df_sales = pd.read_excel(excel_file, sheet_name='Sales')\n",
    "\n",
    "print(\"‚úì Sales sheet loaded successfully!\")\n",
    "print(f\"\\nShape: {df_sales.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_sales.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df_sales.dtypes)\n",
    "\n",
    "# 3. READING MULTIPLE SHEETS\n",
    "print(\"\\n3. READING MULTIPLE SHEETS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Method 1: Read all sheets\n",
    "print(\"\\nMethod 1: Read all sheets at once\")\n",
    "all_sheets = pd.read_excel(excel_file, sheet_name=None)  # Returns dictionary\n",
    "\n",
    "print(f\"Sheets found: {list(all_sheets.keys())}\")\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    print(f\"  {sheet_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Method 2: Read specific sheets\n",
    "print(\"\\nMethod 2: Read specific sheets\")\n",
    "sheets_dict = pd.read_excel(excel_file, sheet_name=['Sales', 'Products'])\n",
    "\n",
    "df_sales = sheets_dict['Sales']\n",
    "df_products = sheets_dict['Products']\n",
    "\n",
    "print(\"‚úì Sales and Products sheets loaded\")\n",
    "print(f\"\\nProducts sheet:\")\n",
    "print(df_products)\n",
    "\n",
    "# 4. ADVANCED READING OPTIONS\n",
    "print(\"\\n4. ADVANCED READING OPTIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Option 1: Read specific columns (Excel notation)\n",
    "print(\"\\nOption 1: Read specific columns (A to C)\")\n",
    "df_subset = pd.read_excel(excel_file, sheet_name='Sales', usecols='A:C')\n",
    "print(f\"Columns: {df_subset.columns.tolist()}\")\n",
    "print(df_subset.head(3))\n",
    "\n",
    "# Option 2: Read specific columns (by name)\n",
    "print(\"\\nOption 2: Read specific columns by name\")\n",
    "df_subset2 = pd.read_excel(excel_file, sheet_name='Sales', \n",
    "                           usecols=['Product', 'Quantity', 'Price'])\n",
    "print(f\"Columns: {df_subset2.columns.tolist()}\")\n",
    "\n",
    "# Option 3: Skip rows\n",
    "print(\"\\nOption 3: Skip first 2 data rows\")\n",
    "df_skip = pd.read_excel(excel_file, sheet_name='Sales', skiprows=[1, 2])\n",
    "print(f\"Shape after skipping: {df_skip.shape}\")\n",
    "\n",
    "# Option 4: Read only first N rows\n",
    "print(\"\\nOption 4: Read only first 3 rows\")\n",
    "df_nrows = pd.read_excel(excel_file, sheet_name='Sales', nrows=3)\n",
    "print(df_nrows)\n",
    "\n",
    "# 5. CALCULATE REVENUE\n",
    "print(\"\\n5. DATA ANALYSIS - CALCULATING REVENUE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate total revenue per row\n",
    "df_sales['Revenue'] = df_sales['Quantity'] * df_sales['Price']\n",
    "\n",
    "print(\"\\nSales with Revenue:\")\n",
    "print(df_sales[['Product', 'Quantity', 'Price', 'Revenue']].head())\n",
    "\n",
    "print(\"\\nTotal Revenue by Product:\")\n",
    "revenue_by_product = df_sales.groupby('Product')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(revenue_by_product)\n",
    "\n",
    "print(\"\\nTotal Revenue by Region:\")\n",
    "revenue_by_region = df_sales.groupby('Region')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(revenue_by_region)\n",
    "\n",
    "print(f\"\\nOverall Total Revenue: ${df_sales['Revenue'].sum():,.2f}\")\n",
    "\n",
    "# 6. MERGE DATA FROM MULTIPLE SHEETS\n",
    "print(\"\\n6. MERGING DATA FROM MULTIPLE SHEETS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Add category information to sales\n",
    "df_sales_enriched = df_sales.merge(\n",
    "    df_products[['ProductName', 'Category', 'Supplier']], \n",
    "    left_on='Product', \n",
    "    right_on='ProductName',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nEnriched Sales Data (first 5 rows):\")\n",
    "print(df_sales_enriched[['Product', 'Category', 'Supplier', 'Revenue']].head())\n",
    "\n",
    "print(\"\\nRevenue by Category:\")\n",
    "category_revenue = df_sales_enriched.groupby('Category')['Revenue'].sum()\n",
    "print(category_revenue)\n",
    "\n",
    "# 7. PERFORMANCE COMPARISON\n",
    "print(\"\\n7. PERFORMANCE COMPARISON: Excel vs CSV\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "import time\n",
    "\n",
    "# Save as CSV\n",
    "csv_file = 'sales_data.csv'\n",
    "df_sales.to_csv(csv_file, index=False)\n",
    "\n",
    "# Time Excel reading\n",
    "start = time.time()\n",
    "df_excel = pd.read_excel(excel_file, sheet_name='Sales')\n",
    "excel_time = time.time() - start\n",
    "\n",
    "# Time CSV reading\n",
    "start = time.time()\n",
    "df_csv = pd.read_csv(csv_file)\n",
    "csv_time = time.time() - start\n",
    "\n",
    "print(f\"Excel reading time: {excel_time*1000:.2f} ms\")\n",
    "print(f\"CSV reading time: {csv_time*1000:.2f} ms\")\n",
    "print(f\"CSV is {excel_time/csv_time:.1f}x faster!\")\n",
    "\n",
    "# 8. VISUALIZATIONS\n",
    "print(\"\\n8. CREATING VISUALIZATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Revenue by Product\n",
    "revenue_by_product.plot(kind='bar', ax=axes[0, 0], color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Total Revenue by Product')\n",
    "axes[0, 0].set_ylabel('Revenue ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Revenue by Region\n",
    "revenue_by_region.plot(kind='pie', ax=axes[0, 1], autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Revenue Distribution by Region')\n",
    "axes[0, 1].set_ylabel('')\n",
    "\n",
    "# Plot 3: Sales Over Time\n",
    "daily_revenue = df_sales.groupby('Date')['Revenue'].sum()\n",
    "axes[1, 0].plot(daily_revenue.index, daily_revenue.values, marker='o', linewidth=2)\n",
    "axes[1, 0].set_title('Revenue Over Time')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Revenue ($)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Product Stock Levels\n",
    "df_products.plot(x='ProductName', y='Stock', kind='barh', ax=axes[1, 1], \n",
    "                color='lightgreen', legend=False)\n",
    "axes[1, 1].set_title('Product Stock Levels')\n",
    "axes[1, 1].set_xlabel('Stock Quantity')\n",
    "axes[1, 1].set_ylabel('Product')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualizations created!\")\n",
    "\n",
    "# 9. WRITING BACK TO EXCEL\n",
    "print(\"\\n9. WRITING RESULTS BACK TO EXCEL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create summary report\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Total Revenue', 'Total Quantity Sold', 'Average Price', 'Unique Products'],\n",
    "    'Value': [\n",
    "        f\"${df_sales['Revenue'].sum():,.2f}\",\n",
    "        df_sales['Quantity'].sum(),\n",
    "        f\"${df_sales['Price'].mean():.2f}\",\n",
    "        df_sales['Product'].nunique()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Write to new Excel file\n",
    "output_file = 'sales_analysis_output.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    df_sales_enriched.to_excel(writer, sheet_name='Detailed_Sales', index=False)\n",
    "    revenue_by_product.to_excel(writer, sheet_name='Revenue_by_Product')\n",
    "    summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "print(f\"‚úì Analysis saved to: '{output_file}'\")\n",
    "print(\"  - Detailed_Sales sheet\")\n",
    "print(\"  - Revenue_by_Product sheet\")\n",
    "print(\"  - Summary sheet\")\n",
    "\n",
    "# Clean up files\n",
    "import os\n",
    "os.remove(excel_file)\n",
    "os.remove(csv_file)\n",
    "os.remove(output_file)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Excel files are common in business environments\")\n",
    "print(\"‚úì Use pd.read_excel() with sheet_name parameter\")\n",
    "print(\"‚úì Can read multiple sheets at once (sheet_name=None)\")\n",
    "print(\"‚úì Much slower than CSV - convert for repeated use\")\n",
    "print(\"‚úì Install openpyxl: pip install openpyxl\")\n",
    "print(\"‚úì Use usecols to read only needed columns\")\n",
    "print(\"‚úì Best for: Small datasets, business reports, multi-sheet data\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
