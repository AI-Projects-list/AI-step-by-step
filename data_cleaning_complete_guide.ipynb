{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dee13bc",
   "metadata": {},
   "source": [
    "# 1. Understanding Data Quality Issues\n",
    "\n",
    "## What Are Data Quality Issues?\n",
    "\n",
    "Data quality issues are problems that make data unreliable, inaccurate, or unsuitable for analysis. Understanding these issues is the first step in effective data cleaning.\n",
    "\n",
    "## Types of Data Quality Problems\n",
    "\n",
    "### 1. **Missing Values**\n",
    "- **What**: Absent or null data entries\n",
    "- **Why it happens**: Data entry errors, system failures, optional fields\n",
    "- **Impact**: Reduced sample size, biased analysis, model errors\n",
    "- **Examples**: NaN, None, NULL, empty strings, placeholder values (999, -1)\n",
    "\n",
    "### 2. **Duplicates**\n",
    "- **What**: Repeated records in dataset\n",
    "- **Why it happens**: Data integration, human error, system glitches\n",
    "- **Impact**: Inflated statistics, skewed analysis, wasted storage\n",
    "- **Examples**: Exact duplicates, fuzzy duplicates (John vs. Jon)\n",
    "\n",
    "### 3. **Inconsistent Data**\n",
    "- **What**: Same information represented differently\n",
    "- **Why it happens**: Multiple data sources, lack of standards, manual entry\n",
    "- **Impact**: Fragmented analysis, incorrect grouping, aggregation errors\n",
    "- **Examples**: USA vs US vs United States, 01/02/2020 vs 2020-01-02\n",
    "\n",
    "### 4. **Incorrect Data Types**\n",
    "- **What**: Data stored in wrong format\n",
    "- **Why it happens**: Poor schema design, automatic type inference errors\n",
    "- **Impact**: Calculation errors, sorting issues, analysis failures\n",
    "- **Examples**: Numbers as strings ('123'), dates as strings\n",
    "\n",
    "### 5. **Outliers & Anomalies**\n",
    "- **What**: Values significantly different from others\n",
    "- **Why it happens**: Measurement errors, data entry mistakes, genuine extreme values\n",
    "- **Impact**: Skewed statistics, distorted models, misleading visualizations\n",
    "- **Examples**: Age = 999, negative prices, impossible dates\n",
    "\n",
    "### 6. **Invalid Values**\n",
    "- **What**: Values that don't make logical sense\n",
    "- **Why it happens**: Human error, system bugs, data corruption\n",
    "- **Impact**: Analysis failures, incorrect conclusions\n",
    "- **Examples**: Age = -5, date in future for birth date, negative quantities\n",
    "\n",
    "### 7. **Formatting Issues**\n",
    "- **What**: Inconsistent formatting within data\n",
    "- **Why it happens**: Multiple data sources, lack of validation\n",
    "- **Impact**: Matching problems, aggregation errors\n",
    "- **Examples**: Extra whitespace, mixed case, special characters\n",
    "\n",
    "### 8. **Encoding Problems**\n",
    "- **What**: Character encoding mismatches\n",
    "- **Why it happens**: Different systems use different encodings\n",
    "- **Impact**: Garbled text, data loss, processing errors\n",
    "- **Examples**: Ã© instead of é, â€™ instead of '\n",
    "\n",
    "## When to Address Each Issue\n",
    "\n",
    "| Issue | When to Address | Priority |\n",
    "|-------|----------------|----------|\n",
    "| Missing Values | Before analysis/modeling | HIGH |\n",
    "| Duplicates | Immediately after loading data | HIGH |\n",
    "| Incorrect Types | Before any operations | HIGH |\n",
    "| Inconsistent Data | Before grouping/aggregation | MEDIUM |\n",
    "| Outliers | After understanding domain | MEDIUM |\n",
    "| Invalid Values | During validation phase | HIGH |\n",
    "| Formatting | Before string operations | LOW |\n",
    "| Encoding | At data loading stage | HIGH |\n",
    "\n",
    "## Why Understanding Is Important\n",
    "\n",
    "- **Choose right cleaning method**: Different problems need different solutions\n",
    "- **Prioritize efforts**: Fix high-impact issues first\n",
    "- **Prevent future issues**: Understand root causes\n",
    "- **Communicate with stakeholders**: Explain data quality concerns\n",
    "- **Validate solutions**: Know when cleaning is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88457ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example: Dataset with multiple quality issues\n",
    "dirty_data = {\n",
    "    'customer_id': [1, 2, 2, 3, 4, 5, None, 7],\n",
    "    'name': ['John Doe', 'jane smith', 'Jane Smith', 'Bob Johnson', None, '  Alice  ', 'Charlie', 'David'],\n",
    "    'age': ['25', '30', '30', 'invalid', '150', '28', '35', '-5'],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'jane@email.com', 'bob@', None, 'alice@email.com', 'charlie@email.com', 'david@email.com'],\n",
    "    'purchase_amount': ['100.50', '200', '200.00', '50.75', '9999999', None, '75.25', '30'],\n",
    "    'country': ['USA', 'US', 'United States', 'UK', 'usa', 'USA', 'Canada', 'US']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dirty_data)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE: Dataset with Multiple Data Quality Issues\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRaw Data:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Missing Values Analysis\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_report = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_report[missing_report['Missing Count'] > 0])\n",
    "\n",
    "# 2. Duplicate Detection\n",
    "print(\"\\n2. DUPLICATES:\")\n",
    "duplicates = df.duplicated()\n",
    "print(f\"   Total duplicate rows: {duplicates.sum()}\")\n",
    "print(f\"   Duplicate customer_ids: {df['customer_id'].duplicated().sum()}\")\n",
    "\n",
    "# 3. Data Type Issues\n",
    "print(\"\\n3. DATA TYPE ISSUES:\")\n",
    "print(\"   Current types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n   Issues:\")\n",
    "print(\"   - 'age' should be numeric, currently: object\")\n",
    "print(\"   - 'purchase_amount' should be numeric, currently: object\")\n",
    "\n",
    "# 4. Inconsistent Values\n",
    "print(\"\\n4. INCONSISTENT VALUES:\")\n",
    "print(\"   Country variations:\")\n",
    "print(df['country'].value_counts())\n",
    "\n",
    "# 5. Invalid Values\n",
    "print(\"\\n5. INVALID VALUES:\")\n",
    "print(\"   Detected issues:\")\n",
    "print(\"   - age contains 'invalid', '150', '-5'\")\n",
    "print(\"   - email contains 'bob@' (invalid format)\")\n",
    "print(\"   - purchase_amount contains '9999999' (potential outlier)\")\n",
    "\n",
    "# 6. Formatting Issues\n",
    "print(\"\\n6. FORMATTING ISSUES:\")\n",
    "print(\"   - Mixed case in 'name': 'John Doe' vs 'jane smith'\")\n",
    "print(\"   - Extra whitespace: '  Alice  '\")\n",
    "print(\"   - Mixed case in 'country': 'USA' vs 'usa'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Data quality issues identified!\")\n",
    "print(\"✓ Next step: Apply appropriate cleaning techniques\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65779552",
   "metadata": {},
   "source": [
    "# 2. Data Profiling & Initial Assessment\n",
    "\n",
    "## What is Data Profiling?\n",
    "\n",
    "Data profiling is the systematic examination of data to understand its structure, content, quality, and relationships. It's the essential first step before any cleaning activity.\n",
    "\n",
    "## Why Data Profiling?\n",
    "\n",
    "### 1. **Understand Your Data**\n",
    "- Know what you're working with before making changes\n",
    "- Identify patterns and anomalies\n",
    "- Understand data distribution and characteristics\n",
    "\n",
    "### 2. **Plan Cleaning Strategy**\n",
    "- Prioritize cleaning tasks based on severity\n",
    "- Choose appropriate cleaning techniques\n",
    "- Estimate time and resources needed\n",
    "\n",
    "### 3. **Set Quality Benchmarks**\n",
    "- Establish baseline metrics\n",
    "- Define acceptable quality levels\n",
    "- Track improvement over time\n",
    "\n",
    "### 4. **Communicate with Stakeholders**\n",
    "- Document data quality issues\n",
    "- Justify cleaning decisions\n",
    "- Set expectations for data improvements\n",
    "\n",
    "## When to Profile Data\n",
    "\n",
    "- **Immediately after loading**: First look at raw data\n",
    "- **Before analysis**: Ensure data is suitable for intended use\n",
    "- **After cleaning**: Verify cleaning effectiveness\n",
    "- **Regular intervals**: Monitor ongoing data quality\n",
    "- **After data updates**: Check for new issues\n",
    "\n",
    "## Key Profiling Activities\n",
    "\n",
    "### 1. **Structural Profiling**\n",
    "- Number of rows and columns\n",
    "- Column names and data types\n",
    "- Memory usage\n",
    "- Index structure\n",
    "\n",
    "### 2. **Content Profiling**\n",
    "- Unique values count\n",
    "- Value distributions\n",
    "- Min, max, mean, median\n",
    "- Null/missing counts\n",
    "\n",
    "### 3. **Quality Profiling**\n",
    "- Missing value percentages\n",
    "- Duplicate counts\n",
    "- Outlier detection\n",
    "- Invalid value identification\n",
    "\n",
    "### 4. **Relationship Profiling**\n",
    "- Correlations between columns\n",
    "- Dependencies and constraints\n",
    "- Cross-field validation\n",
    "\n",
    "## Profiling Tools\n",
    "\n",
    "- **Manual**: pandas describe(), info(), value_counts()\n",
    "- **Automated**: pandas-profiling, sweetviz, ydata-profiling\n",
    "- **Visual**: matplotlib, seaborn for distribution plots\n",
    "- **Custom**: Build your own profiling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Data Profiling Example\n",
    "\n",
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'transaction_id': range(1, n+1),\n",
    "    'customer_id': np.random.randint(1, 100, n),\n",
    "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Headphones', None], n),\n",
    "    'quantity': np.random.randint(1, 10, n),\n",
    "    'price': np.random.uniform(10, 1000, n),\n",
    "    'discount': np.random.choice([0, 5, 10, 15, 20, None], n),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash'], n),\n",
    "    'status': np.random.choice(['Completed', 'Pending', 'Cancelled'], n, p=[0.7, 0.2, 0.1])\n",
    "})\n",
    "\n",
    "# Add some duplicates\n",
    "sample_data = pd.concat([sample_data, sample_data.sample(50)], ignore_index=True)\n",
    "\n",
    "# Add some missing values\n",
    "sample_data.loc[np.random.choice(sample_data.index, 100, replace=False), 'price'] = np.nan\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE DATA PROFILING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. STRUCTURAL PROFILING\n",
    "print(\"\\n1. STRUCTURAL PROFILING\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset Shape: {sample_data.shape[0]:,} rows × {sample_data.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {sample_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nColumn Information:\")\n",
    "print(sample_data.dtypes)\n",
    "\n",
    "# 2. CONTENT PROFILING\n",
    "print(\"\\n2. CONTENT PROFILING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "for col in sample_data.columns:\n",
    "    unique_count = sample_data[col].nunique()\n",
    "    unique_pct = (unique_count / len(sample_data) * 100)\n",
    "    print(f\"   {col:20s}: {unique_count:5d} unique ({unique_pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "print(sample_data.describe())\n",
    "\n",
    "print(\"\\nCategorical Value Distributions:\")\n",
    "for col in ['product', 'payment_method', 'status']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(sample_data[col].value_counts())\n",
    "\n",
    "# 3. QUALITY PROFILING\n",
    "print(\"\\n3. QUALITY PROFILING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': sample_data.columns,\n",
    "    'Missing_Count': sample_data.isnull().sum().values,\n",
    "    'Missing_Pct': (sample_data.isnull().sum() / len(sample_data) * 100).values\n",
    "})\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0].to_string(index=False))\n",
    "\n",
    "print(\"\\nDuplicate Analysis:\")\n",
    "dup_count = sample_data.duplicated().sum()\n",
    "dup_pct = (dup_count / len(sample_data) * 100)\n",
    "print(f\"   Total duplicates: {dup_count} ({dup_pct:.2f}%)\")\n",
    "\n",
    "print(\"\\nOutlier Detection (Numerical Columns):\")\n",
    "for col in ['quantity', 'price', 'discount']:\n",
    "    if sample_data[col].dtype in ['int64', 'float64']:\n",
    "        Q1 = sample_data[col].quantile(0.25)\n",
    "        Q3 = sample_data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outliers = sample_data[(sample_data[col] < lower) | (sample_data[col] > upper)]\n",
    "        print(f\"   {col:15s}: {len(outliers):4d} outliers ({len(outliers)/len(sample_data)*100:.2f}%)\")\n",
    "\n",
    "# 4. CUSTOM QUALITY CHECKS\n",
    "print(\"\\n4. CUSTOM QUALITY CHECKS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Business rule validations\n",
    "print(\"\\nBusiness Rule Validations:\")\n",
    "invalid_qty = (sample_data['quantity'] <= 0).sum()\n",
    "invalid_price = (sample_data['price'] <= 0).sum()\n",
    "invalid_discount = (sample_data['discount'] > 100).sum()\n",
    "\n",
    "print(f\"   Invalid quantity (≤0): {invalid_qty}\")\n",
    "print(f\"   Invalid price (≤0): {invalid_price}\")\n",
    "print(f\"   Invalid discount (>100): {invalid_discount}\")\n",
    "\n",
    "# 5. DATA QUALITY SCORE\n",
    "print(\"\\n5. DATA QUALITY SCORE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_cells = sample_data.shape[0] * sample_data.shape[1]\n",
    "missing_cells = sample_data.isnull().sum().sum()\n",
    "completeness = ((total_cells - missing_cells) / total_cells * 100)\n",
    "\n",
    "unique_pct = (sample_data.duplicated().sum() / len(sample_data) * 100)\n",
    "uniqueness = 100 - unique_pct\n",
    "\n",
    "quality_score = (completeness + uniqueness) / 2\n",
    "\n",
    "print(f\"   Completeness: {completeness:.2f}%\")\n",
    "print(f\"   Uniqueness: {uniqueness:.2f}%\")\n",
    "print(f\"   Overall Quality Score: {quality_score:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Profiling complete!\")\n",
    "print(\"✓ Use insights to plan cleaning strategy\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becb02c",
   "metadata": {},
   "source": [
    "# 3. Handling Missing Values\n",
    "\n",
    "## What are Missing Values?\n",
    "\n",
    "Missing values are absent or undefined data entries in a dataset. They appear as NaN (Not a Number), None, NULL, or empty cells.\n",
    "\n",
    "## Why Missing Values Occur\n",
    "\n",
    "1. **Data Entry Errors**: Human mistakes during manual entry\n",
    "2. **System Failures**: Technical issues during data collection\n",
    "3. **Optional Fields**: Survey questions not answered\n",
    "4. **Data Integration**: Merging datasets with different schemas\n",
    "5. **Privacy**: Sensitive information intentionally omitted\n",
    "6. **Not Applicable**: Value doesn't apply to certain records\n",
    "\n",
    "## When to Handle Missing Values\n",
    "\n",
    "- **Before Analysis**: Missing data affects statistics and models\n",
    "- **Before Modeling**: Most ML algorithms can't handle NaN\n",
    "- **During Validation**: Check if missingness is acceptable\n",
    "- **After Understanding**: Know WHY data is missing before fixing\n",
    "\n",
    "## Types of Missingness\n",
    "\n",
    "### 1. **MCAR (Missing Completely At Random)**\n",
    "- No pattern to missing data\n",
    "- Safe to delete or impute\n",
    "- Example: Random survey non-response\n",
    "\n",
    "### 2. **MAR (Missing At Random)**\n",
    "- Missing related to observed data\n",
    "- Can be predicted from other variables\n",
    "- Example: Income missing for younger respondents\n",
    "\n",
    "### 3. **MNAR (Missing Not At Random)**\n",
    "- Missing related to the missing value itself\n",
    "- Most problematic type\n",
    "- Example: High earners don't report income\n",
    "\n",
    "## Handling Strategies\n",
    "\n",
    "### 1. **Deletion**\n",
    "**When to use**:\n",
    "- < 5% data missing\n",
    "- MCAR missingness\n",
    "- Large dataset with plenty of data\n",
    "\n",
    "**Methods**:\n",
    "- Listwise deletion (drop rows)\n",
    "- Column deletion (drop columns)\n",
    "\n",
    "**Pros**: Simple, no assumptions\n",
    "**Cons**: Loss of data, potential bias\n",
    "\n",
    "### 2. **Imputation (Filling)**\n",
    "**When to use**:\n",
    "- >5% missing data\n",
    "- Can't afford to lose data\n",
    "- Pattern exists in missingness\n",
    "\n",
    "**Methods**:\n",
    "- Mean/Median/Mode\n",
    "- Forward/Backward fill (time series)\n",
    "- Interpolation\n",
    "- Predictive modeling\n",
    "- Multiple imputation\n",
    "\n",
    "**Pros**: Retains data size\n",
    "**Cons**: May introduce bias, assumes patterns\n",
    "\n",
    "### 3. **Flagging**\n",
    "**When to use**:\n",
    "- Missingness is informative\n",
    "- Want to preserve that information\n",
    "\n",
    "**Method**: Create indicator variable\n",
    "**Pros**: Preserves information about missingness\n",
    "**Cons**: Increases dimensions\n",
    "\n",
    "## Why Each Method Works\n",
    "\n",
    "| Method | Why It Works | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| Mean | Preserves central tendency | Normally distributed data |\n",
    "| Median | Robust to outliers | Skewed distributions |\n",
    "| Mode | Most common value | Categorical data |\n",
    "| Forward Fill | Assumes continuity | Time series, ordered data |\n",
    "| Interpolation | Smooth transitions | Temporal or spatial data |\n",
    "| KNN | Uses similar records | Pattern-based missingness |\n",
    "| MICE | Multiple iterations | Complex patterns |\n",
    "\n",
    "## Impact of Missing Data\n",
    "\n",
    "- **Statistical Power**: Reduced sample size\n",
    "- **Bias**: Non-random missingness introduces bias\n",
    "- **Model Performance**: Lower accuracy, wrong conclusions\n",
    "- **Computation**: Some algorithms fail with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Missing Value Handling Examples\n",
    "\n",
    "# Create dataset with missing values\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, n),\n",
    "    'income': np.random.normal(50000, 20000, n),\n",
    "    'credit_score': np.random.randint(300, 850, n),\n",
    "    'years_employed': np.random.randint(0, 40, n),\n",
    "    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston'], n)\n",
    "})\n",
    "\n",
    "# Introduce missing values\n",
    "data.loc[np.random.choice(data.index, 15, replace=False), 'income'] = np.nan\n",
    "data.loc[np.random.choice(data.index, 10, replace=False), 'credit_score'] = np.nan\n",
    "data.loc[np.random.choice(data.index, 5, replace=False), 'years_employed'] = np.nan\n",
    "data.loc[np.random.choice(data.index, 8, replace=False), 'city'] = np.nan\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MISSING VALUE HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. DETECT MISSING VALUES\n",
    "print(\"\\n1. MISSING VALUE DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nMissing Count by Column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Percentage by Column:\")\n",
    "missing_pct = (data.isnull().sum() / len(data) * 100).round(2)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "\n",
    "print(f\"\\nTotal rows with any missing: {data.isnull().any(axis=1).sum()} ({data.isnull().any(axis=1).sum()/len(data)*100:.1f}%)\")\n",
    "print(f\"Complete rows (no missing): {(~data.isnull().any(axis=1)).sum()} ({(~data.isnull().any(axis=1)).sum()/len(data)*100:.1f}%)\")\n",
    "\n",
    "# 2. DELETION STRATEGIES\n",
    "print(\"\\n2. DELETION STRATEGIES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Drop rows with any missing\n",
    "data_dropna = data.dropna()\n",
    "print(f\"\\nListwise deletion (drop any NaN): {len(data)} → {len(data_dropna)} rows ({len(data_dropna)/len(data)*100:.1f}% retained)\")\n",
    "\n",
    "# Drop rows with missing in specific columns\n",
    "data_drop_subset = data.dropna(subset=['income', 'credit_score'])\n",
    "print(f\"Drop if income OR credit_score missing: {len(data)} → {len(data_drop_subset)} rows\")\n",
    "\n",
    "# Drop columns with too many missing\n",
    "threshold = 0.3  # Drop if >30% missing\n",
    "data_drop_cols = data.dropna(thresh=len(data)*(1-threshold), axis=1)\n",
    "print(f\"Drop columns with >{threshold*100}% missing: {data.shape[1]} → {data_drop_cols.shape[1]} columns\")\n",
    "\n",
    "# 3. IMPUTATION STRATEGIES\n",
    "print(\"\\n3. IMPUTATION STRATEGIES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "# Mean imputation (for income)\n",
    "mean_income = data_imputed['income'].mean()\n",
    "data_imputed['income'].fillna(mean_income, inplace=True)\n",
    "print(f\"\\nIncome: Mean imputation ({mean_income:.2f})\")\n",
    "\n",
    "# Median imputation (for credit_score - more robust to outliers)\n",
    "median_credit = data_imputed['credit_score'].median()\n",
    "data_imputed['credit_score'].fillna(median_credit, inplace=True)\n",
    "print(f\"Credit Score: Median imputation ({median_credit:.0f})\")\n",
    "\n",
    "# Mode imputation (for categorical - city)\n",
    "mode_city = data_imputed['city'].mode()[0]\n",
    "data_imputed['city'].fillna(mode_city, inplace=True)\n",
    "print(f\"City: Mode imputation ('{mode_city}')\")\n",
    "\n",
    "# Constant value (for years_employed)\n",
    "data_imputed['years_employed'].fillna(0, inplace=True)\n",
    "print(f\"Years Employed: Constant value (0)\")\n",
    "\n",
    "print(f\"\\nRemaining missing values: {data_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# 4. ADVANCED IMPUTATION\n",
    "print(\"\\n4. ADVANCED IMPUTATION TECHNIQUES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Forward fill (for time series - example)\n",
    "data_ffill = data.copy()\n",
    "data_ffill = data_ffill.fillna(method='ffill')\n",
    "print(f\"\\nForward Fill: {data.isnull().sum().sum()} → {data_ffill.isnull().sum().sum()} missing\")\n",
    "\n",
    "# Backward fill\n",
    "data_bfill = data.copy()\n",
    "data_bfill = data_bfill.fillna(method='bfill')\n",
    "print(f\"Backward Fill: {data.isnull().sum().sum()} → {data_bfill.isnull().sum().sum()} missing\")\n",
    "\n",
    "# Interpolation (for numerical columns)\n",
    "data_interp = data.copy()\n",
    "data_interp[['income', 'credit_score']] = data_interp[['income', 'credit_score']].interpolate(method='linear')\n",
    "print(f\"Interpolation: {data['income'].isnull().sum()} → {data_interp['income'].isnull().sum()} missing in income\")\n",
    "\n",
    "# 5. MISSING VALUE INDICATOR\n",
    "print(\"\\n5. MISSING VALUE INDICATOR (FLAGGING)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "data_flagged = data.copy()\n",
    "\n",
    "# Create indicator columns\n",
    "data_flagged['income_missing'] = data_flagged['income'].isnull().astype(int)\n",
    "data_flagged['credit_missing'] = data_flagged['credit_score'].isnull().astype(int)\n",
    "\n",
    "# Then fill the actual values\n",
    "data_flagged['income'].fillna(data_flagged['income'].mean(), inplace=True)\n",
    "data_flagged['credit_score'].fillna(data_flagged['credit_score'].median(), inplace=True)\n",
    "\n",
    "print(\"\\nCreated indicator columns:\")\n",
    "print(f\"   income_missing: {data_flagged['income_missing'].sum()} cases flagged\")\n",
    "print(f\"   credit_missing: {data_flagged['credit_missing'].sum()} cases flagged\")\n",
    "\n",
    "# 6. COMPARISON OF METHODS\n",
    "print(\"\\n6. METHOD COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Original', 'Deletion', 'Mean/Median', 'Forward Fill', 'Interpolation', 'Flagged'],\n",
    "    'Rows': [len(data), len(data_dropna), len(data_imputed), len(data_ffill), len(data_interp), len(data_flagged)],\n",
    "    'Columns': [data.shape[1], data_dropna.shape[1], data_imputed.shape[1], data_ffill.shape[1], data_interp.shape[1], data_flagged.shape[1]],\n",
    "    'Missing': [data.isnull().sum().sum(), data_dropna.isnull().sum().sum(), data_imputed.isnull().sum().sum(), \n",
    "                data_ffill.isnull().sum().sum(), data_interp.isnull().sum().sum(), data_flagged[data.columns].isnull().sum().sum()]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"• Use DELETION when <5% missing and MCAR\")\n",
    "print(\"• Use MEAN for normal distributions\")\n",
    "print(\"• Use MEDIAN for skewed data or presence of outliers\")\n",
    "print(\"• Use MODE for categorical variables\")\n",
    "print(\"• Use FORWARD/BACKWARD FILL for time series\")\n",
    "print(\"• Use INTERPOLATION for continuous numerical data\")\n",
    "print(\"• Use FLAGGING when missingness is informative\")\n",
    "print(\"• Always document which method you used and why\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
