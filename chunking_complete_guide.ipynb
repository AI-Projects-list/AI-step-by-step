{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692cbd0d",
   "metadata": {},
   "source": [
    "# 1. What is Chunking?\n",
    "\n",
    "## ðŸ“– What is Chunking?\n",
    "\n",
    "**Chunking** is the process of breaking down large pieces of text or data into smaller, manageable segments called \"chunks\" while preserving meaning and context.\n",
    "\n",
    "**Types of Chunking:**\n",
    "1. **Text Chunking**: Split documents into paragraphs, sentences, or fixed-size blocks\n",
    "2. **Linguistic Chunking**: Extract noun phrases, verb phrases (NLP)\n",
    "3. **Semantic Chunking**: Group by meaning, topics, or coherence\n",
    "4. **Data Chunking**: Process large datasets in batches\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Document (10,000 words)\n",
    "    â†“ Chunk into paragraphs\n",
    "50 chunks (200 words each)\n",
    "    â†“ Process individually\n",
    "Embeddings, summaries, or analysis\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Chunk Size**: Number of characters, words, tokens, or sentences\n",
    "- **Overlap**: Shared content between consecutive chunks\n",
    "- **Boundaries**: Where one chunk ends and another begins\n",
    "- **Context Preservation**: Maintaining meaning across chunks\n",
    "\n",
    "## ðŸŽ¯ Why Use Chunking?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Handle Large Documents** - Process documents larger than model context limits\n",
    "2. **Memory Efficiency** - Avoid loading entire document into memory\n",
    "3. **Parallel Processing** - Process chunks independently and concurrently\n",
    "4. **Better Search/Retrieval** - Find relevant sections instead of whole documents\n",
    "5. **Improved Embeddings** - More focused, meaningful vector representations\n",
    "6. **Cost Optimization** - Pay only for relevant chunks in LLM APIs\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Context Loss** - Information split across chunks may lose coherence\n",
    "2. **Boundary Issues** - Important content might be split awkwardly\n",
    "3. **Complexity** - Need to manage chunk relationships and metadata\n",
    "4. **Overhead** - Processing and merging chunks adds computational cost\n",
    "\n",
    "## â±ï¸ When to Use Chunking\n",
    "\n",
    "### âœ… **Use When:**\n",
    "\n",
    "**1. LLM Context Window Limitations**\n",
    "- Example: GPT-3.5 has 4,096 token limit, document is 50,000 tokens\n",
    "- Why: Must split document to fit in context window\n",
    "- Solution: Chunk into 4,000-token segments with 200-token overlap\n",
    "\n",
    "**2. Building RAG (Retrieval-Augmented Generation) Systems**\n",
    "- Example: Create Q&A system over company documentation\n",
    "- Why: Need to retrieve only relevant sections\n",
    "- Benefit: Faster retrieval, more accurate answers\n",
    "\n",
    "**3. Large Document Processing**\n",
    "- Example: Analyze 500-page PDF report\n",
    "- Why: Can't process entire document at once\n",
    "- Use case: Extract insights from each chapter separately\n",
    "\n",
    "**4. Semantic Search**\n",
    "- Example: Search across millions of articles\n",
    "- Why: Paragraph-level chunks give better precision\n",
    "- Benefit: Find exact relevant passage, not just document\n",
    "\n",
    "**5. Memory-Constrained Environments**\n",
    "- Example: Process 10GB dataset on laptop with 8GB RAM\n",
    "- Why: Can't load entire dataset\n",
    "- Solution: Process in 100MB chunks\n",
    "\n",
    "**6. Streaming Data Processing**\n",
    "- Example: Analyze real-time logs from web server\n",
    "- Why: Continuous stream, can't wait for \"complete\" data\n",
    "- Use case: Process chunks as they arrive\n",
    "\n",
    "**7. Fine-Tuning Data Preparation**\n",
    "- Example: Prepare training data for language model\n",
    "- Why: Models need fixed-size input sequences\n",
    "- Benefit: Create consistent training examples\n",
    "\n",
    "**8. Summarization of Long Documents**\n",
    "- Example: Summarize 100-page research paper\n",
    "- Why: Can't fit entire paper in summarization model\n",
    "- Solution: Summarize each section, then combine summaries\n",
    "\n",
    "### âŒ **Don't Use When:**\n",
    "\n",
    "**1. Small Documents**\n",
    "- Problem: Overhead not worth it for 100-word text\n",
    "- Better: Process entire document directly\n",
    "- Why: Chunking adds complexity for no benefit\n",
    "\n",
    "**2. Highly Interconnected Content**\n",
    "- Problem: Math proofs, code logic loses meaning when split\n",
    "- Better: Keep as single unit or use smart boundaries\n",
    "- Why: Context dependencies are critical\n",
    "\n",
    "**3. Real-Time Low-Latency Needs**\n",
    "- Problem: Chunking adds processing time\n",
    "- Better: Use streaming without chunking\n",
    "- Why: Extra processing step increases latency\n",
    "\n",
    "**4. When Order Matters Critically**\n",
    "- Problem: Time-series data, narratives need sequence\n",
    "- Better: Process sequentially or use overlap\n",
    "- Why: Chunks may be processed out of order\n",
    "\n",
    "## ðŸ“Š How It Works\n",
    "\n",
    "**Chunking Pipeline:**\n",
    "1. **Load Document**: Read text from file/database\n",
    "2. **Choose Strategy**: Fixed-size, sentence-based, semantic, etc.\n",
    "3. **Set Parameters**: Chunk size, overlap amount\n",
    "4. **Split Text**: Apply chunking algorithm\n",
    "5. **Add Metadata**: Track position, source, chunk ID\n",
    "6. **Process Chunks**: Embed, summarize, index, etc.\n",
    "7. **Store/Retrieve**: Save chunks for later use\n",
    "\n",
    "**Common Parameters:**\n",
    "- **chunk_size**: 500 tokens, 1000 characters, 3 sentences\n",
    "- **chunk_overlap**: 50 tokens (10% overlap)\n",
    "- **separator**: \"\\n\\n\", \".\", custom delimiter\n",
    "\n",
    "## ðŸŒ Real-World Applications\n",
    "\n",
    "1. **ChatGPT with Documents** - Chunk PDFs for Q&A over custom data\n",
    "2. **Google Search** - Index web pages in chunks for better retrieval\n",
    "3. **Legal Tech** - Analyze contracts section by section\n",
    "4. **Medical Research** - Process clinical trial documents by protocol sections\n",
    "5. **Customer Support** - Search knowledge base by article sections\n",
    "6. **Code Documentation** - Index API docs by function/class\n",
    "7. **E-Learning** - Break courses into digestible lessons\n",
    "8. **News Aggregation** - Process articles paragraph by paragraph\n",
    "9. **Academic Research** - Analyze papers by abstract/intro/methods/results\n",
    "10. **Financial Analysis** - Process quarterly reports by section\n",
    "\n",
    "## ðŸ’¡ Key Insights\n",
    "\n",
    "âœ… **Chunk size** is a tradeoff: too small loses context, too large loses focus  \n",
    "âœ… **Overlap** helps preserve context across boundaries  \n",
    "âœ… **Semantic chunking** (by meaning) often better than fixed-size  \n",
    "âœ… **Sentence boundaries** are safer than mid-word splits  \n",
    "âœ… **Metadata** (chunk position, source) is essential for reconstruction  \n",
    "âœ… **Test different strategies** - optimal chunking is task-dependent  \n",
    "âœ… **LLM chunking**: Use token count, not characters (tokenizers vary)  \n",
    "âœ… **RAG systems**: 200-500 tokens per chunk is typical sweet spot  \n",
    "âœ… **Hierarchical chunking**: Chunk at multiple levels (document â†’ section â†’ paragraph)  \n",
    "âœ… **Always preserve** original document structure (headers, lists, code blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNKING - COMPLETE INTRODUCTION\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHUNKING FUNDAMENTALS - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# NOTE: Install required libraries\n",
    "# pip install nltk spacy langchain\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# 1. BASIC CHUNKING CONCEPTS\n",
    "print(\"\\n1. BASIC CHUNKING CONCEPTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "document = \"\"\"Artificial intelligence is transforming the world. Machine learning enables \n",
    "computers to learn from data. Deep learning uses neural networks with multiple layers. \n",
    "Natural language processing helps computers understand human language. Computer vision \n",
    "allows machines to interpret visual information. Robotics combines AI with physical systems.\"\"\"\n",
    "\n",
    "print(f\"Original document ({len(document)} characters):\")\n",
    "print(document)\n",
    "\n",
    "# 2. FIXED-SIZE CHARACTER CHUNKING\n",
    "print(\"\\n2. FIXED-SIZE CHARACTER CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def chunk_by_characters(text: str, chunk_size: int, overlap: int = 0) -> List[str]:\n",
    "    \"\"\"Split text into fixed-size character chunks with optional overlap\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap  # Move back by overlap amount\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk into 100-character pieces\n",
    "char_chunks = chunk_by_characters(document, chunk_size=100, overlap=0)\n",
    "print(f\"Character chunks (size=100, overlap=0):\")\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "# With overlap\n",
    "char_chunks_overlap = chunk_by_characters(document, chunk_size=100, overlap=20)\n",
    "print(f\"\\nWith 20-character overlap:\")\n",
    "print(f\"  Total chunks: {len(char_chunks_overlap)}\")\n",
    "print(f\"  Overlap between chunks 1-2: '{char_chunks_overlap[0][-20:]}'\")\n",
    "print(f\"                           == '{char_chunks_overlap[1][:20]}'\")\n",
    "\n",
    "# 3. FIXED-SIZE WORD CHUNKING\n",
    "print(\"\\n3. FIXED-SIZE WORD CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def chunk_by_words(text: str, chunk_size: int, overlap: int = 0) -> List[str]:\n",
    "    \"\"\"Split text into fixed-size word chunks with optional overlap\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunks.append(' '.join(chunk_words))\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk into 15-word pieces\n",
    "word_chunks = chunk_by_words(document, chunk_size=15, overlap=0)\n",
    "print(f\"Word chunks (size=15 words, overlap=0):\")\n",
    "for i, chunk in enumerate(word_chunks, 1):\n",
    "    word_count = len(chunk.split())\n",
    "    print(f\"\\nChunk {i} ({word_count} words):\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "# With overlap\n",
    "word_chunks_overlap = chunk_by_words(document, chunk_size=15, overlap=5)\n",
    "print(f\"\\nWith 5-word overlap: {len(word_chunks_overlap)} chunks (vs {len(word_chunks)} without)\")\n",
    "\n",
    "# 4. SENTENCE-BASED CHUNKING\n",
    "print(\"\\n4. SENTENCE-BASED CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def chunk_by_sentences(text: str, sentences_per_chunk: int) -> List[str]:\n",
    "    \"\"\"Split text into chunks of N sentences\"\"\"\n",
    "    # Simple sentence split (basic)\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk_sents = sentences[i:i + sentences_per_chunk]\n",
    "        chunks.append('. '.join(chunk_sents) + '.')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "sent_chunks = chunk_by_sentences(document, sentences_per_chunk=2)\n",
    "print(f\"Sentence chunks (2 sentences per chunk):\")\n",
    "for i, chunk in enumerate(sent_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "# 5. NLTK SENTENCE CHUNKING\n",
    "print(\"\\n5. NLTK SENTENCE CHUNKING (BETTER)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    # nltk.download('punkt')\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    sentences_nltk = sent_tokenize(document)\n",
    "    print(f\"NLTK found {len(sentences_nltk)} sentences:\")\n",
    "    for i, sent in enumerate(sentences_nltk, 1):\n",
    "        print(f\"  {i}. {sent}\")\n",
    "    \n",
    "    # Chunk into groups of 2 sentences\n",
    "    def chunk_sentences_nltk(sentences: List[str], group_size: int) -> List[str]:\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), group_size):\n",
    "            chunk = ' '.join(sentences[i:i + group_size])\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    nltk_chunks = chunk_sentences_nltk(sentences_nltk, group_size=2)\n",
    "    print(f\"\\nGrouped into {len(nltk_chunks)} chunks (2 sentences each):\")\n",
    "    for i, chunk in enumerate(nltk_chunks, 1):\n",
    "        print(f\"\\nChunk {i}:\")\n",
    "        print(f\"  {chunk}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"NLTK not installed. Install with: pip install nltk\")\n",
    "except LookupError:\n",
    "    print(\"NLTK data not downloaded. Run: nltk.download('punkt')\")\n",
    "\n",
    "# 6. SLIDING WINDOW CHUNKING\n",
    "print(\"\\n6. SLIDING WINDOW CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def sliding_window_chunks(text: str, window_size: int, step_size: int) -> List[str]:\n",
    "    \"\"\"Create overlapping chunks using sliding window\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words) - window_size + 1, step_size):\n",
    "        chunk = ' '.join(words[i:i + window_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Window of 10 words, step of 5 (50% overlap)\n",
    "sliding_chunks = sliding_window_chunks(document, window_size=10, step_size=5)\n",
    "print(f\"Sliding window (window=10 words, step=5):\")\n",
    "print(f\"  Total chunks: {len(sliding_chunks)}\")\n",
    "for i, chunk in enumerate(sliding_chunks[:3], 1):  # Show first 3\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "# 7. PARAGRAPH-BASED CHUNKING\n",
    "print(\"\\n7. PARAGRAPH-BASED CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "multi_paragraph = \"\"\"This is the first paragraph. It discusses artificial intelligence.\n",
    "This paragraph has multiple sentences. All about AI.\n",
    "\n",
    "This is the second paragraph. It focuses on machine learning.\n",
    "Machine learning is a subset of AI. It enables computers to learn.\n",
    "\n",
    "This is the third paragraph. Deep learning is discussed here.\n",
    "Neural networks are the foundation. They have multiple layers.\"\"\"\n",
    "\n",
    "def chunk_by_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"Split text into paragraphs (separated by blank lines)\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    return [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "para_chunks = chunk_by_paragraphs(multi_paragraph)\n",
    "print(f\"Paragraph chunks: {len(para_chunks)} paragraphs\")\n",
    "for i, chunk in enumerate(para_chunks, 1):\n",
    "    print(f\"\\nParagraph {i}:\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "# 8. SMART CHUNKING (AVOID MID-SENTENCE SPLITS)\n",
    "print(\"\\n8. SMART CHUNKING (SENTENCE-AWARE)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def smart_chunk(text: str, max_chunk_size: int) -> List[str]:\n",
    "    \"\"\"Chunk text without breaking sentences\"\"\"\n",
    "    sentences = re.split(r'([.!?]+\\s+)', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for i in range(0, len(sentences), 2):\n",
    "        if i + 1 < len(sentences):\n",
    "            sentence = sentences[i] + sentences[i + 1]\n",
    "        else:\n",
    "            sentence = sentences[i]\n",
    "        \n",
    "        if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
    "            current_chunk += sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "smart_chunks = smart_chunk(document, max_chunk_size=150)\n",
    "print(f\"Smart chunks (max 150 chars, sentence-aware):\")\n",
    "for i, chunk in enumerate(smart_chunks, 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk}\")\n",
    "    print(f\"  Complete sentences: {chunk.count('.')}\")\n",
    "\n",
    "# 9. COMPARISON OF CHUNKING STRATEGIES\n",
    "print(\"\\n9. COMPARISON OF CHUNKING STRATEGIES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "test_text = \"\"\"Machine learning is amazing. It powers many applications. Deep learning is \n",
    "a subset. Neural networks are used extensively.\"\"\"\n",
    "\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"\\nDifferent chunking approaches:\")\n",
    "\n",
    "# Character-based\n",
    "char_result = chunk_by_characters(test_text, chunk_size=50, overlap=0)\n",
    "print(f\"\\n1. Character (50 chars): {len(char_result)} chunks\")\n",
    "print(f\"   May split mid-word: '{char_result[0]}'\")\n",
    "\n",
    "# Word-based\n",
    "word_result = chunk_by_words(test_text, chunk_size=7, overlap=0)\n",
    "print(f\"\\n2. Word (7 words): {len(word_result)} chunks\")\n",
    "print(f\"   Clean word boundaries: '{word_result[0]}'\")\n",
    "\n",
    "# Sentence-based\n",
    "sent_result = chunk_by_sentences(test_text, sentences_per_chunk=2)\n",
    "print(f\"\\n3. Sentence (2 sentences): {len(sent_result)} chunks\")\n",
    "print(f\"   Complete thoughts: '{sent_result[0]}'\")\n",
    "\n",
    "# Smart chunking\n",
    "smart_result = smart_chunk(test_text, max_chunk_size=60)\n",
    "print(f\"\\n4. Smart (max 60 chars, sentence-aware): {len(smart_result)} chunks\")\n",
    "print(f\"   Preserves sentences: '{smart_result[0]}'\")\n",
    "\n",
    "# 10. PRACTICAL EXAMPLE: CHUNKING FOR RAG\n",
    "print(\"\\n10. PRACTICAL EXAMPLE: CHUNKING FOR RAG SYSTEM\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "knowledge_base = \"\"\"Python is a high-level programming language. It was created by Guido van Rossum \n",
    "in 1991. Python emphasizes code readability with significant indentation.\n",
    "\n",
    "Machine learning in Python is popular due to libraries like scikit-learn, TensorFlow, and PyTorch. \n",
    "These libraries provide tools for building and training models. Python's simplicity makes it ideal \n",
    "for ML prototyping.\n",
    "\n",
    "Data science uses Python extensively. Libraries like pandas and numpy enable efficient data manipulation. \n",
    "Matplotlib and seaborn are used for visualization. Jupyter notebooks provide interactive environments.\"\"\"\n",
    "\n",
    "print(\"Knowledge base to chunk:\")\n",
    "print(knowledge_base)\n",
    "\n",
    "# Chunk by paragraphs for RAG\n",
    "rag_chunks = chunk_by_paragraphs(knowledge_base)\n",
    "\n",
    "print(f\"\\nChunked into {len(rag_chunks)} paragraphs for RAG:\")\n",
    "for i, chunk in enumerate(rag_chunks, 1):\n",
    "    # Simulate embedding (just show stats)\n",
    "    words = chunk.split()\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Text: {chunk[:100]}...\")\n",
    "    print(f\"  Words: {len(words)}\")\n",
    "    print(f\"  Characters: {len(chunk)}\")\n",
    "    print(f\"  Topic: {words[0]} ... {words[-1]}\")\n",
    "\n",
    "# Simulate retrieval\n",
    "query = \"machine learning libraries\"\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Best matching chunk: Chunk 2 (contains ML libraries)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Chunking breaks large text into manageable pieces\")\n",
    "print(\"âœ“ Character chunking: Simple but may split words/sentences\")\n",
    "print(\"âœ“ Word chunking: Respects word boundaries, better than characters\")\n",
    "print(\"âœ“ Sentence chunking: Preserves complete thoughts\")\n",
    "print(\"âœ“ Paragraph chunking: Natural semantic units\")\n",
    "print(\"âœ“ Sliding window: Creates overlap for context preservation\")\n",
    "print(\"âœ“ Smart chunking: Avoids mid-sentence splits\")\n",
    "print(\"âœ“ Overlap helps maintain context across chunks\")\n",
    "print(\"âœ“ Choose strategy based on: task, content type, model limits\")\n",
    "print(\"âœ“ For RAG: 200-500 tokens per chunk is typical\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fbd06",
   "metadata": {},
   "source": [
    "# 2. Fixed-Size Character Chunking\n",
    "\n",
    "## ðŸ“– What is Fixed-Size Character Chunking?\n",
    "\n",
    "**Fixed-size character chunking** splits text into chunks of exactly N characters, regardless of word or sentence boundaries.\n",
    "\n",
    "**Method:**\n",
    "```python\n",
    "chunk_size = 1000  # 1000 characters per chunk\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Text: \"Hello world! This is amazing.\"\n",
    "Chunk size: 10 characters\n",
    "Result:\n",
    "  Chunk 1: \"Hello worl\"\n",
    "  Chunk 2: \"d! This is\"\n",
    "  Chunk 3: \" amazing.\"\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ Why Use Fixed-Size Character Chunking?\n",
    "\n",
    "### **Advantages:**\n",
    "1. **Simplicity** - Easiest to implement (one line of code)\n",
    "2. **Predictability** - Always know exact chunk size\n",
    "3. **Speed** - Fastest chunking method (O(n))\n",
    "4. **Memory Efficient** - Minimal overhead\n",
    "5. **No Dependencies** - Pure Python, no libraries needed\n",
    "\n",
    "### **Disadvantages:**\n",
    "1. **Breaks Words** - May split \"amazing\" into \"ama\" and \"zing\"\n",
    "2. **Breaks Sentences** - Loses semantic coherence\n",
    "3. **Context Loss** - Mid-word splits destroy meaning\n",
    "4. **Poor for NLP** - Most NLP tasks need complete words/sentences\n",
    "\n",
    "## â±ï¸ When to Use Fixed-Size Character Chunking\n",
    "\n",
    "### âœ… **Use When:**\n",
    "\n",
    "**1. Binary/Non-Text Data**\n",
    "- Example: Split log files, DNA sequences\n",
    "- Why: Content doesn't have semantic meaning\n",
    "- Benefit: Fast, simple processing\n",
    "\n",
    "**2. Display/UI Constraints**\n",
    "- Example: Show max 500 characters in preview\n",
    "- Why: Hard limit on display area\n",
    "- Use case: Text truncation for UI\n",
    "\n",
    "**3. Storage Limitations**\n",
    "- Example: Database field max 1000 chars\n",
    "- Why: Hard constraint from schema\n",
    "- Solution: Split before storing\n",
    "\n",
    "**4. Quick Prototyping**\n",
    "- Example: Initial testing of chunking pipeline\n",
    "- Why: Fast to implement, test logic\n",
    "- Benefit: Can refine later\n",
    "\n",
    "**5. Character-Level Models**\n",
    "- Example: Character-level RNN, transformer\n",
    "- Why: Model operates on characters anyway\n",
    "- Use case: Text generation at character level\n",
    "\n",
    "### âŒ **Don't Use When:**\n",
    "\n",
    "**1. NLP Tasks**\n",
    "- Problem: Broken words/sentences lose meaning\n",
    "- Better: Use sentence or word chunking\n",
    "- Why: NLP needs complete linguistic units\n",
    "\n",
    "**2. LLM Processing**\n",
    "- Problem: Token boundaries don't align with characters\n",
    "- Better: Use token-based chunking\n",
    "- Why: Models tokenize differently (\"running\" vs \"run\"+\"##ning\")\n",
    "\n",
    "**3. Search/Retrieval**\n",
    "- Problem: Partial words don't match queries\n",
    "- Better: Use semantic or sentence chunking\n",
    "- Why: \"amaz\" won't match \"amazing\"\n",
    "\n",
    "**4. Human Readability Required**\n",
    "- Problem: Mid-word splits are confusing\n",
    "- Better: Use paragraph or sentence chunking\n",
    "- Why: Humans need complete thoughts\n",
    "\n",
    "## ðŸ“Š How It Works\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start at position 0\n",
    "2. Extract `chunk_size` characters\n",
    "3. Move to position + `chunk_size`\n",
    "4. Repeat until end of text\n",
    "\n",
    "**With Overlap:**\n",
    "1. Start at position 0\n",
    "2. Extract `chunk_size` characters\n",
    "3. Move to position + `chunk_size` - `overlap`\n",
    "4. Repeat until end\n",
    "\n",
    "**Time Complexity:** O(n)  \n",
    "**Space Complexity:** O(n)\n",
    "\n",
    "## ðŸŒ Real-World Applications\n",
    "\n",
    "1. **Log Processing** - Split log files into fixed blocks\n",
    "2. **DNA Sequencing** - Analyze genetic sequences in fixed windows\n",
    "3. **Binary Data** - Process binary files in chunks\n",
    "4. **Text Preview** - Truncate for UI display\n",
    "5. **File Transfer** - Split large files for network transmission\n",
    "6. **Database Storage** - Fit text into VARCHAR limits\n",
    "\n",
    "## ðŸ’¡ Key Insights\n",
    "\n",
    "âœ… Simplest chunking method - use for prototyping  \n",
    "âœ… Perfect for **non-semantic** content (logs, binary data)  \n",
    "âœ… Add **overlap** to preserve some context  \n",
    "âœ… Use `len(text.encode('utf-8'))` for byte size, not char count  \n",
    "âœ… UTF-8 characters can be 1-4 bytes - be careful!  \n",
    "âœ… Not suitable for **NLP or LLM tasks**  \n",
    "âœ… Consider **smart chunking** as next step  \n",
    "âœ… For production: use sentence or semantic chunking instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED-SIZE CHARACTER CHUNKING - COMPLETE EXAMPLE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED-SIZE CHARACTER CHUNKING - COMPREHENSIVE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "# 1. BASIC CHARACTER CHUNKING\n",
    "print(\"\\n1. BASIC CHARACTER CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text = \"Machine learning transforms data into insights through algorithms.\"\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Length: {len(text)} characters\")\n",
    "\n",
    "chunk_size = 20\n",
    "basic_chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "print(f\"\\nChunks (size={chunk_size}):\")\n",
    "for i, chunk in enumerate(basic_chunks, 1):\n",
    "    print(f\"  Chunk {i} ({len(chunk):2d} chars): '{chunk}'\")\n",
    "\n",
    "print(f\"\\nObservation: Words are split (e.g., 'transfor' and 'ms')\")\n",
    "\n",
    "# 2. CHARACTER CHUNKING WITH OVERLAP\n",
    "print(\"\\n2. CHARACTER CHUNKING WITH OVERLAP\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def chunk_with_overlap(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Split text into overlapping character chunks\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        \n",
    "        if end == len(text):\n",
    "            break\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "overlap_chunks = chunk_with_overlap(text, chunk_size=20, overlap=5)\n",
    "print(f\"Chunks with 5-character overlap:\")\n",
    "for i, chunk in enumerate(overlap_chunks, 1):\n",
    "    print(f\"  Chunk {i}: '{chunk}'\")\n",
    "\n",
    "# Show overlap\n",
    "if len(overlap_chunks) > 1:\n",
    "    print(f\"\\nOverlap demonstration:\")\n",
    "    print(f\"  Chunk 1 ends with: '{overlap_chunks[0][-5:]}'\")\n",
    "    print(f\"  Chunk 2 starts with: '{overlap_chunks[1][:5]}'\")\n",
    "    print(f\"  Match: {overlap_chunks[0][-5:] == overlap_chunks[1][:5]}\")\n",
    "\n",
    "# 3. METADATA TRACKING\n",
    "print(\"\\n3. CHUNKING WITH METADATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def chunk_with_metadata(text: str, chunk_size: int) -> List[dict]:\n",
    "    \"\"\"Create chunks with metadata (position, size, etc.)\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk_text = text[i:i+chunk_size]\n",
    "        chunks.append({\n",
    "            'id': len(chunks),\n",
    "            'text': chunk_text,\n",
    "            'start_pos': i,\n",
    "            'end_pos': i + len(chunk_text),\n",
    "            'size': len(chunk_text),\n",
    "            'word_count': len(chunk_text.split())\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "meta_chunks = chunk_with_metadata(text, chunk_size=25)\n",
    "print(f\"Chunks with metadata:\")\n",
    "for chunk in meta_chunks:\n",
    "    print(f\"\\nChunk {chunk['id']}:\")\n",
    "    print(f\"  Text: '{chunk['text']}'\")\n",
    "    print(f\"  Position: {chunk['start_pos']}-{chunk['end_pos']}\")\n",
    "    print(f\"  Size: {chunk['size']} chars\")\n",
    "    print(f\"  Words: {chunk['word_count']}\")\n",
    "\n",
    "# 4. RECONSTRUCTION FROM CHUNKS\n",
    "print(\"\\n4. RECONSTRUCTING TEXT FROM CHUNKS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Chunk without overlap\n",
    "no_overlap = chunk_with_overlap(text, chunk_size=20, overlap=0)\n",
    "reconstructed = ''.join(no_overlap)\n",
    "\n",
    "print(f\"Original:      '{text}'\")\n",
    "print(f\"Reconstructed: '{reconstructed}'\")\n",
    "print(f\"Match: {text == reconstructed}\")\n",
    "\n",
    "# With overlap - need to remove duplicates\n",
    "with_overlap = chunk_with_overlap(text, chunk_size=20, overlap=5)\n",
    "naive_join = ''.join(with_overlap)\n",
    "print(f\"\\nWith overlap (naive join): {len(naive_join)} chars (original: {len(text)})\")\n",
    "print(f\"Problem: Duplicated overlap regions!\")\n",
    "\n",
    "# 5. BYTE-BASED CHUNKING (IMPORTANT FOR UTF-8)\n",
    "print(\"\\n5. BYTE-BASED VS CHARACTER-BASED CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "unicode_text = \"Hello ä¸–ç•Œ! Ã‰moji: ðŸ˜€\"\n",
    "print(f\"Text: {unicode_text}\")\n",
    "print(f\"Character length: {len(unicode_text)}\")\n",
    "print(f\"Byte length (UTF-8): {len(unicode_text.encode('utf-8'))}\")\n",
    "\n",
    "# Character-based\n",
    "char_chunks = [unicode_text[i:i+5] for i in range(0, len(unicode_text), 5)]\n",
    "print(f\"\\nCharacter chunks (5 chars each):\")\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    bytes_len = len(chunk.encode('utf-8'))\n",
    "    print(f\"  Chunk {i}: '{chunk}' ({len(chunk)} chars, {bytes_len} bytes)\")\n",
    "\n",
    "# Byte-based (careful with UTF-8!)\n",
    "def chunk_by_bytes(text: str, byte_size: int) -> List[str]:\n",
    "    \"\"\"Chunk by bytes, handling UTF-8 encoding properly\"\"\"\n",
    "    encoded = text.encode('utf-8')\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(encoded), byte_size):\n",
    "        chunk_bytes = encoded[i:i+byte_size]\n",
    "        # Try to decode, handle incomplete characters\n",
    "        try:\n",
    "            chunk = chunk_bytes.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # Truncate to last valid character\n",
    "            for j in range(len(chunk_bytes) - 1, -1, -1):\n",
    "                try:\n",
    "                    chunk = chunk_bytes[:j].decode('utf-8')\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "byte_chunks = chunk_by_bytes(unicode_text, byte_size=10)\n",
    "print(f\"\\nByte chunks (10 bytes each):\")\n",
    "for i, chunk in enumerate(byte_chunks, 1):\n",
    "    print(f\"  Chunk {i}: '{chunk}'\")\n",
    "\n",
    "# 6. PERFORMANCE TESTING\n",
    "print(\"\\n6. PERFORMANCE TESTING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Generate large text\n",
    "large_text = \"The quick brown fox jumps over the lazy dog. \" * 10000  # ~450KB\n",
    "print(f\"Large text: {len(large_text):,} characters\")\n",
    "\n",
    "# Method 1: List comprehension\n",
    "start = time.time()\n",
    "chunks1 = [large_text[i:i+1000] for i in range(0, len(large_text), 1000)]\n",
    "time1 = time.time() - start\n",
    "print(f\"\\nList comprehension: {time1:.6f} seconds ({len(chunks1)} chunks)\")\n",
    "\n",
    "# Method 2: Function with loop\n",
    "start = time.time()\n",
    "chunks2 = chunk_with_overlap(large_text, chunk_size=1000, overlap=0)\n",
    "time2 = time.time() - start\n",
    "print(f\"Function with loop: {time2:.6f} seconds ({len(chunks2)} chunks)\")\n",
    "\n",
    "print(f\"\\nList comprehension is {time2/time1:.2f}x faster\")\n",
    "\n",
    "# 7. HANDLING EDGE CASES\n",
    "print(\"\\n7. HANDLING EDGE CASES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Empty string\n",
    "empty_chunks = chunk_with_overlap(\"\", chunk_size=10, overlap=0)\n",
    "print(f\"Empty string: {empty_chunks} (should be empty list)\")\n",
    "\n",
    "# Text shorter than chunk size\n",
    "short_text = \"Hi!\"\n",
    "short_chunks = chunk_with_overlap(short_text, chunk_size=100, overlap=0)\n",
    "print(f\"\\nShort text ('{short_text}'): {short_chunks}\")\n",
    "print(f\"  Returns single chunk even though shorter than chunk_size\")\n",
    "\n",
    "# Chunk size = 0 (invalid)\n",
    "try:\n",
    "    invalid_chunks = chunk_with_overlap(text, chunk_size=0, overlap=0)\n",
    "except:\n",
    "    print(f\"\\nChunk size = 0: Would cause infinite loop!\")\n",
    "\n",
    "# Overlap >= chunk size\n",
    "overlap_chunks_large = chunk_with_overlap(\"Hello world\", chunk_size=5, overlap=5)\n",
    "print(f\"\\nOverlap = chunk_size: {len(overlap_chunks_large)} chunks\")\n",
    "print(f\"  Problem: Each chunk starts at same position!\")\n",
    "\n",
    "# 8. PRACTICAL EXAMPLE: FILE PROCESSING\n",
    "print(\"\\n8. PRACTICAL EXAMPLE: LOG FILE PROCESSING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "log_data = \"\"\"2024-12-11 10:00:00 INFO Application started\n",
    "2024-12-11 10:00:05 DEBUG Loading configuration\n",
    "2024-12-11 10:00:10 INFO Database connected\n",
    "2024-12-11 10:00:15 WARNING High memory usage\n",
    "2024-12-11 10:00:20 ERROR Failed to connect to API\n",
    "2024-12-11 10:00:25 INFO Retrying connection\n",
    "2024-12-11 10:00:30 INFO Connection established\"\"\"\n",
    "\n",
    "print(\"Log data to process:\")\n",
    "print(log_data[:200] + \"...\")\n",
    "\n",
    "# Process in 100-character chunks\n",
    "log_chunks = chunk_with_metadata(log_data, chunk_size=100)\n",
    "\n",
    "print(f\"\\nProcessed into {len(log_chunks)} chunks:\")\n",
    "for chunk in log_chunks[:3]:  # Show first 3\n",
    "    print(f\"\\nChunk {chunk['id']} ({chunk['size']} chars):\")\n",
    "    print(f\"  {chunk['text'][:60]}...\")\n",
    "    \n",
    "# Count error messages\n",
    "error_count = sum(1 for chunk in log_chunks if 'ERROR' in chunk['text'])\n",
    "print(f\"\\nChunks containing 'ERROR': {error_count}\")\n",
    "\n",
    "# 9. COMPARISON WITH OTHER CHUNKING METHODS\n",
    "print(\"\\n9. COMPARISON: CHARACTER VS WORD CHUNKING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sample = \"Natural language processing enables computers to understand text.\"\n",
    "print(f\"Sample: {sample}\")\n",
    "\n",
    "# Character chunks\n",
    "char_chunks = [sample[i:i+20] for i in range(0, len(sample), 20)]\n",
    "print(f\"\\nCharacter chunks (20 chars):\")\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    print(f\"  {i}. '{chunk}'\")\n",
    "\n",
    "# Word chunks (for comparison)\n",
    "words = sample.split()\n",
    "word_chunks = [' '.join(words[i:i+3]) for i in range(0, len(words), 3)]\n",
    "print(f\"\\nWord chunks (3 words):\")\n",
    "for i, chunk in enumerate(word_chunks, 1):\n",
    "    print(f\"  {i}. '{chunk}'\")\n",
    "\n",
    "print(f\"\\nObservation:\")\n",
    "print(f\"  Character: Breaks mid-word, unpredictable content\")\n",
    "print(f\"  Word: Clean boundaries, better semantic meaning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Fixed-size character chunking: Simplest method\")\n",
    "print(\"âœ“ Fast and memory efficient (O(n) time/space)\")\n",
    "print(\"âœ“ Problem: Breaks words and sentences\")\n",
    "print(\"âœ“ Use for: Binary data, logs, prototyping\")\n",
    "print(\"âœ“ Don't use for: NLP, LLM, search, human-readable content\")\n",
    "print(\"âœ“ Overlap helps preserve some context\")\n",
    "print(\"âœ“ Track metadata (position, size) for reconstruction\")\n",
    "print(\"âœ“ Be careful with UTF-8: bytes â‰  characters\")\n",
    "print(\"âœ“ List comprehension faster than loops\")\n",
    "print(\"âœ“ For production NLP: use sentence or semantic chunking\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
